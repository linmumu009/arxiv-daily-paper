# PathoSyn: Imaging-Pathology MRI Synthesis via Disentangled Deviation Diffusion

Jian Wang†, Sixing Rong, Jiarui Xing, Yuling Xu and Weide Liu, Member, IEEE

Abstract—We present PathoSyn, a unified generative framework for Magnetic Resonance Imaging (MRI) image synthesis that reformulates imaging-pathology as a disentangled additive deviation on a stable anatomical manifold. Current generative models typically operate in the global pixel domain or rely on binary masks, these paradigms often suffer from feature entanglement, leading to corrupted anatomical substrates or structural discontinuities. PathoSyn addresses these limitations by decomposing the synthesis task into deterministic anatomical reconstruction and stochastic deviation modeling. Central to our framework is a Deviation-Space Diffusion Model designed to learn the conditional distribution of pathological residuals, thereby capturing localized intensity variations while preserving global structural integrity by construction. To ensure spatial coherence, the diffusion process is coupled with a seam-aware fusion strategy and an inference-time stabilization module, which collectively suppress boundary artifacts and produce high-fidelity internal lesion heterogeneity. PathoSyn provides a mathematically principled pipeline for generating high-fidelity patient-specific synthetic datasets, facilitating the development of robust diagnostic algorithms in low-data regimes. By allowing interpretable counterfactual disease progression modeling, the framework supports precision intervention planning and provides a controlled environment for benchmarking clinical decision-support systems. Quantitative and qualitative evaluations on tumor imaging benchmarks demonstrate that PathoSyn significantly outperforms holistic diffusion and mask-conditioned baselines in both perceptual realism and anatomical fidelity. The source code of this work will be made publicly available.

Index Terms—Image Generation; Disentangled Representation; Diffusion Model.

# I. INTRODUCTION

Pathological image generation is increasingly vital for medical image analysis, supporting data augmentation [1]–[3], disease progression modeling [4], [5], and clinical decision

Manuscript received XX XX, 2025; revised XX XX, 2025.

Jian Wang was with the Department of Radiology, Boston Children's Hospital, Harvard Medical School, Boston, MA 02115 USA. (e-mail: jianbljh@gmail.com).

Sixing Rong is with the College of Science, Northeastern University, Boston, MA, 02115, USA. (e-mail: rong.s@northeastern.edu).

Jiarui Xing is with the School of Medicine, Yale University, New Haven, CT 06510 USA. (e-mail: jiarui.xing@yale.edu)

Yuling Xu is with the Department of Cardiac Surgery, The Second Affiliated Hospital of Jiangxi Medical College, Nanchang University, Nanchang 330000, China.

Weide Liu is with the College of Computing and Data Science, Nanyang Technological University, Singapore 639798. (e-mail: weide001@e.ntu.edu.sg)

support [6], [7]. However, acquiring balanced datasets is hindered by limited patient availability, ethical constraints, and high annotation costs. Generative models mitigate this scarcity by synthesizing realistic pathological images that complement real-world data. Beyond algorithmic utility, high-fidelity synthetic pathology aids in clinical intervention planning by illustrating plausible disease trajectories, surfacing rare edge cases, and enabling controlled evaluation of models prior to deployment [4], [5], [8]–[10]. Enhancing the fidelity and consistency of these images is therefore directly linked to developing safer, more reliable tools for clinical decision-making.

Despite growing interest, generating pathological images remains challenging due to the intrinsic asymmetry between anatomy and disease. The anatomical structure, including organ geometry and spatial layout, is largely stable for a given subject, whereas the pathological appearance is the main source of uncertainty, varying in intensity, texture, and clinical stage. The most clinically relevant variability is concentrated in the way disease perturbs a stable anatomical substrate. Existing generative models [3], [11] rarely make this distinction explicit. When operating directly in image space, anatomy and pathology are treated equally stochastic; the model must learn a high-dimensional distribution over all pixels simultaneously. This unnecessarily enlarges the solution space and permits the generator to introduce anatomical modifications in order to fit the data, thereby producing systematic distortions of anatomically stable structures. Consequently, current approaches fall into two suboptimal extremes: either they model the image as a holistic entity with no explicit separation [12], or they enforce complete separation via masking, which identifies location, but overlooks internal lesion appearance and continuity with surrounding tissue.

Although deep learning-based methods have significantly advanced visual fidelity compared to traditional rule-based algorithms, they have remained largely confined to the same image-space paradigm [13], [14]. Early applications of generative adversarial networks (GANs) synthesized lesions by modifying entire slices or volumes, sometimes conditioned on labels or coarse masks [1], [15]–[18]; these models can create sharp and visually plausible images, but tend to be unstable, difficult to control, and prone to hallucination of anatomy that does not correspond to any realistic background. Variational autoencoders (VAEs) introduced a probabilistic latent representation and more stable training [19]–[21], and structured or disentangled variants attempted to separate anatomy and

![](images/e632220e030b36fc190f4ea83b40a70985d45149b1cb924a421196ffaa5782c5.jpg)  
Fig. 1. Conventional models synthesize images either by processing the entire image holistically or by fully segregating pathological from non-lesion regions. In contrast, PathoSyn explicitly disentangles anatomical structure from pathological alterations and performs generative modeling within a deviation space, thereby enabling controlled modulation of lesion characteristics while preserving the underlying anatomical substrate.

pathology in latent space [22], [23]. However, because decoding still happens directly to full images, they often oversmooth fine pathological details and leak stochastic variation into non-lesional regions. More recently, diffusion models have become the dominant paradigm for high-fidelity synthesis in both natural and medical imaging [11], [12], [24]–[26]. By iteratively denoising from a simple prior, they capture rich, multimodal image statistics and offer strong sample diversity. However, most medical diffusion models still learn a distribution over entire images or over mask-conditioned images, without explicitly isolating where uncertainty should reside. Mask conditioning helps to specify where pathology should appear but not what it should look like internally [27]–[29]; lesions may have plausible boundaries, but lack realistic internal texture, heterogeneity, or progression patterns. As a result, even advanced models still entangle anatomical background with pathological variation and fail to exploit that anatomy is largely deterministic while pathology is a structured deviation (Fig. 1).

These limitations indicate that progress in pathological synthesis is not primarily determined by the choice of network architecture but by how the problem itself is represented. When a model treats the entire image as an unconstrained random variable, it must learn a full high-dimensional distribution in which anatomy and pathology vary simultaneously, even though only one of them is expected to change. Instead, a more principled formulation views a pathological image as the combination of two qualitatively different factors: a stable anatomical substrate that should remain preserved and a pathological deviation that carries the uncertainty, diversity and progression of the disease. Structurally separating these components reduces the effective complexity of the generative task and better reflects clinical reality: anatomy behaves as the baseline, while pathology introduces variation. In other words, disease does not replace the entire image; it perturbs an existing anatomical state. Motivated by this perspective, we introduce PathoSyn, a deviation-based framework for pathological image generation. Rather than end-to-end synthesizing

full images, PathoSyn focuses the generative process on the pathology component while preserving the underlying anatomical substrate. We instantiate this concept using a diffusion process defined over the deviation rather than the image itself, enabling pathological variation to be modeled as a controlled and spatially coherent modification that can be integrated with anatomy without introducing unintended structural change. This localizes stochasticity to where it belongs, within pathology, while maintaining anatomical fidelity by construction. As a result, PathoSyn captures clinically significant lesion heterogeneity, preserves global structure, and supports controllable manipulation of pathology appearance without destabilizing non-lesional regions. In summary, PathoSyn provides three key contributions:

- A representation shift that models pathology as a structured deviation from a preserved anatomical substrate, rather than treating the entire image as stochastic.  
- A joint deep learning framework with deviation-space diffusion formulation that localizes generative uncertainty to disease regions, enabling controlled, interpretable, and anatomically consistent synthesis.  
- A practical synthesis pipeline that produces realistic pathological variation while reducing anatomical distortion and improving the reliability of downstream analysis.

# II. RELATED WORK

Prior approaches to pathological image generation can be differentiated by the domain in which the generative distribution is defined and, consequently, which components of the image are treated as stochastic.

a) Holistic Image Synthesis: Early GAN- and VAE-based models [1], [15]-[21] assume a holistic generative process  $\mathbf{x} \sim p_{\theta}(\mathbf{x})$ , which requires the network to assign the probability mass across the entire image manifold  $\mathcal{X} \in \mathbb{R}^{H \times W \times C}$ . This implicitly treats both the anatomical substrate  $\mathbf{a}$  and the pathological appearance  $\mathbf{d}$  as coupled random variables. Because  $p_{\theta}(\mathbf{x})$  must account for the morphology of the lesion without explicit structural constraints, the model often exploits

anatomical degrees of freedom to minimize the training objective, leading to realistic pathology at the cost of distorted background anatomy.

b) Mask-Conditioned and Inpainting Methods: To constrain the generative search space, mask-conditioned methods [27]–[29] narrow the problem to  $\mathbf{x} \sim p_{\theta}(\mathbf{x} \mid \mathbf{m})$ , where  $\mathbf{m} \in \{0,1\}^{H \times W}$  defines a binary lesion region. Although this specifies where changes occur, it does not define how the disease should manifest. The mask  $\mathbf{m}$  encodes spatial support but lacks internal semantic identity; consequently, the generator solves a boundary-respecting fill-in problem rather than a clinically grounded deviation. This produces plausible contours but insufficient internal heterogeneity, as  $\mathbf{m}$  resolves spatial ambiguity but not distributional uncertainty of the state of the disease.

c) Factorized Representation Learning: Representation-based approaches introduce latent factors [30], often decomposing the image into  $(\mathbf{z}_{anat},\mathbf{z}_{path})$  to separate structural and pathological attributes. This aims to model  $p_{\theta}(\mathbf{x})$  through a factorized prior  $p(\mathbf{z}_{anat})p(\mathbf{z}_{path})$ . However, the transformation back to the pixel space,  $\mathbf{x} = f_{\theta}(\mathbf{z}_{anat},\mathbf{z}_{path})$ , inevitably recouples these factors. In practice, stochastic variation intended for the pathology leaks into non-lesional regions. While the representation is disentangled in latent space, the generative mapping is not, meaning anatomical preservation is not guaranteed by construction.

d) Diffusion-based Generative Models: Modern diffusion models improve stability and multimodal sampling, yet they typically evolve the stochastic process over the image  $\mathbf{x}$  itself. Even context-aware variants implement denoising transitions  $\mathbf{x}_t \to \mathbf{x}_{t-1}$  across the entire spatial domain. This enforces neither anatomical invariance nor deviation locality; the model effectively re-synthesizes the entire image rather than expressing uncertainty exclusively where pathology resides. Consequently, diffusion improves local fidelity while retaining the core representational limitation of image-space modeling.

In summary, existing methods either (i) conflate anatomy and pathology in a single stochastic space or (ii) enforce rigid spatial partitioning without modeling pathology as a continuous, anatomy-dependent deviation. Both reflect the same underlying issue: they attempt to learn  $p_{\theta}(\mathbf{x})$  directly, while a clinically grounded formulation should localize uncertainty to the disease-bearing component  $\mathbf{d}$  while treating anatomy  $\mathbf{a}$  as a stable substrate. This motivates the shift toward our proposed deviation-space representation.

# III. METHODOLOGY

# A. Disentangled Representation of Pathological Images

We formulate generative modeling of pathological images by explicitly disentangling the subject-specific anatomical structure from disease-induced appearance variability. This decomposition is based on the distinct statistical properties of these two factors: for a given subject, the anatomical structure is largely invariant, while pathological manifestations exhibit high stochasticity, internal heterogeneity, and spatial localization, even within identical anatomical contexts.

In this framework, a pathological image  $\mathbf{x} \in \mathbb{R}^{H \times W}$  is represented as the additive superposition of a deterministic anatomical substrate  $\mathbf{x}_{\mathrm{sub}}$  and a stochastic pathological deviation field  $\mathbf{r}$ :

$$
\mathbf {x} = \mathbf {x} _ {\text {s u b}} + \mathbf {r}, \tag {1}
$$

where  $\mathbf{x}_{\mathrm{sub}}$  characterizes the stable anatomical configuration, including organ morphology and global tissue organization, and  $\mathbf{r}$  represents localized intensity deviations induced by pathological processes.

This additive formulation contrasts fundamentally with deformation-based models, such as metamorphosis, which account for pathological changes through geometric transformations of an underlying template. By isolating disease-related variation within the appearance space, our formulation enables the independent modeling of pathological features without distorting the underlying anatomical geometry. Consequently,  $\mathbf{x}_{\mathrm{sub}}$  is treated as a subject-specific baseline, while  $\mathbf{r}$  is interpreted as a structured residual that captures the uncertainty and progression of the disease.

The decomposition in Eq. (1) naturally leads to a conditional probabilistic formulation. Since the anatomical substrate is assumed to be preserved for a given subject, we model the deviation field as a random variable conditioned on both the anatomy and the spatial support of the lesion:

$$
\mathbf {r} \sim p (\mathbf {r} \mid \mathbf {x} _ {\text {s u b}}, \mathbf {m}), \tag {2}
$$

where  $\mathbf{m} \in \{0,1\}^{H \times W}$  denotes a binary mask defining the spatial domain of the pathology. By conditioning the distribution on  $\mathbf{m}$ , we explicitly constrain stochastic variation to the lesion site, thereby preventing unintended alterations to healthy tissue and ensuring anatomical fidelity by construction.

# B. Anatomical Substrate and Deviation Field Decomposition

a) Inference of Latent Components: Under the additive model defined in Eq. (1), we assume that an observed pathological image  $\mathbf{x}$  admits a latent decomposition  $\mathbf{x} = \mathbf{x}_{\mathrm{sub}} + \mathbf{r}$ . Let  $\Omega \subset \mathbb{Z}^2$  denote the discrete image lattice, where  $\mathbf{x} \colon \Omega \to \mathbb{R}$  represents the intensity function. The spatial support of the pathology is defined by a binary mask  $\mathbf{m} \colon \Omega \to \{0, 1\}$ , where  $\mathbf{m}(p) = 1$  indicates that the pixel  $p \in \Omega$  resides within the lesional region. We denote the complementary non-lesional mask as  $\bar{\mathbf{m}} := \mathbf{1} - \mathbf{m}$ .

Recovering  $(\mathbf{x}_{\mathrm{sub}},\mathbf{r})$  from a single observation  $\mathbf{x}$  is an ill-posed inverse problem. To achieve a well-posed formulation, we impose structural constraints consistent with clinical priors: (i) intensities in the non-lesional domain  $\bar{\mathbf{m}}$  are dominated by subject-specific anatomy, and (ii) pathological appearance variations are strictly localized to the support  $\mathbf{m}$ . These assumptions motivate a constrained inference scheme in which  $\mathbf{x}_{\mathrm{sub}}$  is estimated primarily from the context of healthy tissue and  $\mathbf{r}$  is limited to the pathological region.

b) Anatomical Substrate Estimation: We define the anatomical substrate  $\mathbf{x}_{\mathrm{sub}}\colon \Omega \to \mathbb{R}$  as a pathologically-suppressed representation that preserves subject-specific morphology. To enforce structural consistency outside the lesion, we utilize the Hadamard product  $\odot$ . The constraint  $\mathbf{x}_{\mathrm{sub}}\approx \mathbf{x}$  on  $\bar{\mathbf{m}}$

is formalized by a masked fidelity term  $\| (\mathbf{x}_{\mathrm{sub}} - \mathbf{x})\odot \bar{\mathbf{m}}\| _2^2$  where  $\| \mathbf{y}\| _2^2 \coloneqq \sum_{p\in \Omega}\mathbf{y}(p)^2$

Because the underlying anatomy within the lesional region  $\mathbf{m}$  is not observed, we introduce a counterfactual healthy reference  $\mathbf{x}_{\mathrm{ph}} := \mathrm{Inp}(\mathbf{x}, \mathbf{m})$ , where  $\mathrm{Inp}(\cdot)$  denotes a fixed inpainting operator that extrapolates the anatomical context from  $\bar{\mathbf{m}}$  to  $\mathbf{m}$ . We estimate  $\mathbf{x}_{\mathrm{sub}}$  by minimizing the variational objective:

$$
\begin{array}{l} \mathcal {L} _ {\mathrm {s u b}} \left(\mathbf {x} _ {\mathrm {s u b}}; \mathbf {x}, \mathbf {m}\right) = \lambda_ {\mathrm {o u t}} \| (\mathbf {x} _ {\mathrm {s u b}} - \mathbf {x}) \odot \bar {\mathbf {m}} \| _ {2} ^ {2} \\ + \lambda_ {\mathrm {i n}} \| (\mathbf {x} _ {\mathrm {s u b}} - \mathbf {x} _ {\mathrm {p h}}) \odot \mathbf {m} \| _ {2} ^ {2}, \tag {3} \\ \end{array}
$$

where  $\lambda_{\mathrm{out}},\lambda_{\mathrm{in}} > 0$  are hyperparameters that balance extrinsic fidelity and intrinsic regularization. In practice,  $\mathbf{x}_{\mathrm{sub}}$  is parameterized by a neural predictor  $f_{\theta}$  optimized for this objective.

c) Pathological Deviation Field: Given the estimated substrate  $\mathbf{x}_{\mathrm{sub}}$ , the subject-specific deviation field is defined as the residual supported by the lesion:

$$
\mathbf {r} _ {0} := \left(\mathbf {x} - \mathbf {x} _ {\text {s u b}}\right) \odot \mathbf {m}. \tag {4}
$$

By construction,  $\mathbf{r}_0(p) = 0$  for all  $p \in \bar{\mathbf{m}}$ , ensuring that stochasticity is restricted to support of the lesion. Unlike deformation-based models that characterize pathology via non-rigid warps,  $\mathbf{r}_0$  captures additive intensity and texture deviations in the appearance space. To bound the dynamic range and ensure numerical stability for the subsequent diffusion process, we apply a point-wise saturation operator:

$$
\mathbf {r} _ {0} \leftarrow \delta \tanh  \left(\frac {\mathbf {r} _ {0}}{\delta}\right), \tag {5}
$$

where  $\delta > 0$  defines the maximum admissible deviation magnitude.

# C. Conditional Diffusion for Deviation Field Modeling

Building upon the decomposition defined in Eq. (1), we model the conditional distribution  $p(\mathbf{r} \mid \mathbf{x}_{\mathrm{sub}}, \mathbf{m})$  using a Denoising Diffusion Probabilistic Model (DDPM) [24]. The diffusion process is defined over the deviation field  $\mathbf{r}$  rather than the global image manifold  $\mathbf{x}$ . This formulation ensures that the generative modeling capacity is exclusively allocated to the pathological appearance, while the anatomical substrate remains deterministic and invariant.

a) Forward Diffusion Process: Let  $\mathbf{r}_0$  denote the clean deviation field extracted via Eq. (4). We define a Markovian forward process that iteratively adds Gaussian noise over  $T$  steps according to a variance schedule  $\{\beta_t\}_{t=1}^T$ :

$$
q \left(\mathbf {r} _ {t} \mid \mathbf {r} _ {t - 1}\right) = \mathcal {N} \left(\sqrt {1 - \beta_ {t}} \mathbf {r} _ {t - 1}, \beta_ {t} \mathbf {I}\right), \quad t = 1, \dots , T. \tag {6}
$$

The latent state at any timestep  $t$  can be expressed in closed form as:

$$
\mathbf {r} _ {t} = \sqrt {\bar {\alpha} _ {t}} \mathbf {r} _ {0} + \sqrt {1 - \bar {\alpha} _ {t}} \boldsymbol {\epsilon}, \quad \boldsymbol {\epsilon} \sim \mathcal {N} (0, \mathbf {I}), \tag {7}
$$

where  $\alpha_{t} = 1 - \beta_{t}$  and  $\bar{\alpha}_{t} = \prod_{s=1}^{t} \alpha_{s}$ . To ensure that stochasticity remains confined to the pathological domain, we enforce a spatial support constraint at each step:

$$
\mathbf {r} _ {t} \leftarrow \mathbf {r} _ {t} \odot \mathbf {m}, \quad \forall t \in \{0, \dots , T \}. \tag {8}
$$

This projection prevents the diffusion process from introducing spurious variances or intensity shifts in the non-lesional anatomical regions.

b) Reverse Denoising and Training Objective: The reverse process is parameterized by a conditional noise predictor  $\epsilon_{\theta}$ , which estimates the noise component added to the deviation field. The network is conditioned on the noisy deviation  $\mathbf{r}_t$ , the anatomical substrate  $\mathbf{x}_{\mathrm{sub}}$ , the spatial mask  $\mathbf{m}$ , and the diffusion time step  $t$ . We optimize the parameters  $\theta$  via the mean-squared error (MSE) objective:

$$
\mathcal {L} _ {\text {d i f f}} = \mathbb {E} _ {\mathbf {r} _ {0}, \epsilon , t} \left[ \| \epsilon - \epsilon_ {\theta} (\mathbf {r} _ {t}, \mathbf {x} _ {\text {s u b}}, \mathbf {m}, t) \| _ {2} ^ {2} \right], \tag {9}
$$

where  $t\sim \mathcal{U}(1,T)$  and  $\mathbf{r}_t$  is sampled according to Eq. (7) subject to the constraint in Eq. (8).

c) Ancestral Sampling and Synthesis: During inference, synthesis begins with sampled Gaussian noise restricted to the lesion support:  $\mathbf{r}_T\sim \mathcal{N}(0,\mathbf{I})$  followed by  $\mathbf{r}_T\gets \mathbf{r}_T\odot \mathbf{m}$ . We iteratively recover the deviation field by applying the reverse transition for  $t = T,\ldots ,1$ :

$$
\mathbf {r} _ {t - 1} = \frac {1}{\sqrt {\alpha_ {t}}} \left(\mathbf {r} _ {t} - \frac {1 - \alpha_ {t}}{\sqrt {1 - \bar {\alpha} _ {t}}} \boldsymbol {\epsilon} _ {\theta} \left(\mathbf {r} _ {t}, \mathbf {x} _ {\text {s u b}}, \mathbf {m}, t\right)\right) + \sigma_ {t} \mathbf {z}, \tag {10}
$$

where  $\mathbf{z} \sim \mathcal{N}(0, \mathbf{I})$  for  $t > 1$  and  $\mathbf{z} = \mathbf{0}$  for  $t = 1$ . To preserve anatomical fidelity, we re-apply the spatial projection  $\mathbf{r}_{t-1} \gets \mathbf{r}_{t-1} \odot \mathbf{m}$  after each update. The final sample  $\hat{\mathbf{r}} = \mathbf{r}_0$  constitutes a stochastic realization of the pathological deviation conditioned on the subject-specific anatomy.

# D. PathoSyn: Deep Learning in the Deviation Space

We implement the proposed formulation through a unified deep learning framework that jointly estimates the anatomical substrate and models the conditional distribution of the pathological deviation field. All components are optimized under a shared objective, ensuring that anatomical-pathological decomposition and subsequent recomposition remain mutually consistent throughout training.

a) Network Architecture: The anatomical substrate estimator  $f_{\mathrm{sub}}$  is realized as a symmetric U-Net with four down-sampling stages and skip connections, facilitating the preservation of high-frequency anatomical details while aggregating global contextual information. The diffusion noise predictor  $\epsilon_{\theta}$  utilizes a high-capacity U-Net architecture composed of Wide-ResNet blocks, with spatial self-attention integrated in the  $16 \times 16$  bottleneck resolution to capture long-range dependencies within pathological regions. The diffusion time step  $t$  is projected through sinusoidal positional embeddings and injected into each residual block to condition the denoising process.  
b) Pathology-Deviation Regularization: Although the diffusion objective in Eq. (9) learns the conditional distribution of pathological deviation fields, it does not explicitly enforce boundary smoothness or subject-specific consistency. We therefore introduce pathology-aware regularization to suppress spatial leakage and stabilize the interaction between the anatomical substrate and the deviation field.

Let  $\mathbf{S} \in [0,1]^{H \times W}$  denote a soft blend map derived by applying a Gaussian kernel to the binary lesion mask  $\mathbf{m}$ , defining a narrow transition band in the periphery of the lesion. From  $\mathbf{S}$ , we derive a boundary weighting function:

$$
\mathbf {w} _ {\text {r i n g}} = 4 \mathbf {S} (1 - \mathbf {S}), \tag {11}
$$

![](images/4d4f19e6179162c863a21950f5025491a4dfefb0a052e5be7038cd6c6b2734ff.jpg)  
Fig. 2. The PathoSyn architecture consists of two encoders and two decoders that disentangle an image into an anatomical substrate and a pathology deviation. The anatomical encoder-decoder pair learns a stable anatomical substrate that preserves underlying tissue geometry, while the pathology encoder-decoder pair models localized pathology deviation driven by lesion appearance. A generative diffusion module refines the pathology deviation to capture diverse and spatially coherent variations. A fusion layer recombines the anatomical substrate with the pathology deviation, enabling reconstruction and controllable synthesis of pathological images by adjusting lesion severity, extent, and spatial distribution while maintaining anatomical consistency. Please refer to the training and inference in Alg. 1 and Alg. 2.

which emphasizes the lesion boundary while vanishing in both the interior and exterior domains. During training, we compute a denoised estimate of the clean deviation field:

$$
\hat {\mathbf {r}} _ {0} = \frac {1}{\sqrt {\bar {\alpha} _ {t}}} \left(\mathbf {r} _ {t} - \sqrt {1 - \bar {\alpha} _ {t}} \boldsymbol {\epsilon} _ {\theta} \left(\mathbf {r} _ {t}, \mathbf {x} _ {\text {s u b}}, \mathbf {m}, t\right)\right). \tag {12}
$$

We constrain  $\hat{\mathbf{r}}_0$  to match the subject-specific deviation target  $\mathbf{r}_0 = (\mathbf{x} - \mathbf{x}_{\mathrm{sub}}) \odot \mathbf{m}$  via a multi-term regularization loss:

$$
\begin{array}{l} \mathcal {L} _ {\mathrm {d e v}} = \lambda_ {\mathrm {p a t}} \| (\hat {\mathbf {r}} _ {0} - \mathbf {r} _ {0}) \odot \mathbf {m} \| _ {1} \\ + \lambda_ {\text {r i n g}} \| (\hat {\mathbf {r}} _ {0} - \mathbf {r} _ {0}) \odot \mathbf {w} _ {\text {r i n g}} \| _ {1} \\ + \lambda_ {\text {l e a k}} \| \hat {\mathbf {r}} _ {0} \odot (\mathbf {1} - \mathbf {m}) \| _ {1}, \tag {13} \\ \end{array}
$$

where the terms, respectively, enforce internal fidelity, boundary consistency, and the suppression of pathological leakage into non-lesional regions.

c) Seam-Aware Decomposition: To explicitly pair deviation modeling with image synthesis, a seam-aware decomposition layer is incorporated during training. Given the estimated anatomical substrate and deviation field, the synthesized pathological image is formed as follows:

$$
\hat {\mathbf {x}} = \mathbf {x} _ {\text {s u b}} + \mathbf {S} \odot \hat {\mathbf {r}} _ {0}, \tag {14}
$$

The consistency of reconstruction is enforced through  $\mathcal{L}_{\mathrm{syn}} = \left\| (\hat{\mathbf{x}} -\mathbf{x})\odot \mathbf{S}\right\| _1$  , which prioritizes the supervision of anatomically sensitive transition zones to ensure seamless integration.

# Algorithm 1: PathoSyn Joint Training

Input: Pairs  $(\mathbf{x},\mathbf{m})$ ; pre-trained  $E_{r}$ ,  $D_{r}$ ; steps  $T$ .

Output: Parameters for  $f_{\mathrm{sub}}$  and  $\epsilon_{\theta}$ .

while not converged do

// 1. Masking & Substrate Extraction

$$
\begin{array}{l} \bar {\mathbf {m}} \leftarrow \mathbf {1} - \mathbf {m}; \mathbf {S} \leftarrow \operatorname {S m o o t h} (\mathbf {m}) \\ \mathbf {x} _ {\mathrm {s u b}} \leftarrow f _ {\mathrm {s u b}} (\mathbf {x} \odot \bar {\mathbf {m}}, \bar {\mathbf {m}}); \mathbf {r} _ {0} \leftarrow (\mathbf {x} - \mathbf {x} _ {\mathrm {s u b}}) \odot \mathbf {m} \\ / / 2. F o r w a r d D i f f u s i o n \\ \text {S a m p l e} t \sim \mathcal {U} \{1, \dots , T \}, \epsilon \sim \mathcal {N} (\mathbf {0}, \mathbf {I}) \\ \mathbf {r} _ {t} \leftarrow \left(\sqrt {\bar {\alpha} _ {t}} \mathbf {r} _ {0} + \sqrt {1 - \bar {\alpha} _ {t}} \epsilon\right) \odot \mathbf {m} \\ / / 3. C o n d i t i o n a l D e n o i s i n g \& \\ \text {R e c o n s t r u c t i o n} \\ \hat {\epsilon} \leftarrow \epsilon_ {\theta} ([ \mathbf {r} _ {t}, \mathbf {x} _ {\text {s u b}}, \mathbf {m} ], t) \quad / / \text {C o n d i t i o n} \\ \begin{array}{c} \text {s u b s t r a t e} \end{array} \\ \hat {\mathbf {r}} _ {0} \leftarrow \frac {1}{\sqrt {\bar {\alpha} _ {t}}} \left(\mathbf {r} _ {t} - \sqrt {1 - \bar {\alpha} _ {t}} \hat {\boldsymbol {\epsilon}}\right) \odot \mathbf {m} \\ / / 4. \text {S y n t h e s i s} + \text {L o s s C o m p u t a t i o n} \\ \hat {\mathbf {x}} \leftarrow \mathbf {x} _ {\mathrm {s u b}} + \mathbf {S} \odot \hat {\mathbf {r}} _ {0} \\ \mathcal {L} _ {\text {d i f f}} \leftarrow \| (\boldsymbol {\epsilon} - \hat {\boldsymbol {\epsilon}}) \odot \mathbf {m} \| _ {2} ^ {2}; \mathcal {L} _ {\text {s y n}} \leftarrow \| (\hat {\mathbf {x}} - \mathbf {x}) \odot \mathbf {S} \| _ {1} \\ \mathcal {L} \leftarrow \mathcal {L} _ {\text {s u b}} + \lambda_ {d} \mathcal {L} _ {\text {d i f f}} + \lambda_ {v} \mathcal {L} _ {\text {d e v}} + \lambda_ {s} \mathcal {L} _ {\text {s y n}} \\ \text {U p d a t e} f _ {\text {s u b}} \text {a n d} \epsilon_ {\theta} \text {v i a b k p r o p a g a t i o n} \\ \end{array}
$$

d) Overall Training Objective: The final objective function jointly integrates anatomical reconstruction, diffusion modeling, and deviation regularization:

$$
\mathcal {L} = \mathcal {L} _ {\text {s u b}} + \lambda_ {\text {d i f f}} \mathcal {L} _ {\text {d i f f}} + \lambda_ {\text {d e v}} \mathcal {L} _ {\text {d e v}} + \lambda_ {\text {s y n}} \mathcal {L} _ {\text {s y n}}. \tag {15}
$$

Algorithm 2: PathoSyn Inference  
Input: Image x, mask m; trained  $f_{\mathrm{sub}},\epsilon_{\theta}$  ; steps T.   
Output: Synthesized pathological image  $\hat{\mathbf{x}}$    
// 1. Substrate Extraction & Preparation   
 $\bar{\mathbf{m}}\gets \mathbf{1} - \mathbf{m};\mathbf{S}\gets \mathrm{Smooth}(\mathbf{m})$ $\mathbf{x}_{\mathrm{sub}}\leftarrow f_{\mathrm{sub}}(\mathbf{x}\odot \bar{\mathbf{m}},\bar{\mathbf{m}})$    
// 2. Residual Noise Initialization   
Sample  $\mathbf{r}_T\sim \mathcal{N}(\mathbf{0},\mathbf{I})$  .  $\mathbf{r}_T\gets \mathbf{r}_T\odot \mathbf{m}\quad /$  Initialize within mask   
for  $t = T,\dots ,1$  do   
// Denoising conditioned on substrate and mask   
 $\hat{\epsilon}\gets \epsilon_{\theta}([\mathbf{r}_t,\mathbf{x}_{\mathrm{sub}},\mathbf{m}],t)$    
Sample  $\epsilon_{n}\sim \mathcal{N}(\mathbf{0},\mathbf{I})$  if  $t > 1$  else  $\epsilon_{n}\gets 0$    
// Reverse Diffusion Step   
 $\mathbf{r}_{t - 1}\gets \frac{1}{\sqrt{\alpha_t}} (\mathbf{r}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}}\hat{\pmb{\epsilon}}) + \sigma_t\pmb {\epsilon}_n$ $\mathbf{r}_{t - 1}\gets \mathbf{r}_{t - 1}\odot \mathbf{m}$  // Enforce spatial support   
// 3. Recomposition   
 $\hat{\mathbf{x}}\gets \mathbf{x}_{\mathrm{sub}} + \mathbf{S}\odot \mathbf{r}_0$  // Combine substrate and residual   
return  $\hat{\mathbf{x}}$

# IV. EXPERIMENTAL EVALUATION

1) Dataset and Pre-processing: Experiments are conducted on the BraTS 2020 brain tumor dataset [31]–[33], which comprises multi-institutional, multi-modal magnetic resonance images including T1, T1-weighted contrast-enhanced (T1Gd), T2-weighted and T2 Fluid Attenuated Inversion Recovery (FLAIR) volumes. The dataset provides expert voxel-level annotations for three tumor sub-regions: the necrotic and non-enhancing tumor core (NCR/NET), the peritumoral edema (ED), and the GD-enhancing tumor (ET). All imaging data are provided in a pre-processed format, including co-registration to a common anatomical template, interpolation to a uniform  $1\mathrm{mm}^3$  resolution, and skull-stripping.

We utilize the official training partition consisting of  $n = 369$  subjects. To ensure a rigorous evaluation framework and prevent data leakage, these subjects are divided at the patient level into three disjoint sets: a training set of 295 subjects, a validation set of 37 subjects and a hold-out testing set of 37 subjects. While the training set is used to optimize the parameters of  $f_{\mathrm{sub}}$  and  $\epsilon_{\theta}$ , the validation set is used to monitor convergence and hyperparameter tuning. The final synthesis performance is evaluated exclusively on the testing set to ensure that the model is generalizable to unseen anatomical substrates. For computational efficiency, all 3D MRI volumes are processed into 2D axial slices and stored in HDF5 format, maintaining strict subject-level separation across all partitions.

All scans undergo standardized preprocessing, including skull stripping, spatial resampling to a common grid, and normalization of the intensity. To disentangle the effect of the quality of the synthesis from that of the size of the training set, all the augmentation regimes, comprising the baseline methods (Brain-LDM [12], MaskDiff-Inpaint [34]) are configured to contribute an equivalent number of synthetic samples to the training pool. To ensure a fair, controlled comparison and seamless integration with our proposed methods,

we implement a pact-wise VAE-GAN [35] within our unified PathoSyn framework. This allows us to standardize training protocols, architectural choices, and evaluation procedures across PathoSyn: VAE-GAN and our final model, PathoSyn: Diff, thus isolating the contribution of the generative paradigm rather than confounding factors from differing implementations.

# A. Implementation Details

For methodological parity, all task-specific models share an identical 2D architecture and optimization pipeline. We employ the AdamW optimizer ( $\eta = 10^{-4}$ , weight decay  $10^{-5}$ ) with a cosine annealing scheduler. Generative models are trained for 300 epochs, while downstream task models are trained for 200. PathoSyn-Diff utilizes a  $T = 1000$  step linear noise schedule with DDIM sampling during inference. PathoSyn-VAE-GAN employs a 128-dimensional latent space and a PatchGAN discriminator with an adversarial weight  $\lambda_{\mathrm{adv}} = 0.1$ . Training is performed on NVIDIA RTX 3090/A6000 GPUs (24-48 GB VRAM). Inference utilizes test-time normalization only, without augmentation, to prevent masking potential domain shifts.

# B. Evaluation Framework

a) Data Realism Analysis: We quantify visual and statistical fidelity through a real-versus-synthetic discrimination task. A binary classifier is trained to distinguish authentic BRaTS 2023 slices from generated samples across all synthesis paradigms. Performance is measured via Receiver Operating Characteristic (ROC) curves and Area Under the Curve (AUC).  
-  $95\%$  bootstrap confidence intervals are used to evaluate statistical robustness.  
- Lower AUC values  $(\downarrow)$  signify higher realism, indicating that synthetic samples are statistically closer to the real data manifold.  
b) Mathematical Disentanglement Analysis: To rigorously evaluate the separation of subject-specific anatomy and pathological variations, we analyze the statistical independence between the anatomical substrate  $\mathbf{x}_{\mathrm{sub}}$  and the deviation field  $\mathbf{r}$ . Ideally, a robust generative framework should achieve high-level feature orthogonality while maintaining low-level spatial alignment. We define two metrics to quantify the disentanglement in the latent feature space  $\Phi$ :  
1) Feature Orthogonality  $(\mathcal{C})$ : We measure the absolute cosine similarity between the feature vectors  $f_{\mathrm{sub}} = \phi(\mathbf{x}_{\mathrm{sub}})$  and  $f_{\mathrm{res}} = \phi(\mathbf{r})$  extracted by a frozen medical encoder  $\phi(\cdot)$ . A value  $\mathcal{C} \approx 0$  indicates that the two representations reside in orthogonal subspaces.  
2) Mutual Information (MI): We estimate the non-linear dependency between  $f_{\mathrm{sub}}$  and  $f_{\mathrm{res}}$ . A reduction in MI signifies that the stochastic pathological synthesis does not leak anatomical information into the deviation field.  
c) Downstream Task Utility: We evaluate the functional utility of synthetic data by measuring performance gains in segmentation and classification tasks. All experiments are conducted in 2D to isolate the influence of generative quality from architectural advantages.

- Segmentation: We benchmark across the supervised regimes (nnU-Net [36], U-Net [37]), semi-supervised regimes (Mean-Teacher U-Net [38]), and unsupervised (Contra-Seg [39]). The results are reported as mean Dice similarity coefficient (DSC)  $\pm$  standard deviation.  
- Classification: Models are assessed using Accuracy, AUC, and calibration error to determine whether synthetic augmentation improves generalization or introduces systematic bias.

d) Clinical Distributional Consistency: To ensure that synthetic tumors respect clinical appearance statistics, we analyze feature-space alignment with real-world pathology:

- Perceptual Statistics: Deep perceptual texture features are extracted to capture high-level semantic alignment.  
- Photometric reliability: Intensity and texture distributions are assessed to verify mesoscopic consistency with real MRI sequences.  
- Statistical Proximity: We analyze Empirical Cumulative Distribution Functions (CDFs) of feature distances; a shift toward the origin indicates a reduction in distributional shift from the authentic pathological manifold.

# C. Realism Evaluation via Discriminability and Stability

Fig. 3 (top) illustrates the performance of a binary discriminator trained to differentiate authentic BRaTS 2023 slices from synthetic samples. The results are presented as Receiver Operating Characteristic (ROC) curves, where shaded regions represent  $95\%$  bootstrap confidence intervals. In this context, a lower Area Under the Curve (AUC  $\downarrow$ ) indicates a reduced separability between the real and synthetic domains, signifying superior visual realism and minimized distributional bias. This convergence toward indistinguishability suggests that PathoSyn generates samples that reside closer to the authentic data manifold, thus mitigating the risk of introducing systematic biases during downstream clinical task training.

Fig. 3 (bottom) details the distribution of discriminability AUC values obtained via bootstrap resampling. The violin and box plots depict the variance in AUC estimates, while diamonds denote the point-estimate AUC derived from the mean ROC curve. Narrower distributions paired with lower medians  $(\downarrow)$  reflect improved generative stability and a consistent alignment with the real data distribution. This statistical stability across resamples demonstrates that the achieved realism is an inherent property of the model's learned representation rather than a stochastic artifact, ensuring robust generalization and minimal domain shift between synthetic and authentic pathological images.

# D. Mathematical Disentanglement Analysis

As summarized in Tab. I, we evaluate the mathematical independence between the anatomical substrate and pathological deviations.

Anatomical Corruption in Holistic Models: A fundamental flaw of holistic models like Brain-LDM is the lack of explicit structural constraints. Since they treat the image as a monolithic distribution, the synthesis of a lesion inevitably

![](images/b9e97dbaf39e80fb99e674a9ec8176bd529db47ab3e6e77faebd3692c6e7d2a8.jpg)

![](images/ad289da68ca18a22f781b1240684b8aae98a1bb4e45c72c3bf4ddcb8d9821f92.jpg)  
Fig. 3. Top: ROC curves with  $95\%$  bootstrap confidence intervals  $(\downarrow)$ , where reduced detectability indicates greater alignment with real data. Bottom: Distribution of bootstrap AUC discriminability scores between real and generated images  $(\downarrow$  is better).

TABLEI QUANTITATIVE DISENTANGLEMENT ANALYSIS. C AND MI MEASURE THE INDEPENDENCE BETWEEN ANATOMY AND PATHOLOGY. RESULTS ARE MEAN  $\pm$  SD.  

<table><tr><td>Method</td><td>Mech.</td><td>Disent.</td><td>Cos Sim (C) ↓</td><td>MI ↓</td></tr><tr><td>Brain-LDM [12]</td><td>Holistic</td><td>×</td><td>0.425 ± .082</td><td>1.24 ± .15</td></tr><tr><td>Meta-Auto [30]</td><td>Dual-Enc</td><td>✓</td><td>0.245 ± .041</td><td>0.78 ± .12</td></tr><tr><td>MaskDiff-Inpaint</td><td>FG/BG Sep</td><td>✓</td><td>0.185 ± .038</td><td>0.65 ± .09</td></tr><tr><td>PathoSyn (Ours)</td><td>Dev-Space</td><td>✓</td><td>0.072 ± .018†</td><td>0.21 ± .04†</td></tr></table>

leads to anatomical leakage, where the intensity and texture of healthy brain tissues are non-linearly corrupted. This is reflected in the high feature coupling  $(C = 0.425)$ , indicating that the model cannot alter the pathology without inadvertently modifying the identity of the underlying subject.

Structured Disentanglement via Deviation Space: Unlike dual-encoder frameworks (e.g., Meta-Auto [30]) that still exhibit significant latent overlap, PathoSyn achieves nearorthogonality  $(C = 0.072)$ . By constraining the generation to a dedicated deviation space, we effectively "shield" the anatomical manifold from pathological stochasticity. This ensures that the digital twins generated are not only visually realistic, but also mathematically consistent, providing a safer and more reliable foundation for the training of clinical AI.

# E. Downstream Evaluation Protocol

Tab. II summarizes the segmentation performance (Dice Similarity Coefficient) across four distinct learning regimes,

![](images/fecf883d6fca247073d0080773e27edf8ca7a506482625f56ae45892fa63f994.jpg)  
Fig. 4. Qualitative comparison of pathological image synthesis. From left to right: the given image, Brain-LDM reconstruction, MaskDiff-Inpaint completion, PathoSyn (VAE-GAN), and PathoSyn (diffusion). The diffusion-based PathoSyn results further display the anatomical substrate and the corresponding pathology deviation fields, illustrating clearer separation between structural content and lesion-driven appearance, with improved spatial coherence and controllable pathology representation.

ranging from fully supervised to unsupervised configurations. The results indicate that the inclusion of PathoSyn: Diff synthetic data leads to a consistent and statistically significant improvement  $(p < 0.01)$  in all evaluated architectures. Specifically, on the nnU-Net supervised baseline, PathoSyn: Diff achieves a mean Dice score of  $0.845 \pm 0.021$ , outperforming the "No Augmentation" baseline by an absolute margin of  $+6.5\%$ . As shown in Tab. II, the performance gains are not limited to supervised models; in the unsupervised ContraSeg regime, our diffusion-based approach yields a  $+5.5\%$  improvement over the "No Augmentation" baseline, suggesting that PathoSyn effectively captures the underlying pathological manifold in a way that remains useful even without explicit voxel-level labels. Furthermore, the reduced standard deviation observed in the PathoSyn cohorts suggests that our disentangled deviation-space modeling provides more consistent and stable training signals compared to holistic synthesis methods

like Brain-LDM, which can occasionally introduce anatomical artifacts.

Tab. III summarizes the impact of various synthesis strategies on the classification of pathological states in three diverse architectural backbones: ResNet-50 (CNN), DenseNet (3D-Volumetric) and Swin Transformer. The results indicate that data augmentation via PathoSyn: Diff consistently yields the highest discriminative performance, achieving a peak AUROC of 0.915 on the ResNet-50 backbone. This represents a substantial improvement over holistic synthesis models such as Brain-LDM and MaskDiff-Inpaint, underscoring the advantage of modeling stochasticity within a constrained deviation space rather than across the entire image manifold. PathoSyn: Diff markedly improves model calibration, as evidenced by the lowest observed Expected Calibration Error (ECE) across all test regimes (0.043–0.045). As indicated in Tab. III, while conventional and holistic augmentations improve AUROC,

TABLE II SEGMENTATION PERFORMANCE (DICE,  $\uparrow$  ) FOR DIFFERENT SYNTHESIS STRATEGIES EVALUATED ACROSS FOUR SEGMENTATION MODELS SPANNING SUPERVISED (NNU-NET, U-NET), SEMI-SUPERVISED (MEAN TEACHER U-NET), AND UNSUPERVISED (CONTRA-SEG) REGIMES. RESULTS ARE REPORTED AS MEAN  $\pm$  STANDARD DEVIATION.  $\dagger$  INDICATES STATISTICALLY SIGNIFICANT IMPROVEMENT OVER ALL NON-PATHOSYN BASELINES (PAIRED t-TEST,  $p <   0.01$  

<table><tr><td>Synthesis Method</td><td>nnU-Net (Sup.)</td><td>U-Net (Sup.)</td><td>Mean Teacher (Semi-Sup.)</td><td>Contra-Seg (Unsup.)</td></tr><tr><td>No Augmentation</td><td>0.780 ± 0.035</td><td>0.770 ± 0.038</td><td>0.755 ± 0.045</td><td>0.740 ± 0.052</td></tr><tr><td>Conventional Augmentation</td><td>0.800 ± 0.032</td><td>0.788 ± 0.035</td><td>0.770 ± 0.040</td><td>0.748 ± 0.049</td></tr><tr><td>Brain-LDM</td><td>0.810 ± 0.030</td><td>0.802 ± 0.032</td><td>0.785 ± 0.037</td><td>0.760 ± 0.045</td></tr><tr><td>MaskDiff-Inpaint</td><td>0.815 ± 0.029</td><td>0.804 ± 0.031</td><td>0.790 ± 0.036</td><td>0.765 ± 0.043</td></tr><tr><td>PathoSyn: VAE-GAN</td><td>0.830 ± 0.024†</td><td>0.820 ± 0.027†</td><td>0.805 ± 0.030†</td><td>0.780 ± 0.038†</td></tr><tr><td>PathoSyn: Diff</td><td>0.845 ± 0.021†</td><td>0.835 ± 0.025†</td><td>0.820 ± 0.027†</td><td>0.795 ± 0.035†</td></tr></table>

they often provide marginal gains in ECE. In contrast, our diffusion-based approach reduces calibration error by approximately  $35\%$  compared to the No Augmentation baseline. This suggests that the high-fidelity, anatomically-grounded lesions generated by PathoSyn prevent the classifiers from becoming overconfident on synthetic artifacts, leading to more reliable and trustworthy probability estimates that are essential for clinical decision-support systems.

Fig. 5 (Top Left) illustrates the tumor segmentation performance when models are augmented with various synthesis strategies. Beyond global improvements, we specifically analyze the sensitivity of segmentation models to the tumor scale. As shown in comparative analysis, while most generative methods provide marginal gains for large, high-contrast lesions, their performance typically degrades as the size of the lesion decreases due to the loss of high-frequency textural cues in the latent space. In contrast, PathoSyn: Diff demonstrates remarkable robustness across the entire size spectrum. Specifically, for small tumors (defined as the lower quartile of the volume distribution of the injury), our method achieves the most significant relative performance gain. This is attributed to our localized deviation-space modeling, which prevents the "blurring" effect common in holistic models like Brain-LDM. By constraining the diffusion process to the specific pathological support m, PathoSyn preserves the sharp boundaries and mesoscopic textures necessary for a segmentation network to identify early-stage or small-scale pathologies. Tab. II and Fig. 5 collectively indicate that by providing high-fidelity synthetic examples of small lesions—which are often underrepresented in the training distribution-PathoSyn effectively mitigates the class imbalance at the voxel level, leading to a more calibrated and sensitive segmentation output.

# F. Clinical Distributional Consistency

Fig. 4 qualitatively compares the clinical plausibility of the lesions generated. A primary requirement for clinical consistency in neuro-oncology is the adherence to anatomical boundaries and tissue-specific intensity profiles. While holistic models like Brain-LDM often generate "low-level" lesions that ignore the underlying textures, PathoSyn: Diff ensures that pathological deviations—such as peritumoral edema—respect the orientation of white matter tracts and the constraints of the ventricular system. Preservation of the anatomical substrate during the diffusion process ensures that the synthesized

pathology appears physiologically "anchored" rather than superimposed.

Fig. 5 (top right) quantifies the Deep Conceptual Texture Distance, serving as a computational proxy for high-level semantic alignment. In clinical practice, radiologists characterize gliomas through complex morphological patterns, including internal mottling, rim enhancement, and central necrosis. While holistic generative models often produce over-smoothed or biologically sterile textures, PathoSyn: Diff achieves the lowest conceptual distance to the authentic BRaTS distribution. This indicates that the model's latent representations successfully capture the intricate semantic markers essential for diagnostic differentiation. The primary utility of this high-level alignment is the mitigation of shortcut learning. When synthetic data contains subtle digital artifacts or lacks biological complexity, downstream AI models may learn to recognize fake features rather than actual pathology. By narrowing the conceptual gap, PathoSyn ensures that a segmentation or classification network learns the same radiomic signatures present in human patients. This improves the clinical reliability and safety of AI tools, ensuring they generalize effectively to real-world diagnostic workflows without being misled by synthetic inconsistencies.

Fig. 5 (bottom left) provides a granular evaluation of Intensity and Texture Consistency, encompassing first-order statistics (Mean, Skewness, Kurtosis) and second-order GLCM metrics (Contrast, Homogeneity). Radiologically, high-grade gliomas are defined by their extreme heterogeneity, characterized by high intensity variance and specific gradient magnitudes at the infiltrative margins. The alignment of our results with authentic clinical data across these descriptors confirms that PathoSyn effectively models the complex internal photometric environment of the lesion. This level of textural fidelity is essential to ensure that diagnostic models trained on these data are sensitive to the subtle variations of tissue used to determine tumor grade.

Fig. 5 (bottom right) presents the Empirical Cumulative Distribution Function (CDF) of the global feature distance to the real pathological manifold. This curve provides a population-level assessment of generative reliability. The pronounced shift to the left of the PathoSyn curve indicates that the model consistently produces samples within the "clinically acceptable" range of the authentic distribution. In a clinical context, this distributional stability confirms that the framework does not merely produce isolated high-quality samples but consistently

TABLE III CLASSIFICATION PERFORMANCE UNDER DIFFERENT SYNTHESIS STRATEGIES EVALUATED ACROSS CNN, VOLUMETRIC, AND TRANSFORMER ARCHITECTURES. AUROC  $(\uparrow)$  MEASURES DISCRIMINATIVE CAPABILITY, WHILE EXPECTED CALIBRATION ERROR (ECE,  $\downarrow$  ) ASSES THE RELIABILITY OF PROBABILITY ESTIMATES. RESULTS ARE REPORTED AS MEAN  $\pm$  STANDARD DEVIATION.  $\dagger$  DENOTES STATISTICALLY SIGNIFICANT IMPROVEMENT OVER ALL NON-PATHOSYN BASELINES (PAIRED t-TEST,  $p <   0.01$  ).  

<table><tr><td rowspan="2">Synthesis Method</td><td colspan="2">ResNet-50 (2D CNN)</td><td colspan="2">DenseNet (3D-Vol.)</td><td colspan="2">Swin Transformer</td></tr><tr><td>AUROC ↑</td><td>ECE ↓</td><td>AUROC ↑</td><td>ECE ↓</td><td>AUROC ↑</td><td>ECE ↓</td></tr><tr><td>No Augmentation</td><td>0.860 ± 0.022</td><td>0.070 ± 0.011</td><td>0.855 ± 0.024</td><td>0.068 ± 0.012</td><td>0.852 ± 0.023</td><td>0.066 ± 0.013</td></tr><tr><td>Conventional Augmentation</td><td>0.880 ± 0.020</td><td>0.062 ± 0.010</td><td>0.875 ± 0.021</td><td>0.060 ± 0.011</td><td>0.870 ± 0.021</td><td>0.059 ± 0.011</td></tr><tr><td>Brain-LDM</td><td>0.885 ± 0.019</td><td>0.060 ± 0.010</td><td>0.880 ± 0.019</td><td>0.058 ± 0.010</td><td>0.875 ± 0.020</td><td>0.057 ± 0.010</td></tr><tr><td>MaskDiff-Inpaint</td><td>0.890 ± 0.018</td><td>0.058 ± 0.009</td><td>0.885 ± 0.018</td><td>0.056 ± 0.009</td><td>0.880 ± 0.019</td><td>0.055 ± 0.009</td></tr><tr><td>PathoSyn: VAE-GAN</td><td>0.905 ± 0.016†</td><td>0.050 ± 0.008†</td><td>0.900 ± 0.016†</td><td>0.048 ± 0.009†</td><td>0.895 ± 0.017†</td><td>0.047 ± 0.009†</td></tr><tr><td>PathoSyn: Diff</td><td>0.915 ± 0.015†</td><td>0.045 ± 0.008†</td><td>0.910 ± 0.015†</td><td>0.044 ± 0.008†</td><td>0.905 ± 0.015†</td><td>0.043 ± 0.008†</td></tr></table>

![](images/84357f7b19b331796460eeccb2646f42779aced7fd546b180101e146a4daabd0.jpg)

![](images/497618863c2659ed85096d491ce7cd2b07cc12ed05bc9bbacf0dbc118bbef2b4.jpg)

![](images/804902c6c14d579a85e8a6d90a09eaf22568786a9a1f472b1ff9c40881ccea2f.jpg)  
Fig. 5. Top left: segmentation performance on generated images from multiple models, including no augmentation and conventional augmentation baselines, reported by Dice coefficients to evaluate downstream utility. Top right: deep perceptual texture statistics quantifying alignment between generated and real images in feature space. Bottom left: distributions of intensity- and texture-based features comparing generated versus real samples, reflecting low-level photometric and mesoscopic consistency. Bottom right: empirical cumulative distribution functions of feature distances, with the x-axis denoting deviation from real data; curves concentrated toward the origin indicate improved fidelity and reduced distributional shift.

![](images/72ec1eb8102f3254b5b0bf01b13414b0c7396d9b7a97eac0f22f85e5a8996878.jpg)

populates the data space with diverse and biologically credible digital twins. This minimizes the risk of generating outlier samples that could degrade the robustness of downstream clinical AI applications.

# V. DISCUSSION & CONCLUSION

The PathoSyn framework advances medical image synthesis by shifting from unstructured holistic generation to a principled decomposition of pathology. By modeling a stochastic deviation field over a deterministic anatomical substrate, it directly addresses the trade-off between anatomical preservation and pathological realism. Quantitative analysis shows that the clinical utility of synthetic data depends on its distributional and structural consistency. The low feature correlation  $(C = 0.072)$  acts as a safeguard against anatomical leakage, ensuring that the patient's underlying identity remains intact during disease synthesis. By capturing the high-frequency textural heterogeneity and non-Gaussian intensity fluctuations

of high-grade gliomas, PathoSyn reduces shortcut learning and forces downstream models to learn genuine radiomic signatures, such as infiltrative margins and necrotic cores, that transfer to real-world clinical scenarios. Grounded in a diffusion training objective with seam-aware blending and boundary consistency, the framework removes the need for inference-time heuristics and yields seamless, anatomically coherent synthesis, enabling multiple biologically credible digital twins for stress-testing clinical algorithms on rare morphologies. Future work will extend PathoSyn into a fully end-to-end generative pipeline that also models non-lesion structural deformations. In particular, we aim to capture mass effect, where pathological growth induces global geometric shifts and compression of healthy brain tissues, including ventricular displacement and midline shifts. Unifying these photometric and morphometric changes in a single generative system will be key for building the next generation of resilient, accurate, and trustworthy medical AI diagnostic tools.

# CONFLICT OF INTEREST

The authors declare no conflicts of interest.

# VI. REFERENCE

[1] M. Frid-Adar, E. Klang, M. Amitai, J. Goldberger, and H. Greenspan, "Synthetic data augmentation using gan for improved liver lesion classification," in 2018 IEEE 15th international symposium on biomedical imaging (ISBI 2018). IEEE, 2018, pp. 289-293.  
[2] A. Kebaili, J. Lapuyade-Lahorgue, and S. Ruan, "Deep learning approaches for data augmentation in medical imaging: a review," Journal of imaging, vol. 9, no. 4, p. 81, 2023.  
[3] L. R. Koetzier, J. Wu, D. Mastrodicasa, A. Lutz, M. Chung, W. A. Koszek, J. Pratap, A. S. Chaudhari, P. Rajpurkar, M. P. Lungren et al., "Generating synthetic data for medical imaging," Radiology, vol. 312, no. 3, p. e232471, 2024.  
[4] J. S. Yoon, C. Zhang, H.-I. Suk, J. Guo, and X. Li, “Sadm: Sequence-aware diffusion model for longitudinal medical image generation,” in International Conference on Information Processing in Medical Imaging. Springer, 2023, pp. 388-400.  
[5] L. Puglisi, D. C. Alexander, and D. Ravi, “Enhancing spatiotemporal disease progression models via latent diffusion and prior knowledge,” in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2024, pp. 173–183.  
[6] L. Pinto-Coelho, “How artificial intelligence is shaping medical imaging technology: a survey of innovations and applications,” Bioengineering, vol. 10, no. 12, p. 1435, 2023.  
[7] D. Comaniciu, K. Engel, B. Georgescu, and T. Mansi, "Shaping the future through innovations: From medical imaging to precision medicine," pp. 19-26, 2016.  
[8] B. Kim and J. C. Ye, "Diffusion deformable model for 4d temporal medical image generation," in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2022, pp. 539-548.  
[9] Z. Shang, M. A. Turja, E. Feczko, A. Houghton, A. Rueter, L. A. Moore, K. Snider, T. Hendrickson, P. Reiners, S. Stoyell et al., "Learning strategies for contrast-agnostic segmentation via synthseg for infant mri data," in International conference on medical imaging with deep learning. PMLR, 2022, pp. 1075–1084.  
[10] D. J. Van Booven, C.-B. Chen, S. Malpani, Y. Mirzabeigi, M. Mohammadi, Y. Wang, O. N. Kryvenko, S. Punnen, and H. Arora, "Synthetic genitourinary image synthesis via generative adversarial networks: Enhancing artificial intelligence diagnostic precision," Journal of personalized medicine, vol. 14, no. 7, p. 703, 2024.  
[11] A. Kazerouni, E. K. Aghdam, M. Heidari, R. Azad, M. Fayyaz, I. Hacihaliloglu, and D. Merhof, "Diffusion models for medical image analysis: A comprehensive survey," arXiv preprint arXiv:2211.07804, 2022.  
[12] W. H. Pinaya, P.-D. Tudosiu, J. Dafflon, P. F. Da Costa, V. Fernandez, P. Nachev, S. Ourselin, and M. J. Cardoso, “Brain imaging generation with latent diffusion models,” in MICCAI workshop on deep generative models. Springer, 2022, pp. 117–126.  
[13] A. Jog, A. Carass, S. Roy, D. L. Pham, and J. L. Prince, “Random forest regression for magnetic resonance image synthesis,” Medical image analysis, vol. 35, pp. 475–488, 2017.  
[14] D. H. Ye, D. Zikic, B. Glocker, A. Criminisi, and E. Konukoglu, "Modality propagation: coherent synthesis of subject-specific scans with data-driven regularization," in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2013, pp. 606-613.  
[15] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, "Generative adversarial networks," Communications of the ACM, vol. 63, no. 11, pp. 139-144, 2020.  
[16] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, "Image-to-image translation with conditional adversarial networks," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 1125-1134.  
[17] J. M. Wolterink, A. M. Dinkla, M. H. Savenije, P. R. Seevinck, C. A. van den Berg, and I. Isgum, "Deep mr to ct synthesis using unpaired data," in International workshop on simulation and synthesis in medical imaging. Springer, 2017, pp. 14-23.  
[18] X. Yi, E. Walia, and P. Babyn, "Generative adversarial network in medical imaging: A review," Medical image analysis, vol. 58, p. 101552, 2019.

[19] D. P. Kingma and M. Welling, "Auto-encoding variational bayes," arXiv preprint arXiv:1312.6114, 2013.  
[20] C. Baur, S. Denner, B. Wiestler, N. Navab, and S. Albarqouni, "Autoencoders for unsupervised anomaly segmentation in brain mr images: a comparative study," Medical image analysis, vol. 69, p. 101952, 2021.  
[21] J. Ehrhardt and M. Wilms, "Autoencoders and variational autoencoders in medical image analysis," in Biomedical image synthesis and simulation. Elsevier, 2022, pp. 129-162.  
[22] A. Chartsias, T. Joyce, G. Papanastasiou, S. Semple, M. Williams, D. E. Newby, R. Dharmakumar, and S. A. Tsaftaris, "Disentangled representation learning in cardiac image analysis," Medical image analysis, vol. 58, p. 101535, 2019.  
[23] I. Cetin, M. Stephens, O. Camara, and M. A. G. Ballester, "Attri-vae: Attribute-based interpretable representations of medical images with variational autoencoders," Computerized Medical Imaging and Graphics, vol. 104, p. 102158, 2023.  
[24] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” Advances in neural information processing systems, vol. 33, pp. 6840-6851, 2020.  
[25] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution image synthesis with latent diffusion models,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 10684–10695.  
[26] M. M. Ahsan, S. Raman, Y. Liu, and Z. Siddique, “A comprehensive survey on diffusion models and their applications,” Applied Soft Computing, vol. 181, p. 113470, 2025.  
[27] Y. Ren, Z. Zhu, Y. Li, D. Kong, R. Hou, L. J. Grimm, J. R. Marks, and J. Y. Lo, "Mask embedding for realistic high-resolution medical image synthesis," in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2019, pp. 422-430.  
[28] H. Zhang, Y. Liu, J. Yang, S. Wan, X. Wang, W. Peng, and P. Fua, “Lefusion: Controllable pathology synthesis via lesion-focused diffusion models,” arXiv preprint arXiv:2403.14066, 2024.  
[29] X. Meng, K. Sun, J. Xu, X. He, and D. Shen, "Multi-modal modality-masked diffusion network for brain mri synthesis with random modality missing," IEEE Transactions on Medical Imaging, vol. 43, no. 7, pp. 2587-2598, 2024.  
[30] A. Bône, P. Vernhet, O. Colliot, and S. Durrleman, “Learning joint shape and appearance representations with metamorphic auto-encoders,” in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2020, pp. 202–211.  
[31] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, Y. Burren, N. Porz, J. Slotboom, R. Wiest et al., "The multimodal brain tumor image segmentation benchmark (brats)," IEEE transactions on medical imaging, vol. 34, no. 10, pp. 1993-2024, 2014.  
[32] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. S. Kirby, J. B. Freymann, K. Farahani, and C. Davatzikos, "Advancing the cancer genome atlas glioma mri collections with expert segmentation labels and radiomic features," Scientific data, vol. 4, no. 1, pp. 1-13, 2017.  
[33] S. Bakas, M. Reyes, A. Jakab, S. Bauer, M. Rempfler, A. Crimi, R. T. Shinohara, C. Berger, S. M. Ha, M. Rozycki et al., "Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the brats challenge," arXiv preprint arXiv:1811.02629, 2018.  
[34] C. Corneanu, R. Gadde, and A. M. Martinez, “Latentpaint: Image inpainting in latent space with diffusion models,” in Proceedings of the IEEE/CVF winter conference on applications of computer vision, 2024, pp. 4334–4343.  
[35] S. Gur, S. Benaim, and L. Wolf, "Hierarchical patch vae-gan: Generating diverse videos from a single sample," Advances in Neural Information Processing Systems, vol. 33, pp. 16761-16772, 2020.  
[36] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-Hein, "nnu-net: a self-configuring method for deep learning-based biomedical image segmentation," Nature methods, vol. 18, no. 2, pp. 203-211, 2021.  
[37] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,” in International Conference on Medical image computing and computer-assisted intervention. Springer, 2015, pp. 234–241.  
[38] A. Tarvainen and H. Valpola, "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results," in Advances in Neural Information Processing Systems (NeurIPS), vol. 30, 2017.  
[39] L. Liu, A. I. Aviles-Rivero, and C.-B. Schonlieb, "Contrastive registration for unsupervised medical image segmentation," IEEE Transactions on Neural Networks and Learning Systems, 2023.