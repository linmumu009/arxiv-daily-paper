# VLegal-Bench: Cognitively Grounded Benchmark for Vietnamese Legal Reasoning of Large Language Models

Nguyen Tien Dong*

CMC OpenAI

Viet Nam

Minh-Anh Nguyen*

CMC OpenAI

Viet Nam

Thanh Dat Hoang

CMC OpenAI, Griffith University

Australia

Nguyen Tuan Ngoc

CMC OpenAI

Viet Nam

Dao Xuan Quang Minh

CMC OpenAI

Viet Nam

Phan Phi Hai

CMC OpenAI

Viet Nam

Nguyen Thi Ngoc Anhâ€ 

CMC OpenAI, HUST

Viet Nam

Dang Van Tu

CMC OpenAI

Viet Nam

Binh  $\mathbf{V}\mathbf{u}^{\dagger}$

SRH University Heidelberg

Germany

# Abstract

The rapid advancement of large language models (LLMs) has enabled new possibilities for applying artificial intelligence within the legal domain. Nonetheless, the complexity, hierarchical organization, and frequent revisions of Vietnamese legislation pose considerable challenges for evaluating how well these models interpret and utilize legal knowledge. To address this gap, Vietnamese Legal Benchmark (VLegal-Bench) is introduced, the first comprehensive benchmark is designed to systematically assess LLMs on Vietnamese legal tasks. Informed by Bloom's cognitive taxonomy, VLegal-Bench encompasses multiple levels of legal understanding through tasks designed to reflect practical usage scenarios. The benchmark comprises 10,450 samples generated through a rigorous annotation pipeline, where legal experts label and cross-validate each instance using our annotation system to ensure every sample is grounded in authoritative legal documents and mirrors real-world legal assistant workflows, including general legal questions and answers, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving tailored to Vietnamese law. By providing a standardized, transparent, and cognitively informed evaluation framework, VLegal-Bench establishes a solid foundation for assessing LLM performance in Vietnamese legal contexts and supports the development of more reliable, interpretable, and ethically aligned AI-assisted legal systems.

# Keywords

VLegal-Bench, Civil Law Benchmark, Legal Reasoning, Large Language Model, Legal Tech

# 1 Introduction

The emergence of large language models (LLMs) has transformed natural language processing and opened new possibilities for applying artificial intelligence in law [9, 19, 21]. By leveraging massive textual corpora, these models exhibit remarkable capabilities in language understanding, reasoning, and generation. However, despite their success in general tasks, their performance within the legal domain remains uncertain, particularly in languages with complex

legal frameworks such as Vietnamese. The formal, hierarchical, and evolving nature of Vietnamese law demands specialized evaluation to ensure that LLMs can reason and generate outputs in a way that aligns with legal accuracy, consistency, and ethical principles. Existing legal benchmarks [4, 6, 11] have primarily focused on English and Chinese, leaving a notable gap in evaluating LLMs' knowledge of Vietnamese law. Moreover, these benchmarks largely reflect the characteristics of common law systems that rely heavily on case precedents and judicial interpretations. In contrast, Vietnam follows the civil law system (or continental law system) with characteristics of socialist law, where legal authority is derived primarily from codified statutes organized hierarchically. This fundamental difference necessitates a distinct evaluation approach. Vietnamese legal texts differ significantly in structure and linguistic characteristics, relying heavily on hierarchical references among Articles, Clauses, and Points, as well as frequent legislative amendments. While existing legal benchmarks focus on case-based reasoning and precedent interpretation characteristic of common law traditions, the civil law paradigm presents unique challenges:

- Hierarchical statutory interpretation: Models must navigate multi-level legislative structures (Articles  $\rightarrow$  Clauses  $\rightarrow$  Points) and understand how legal provisions are organized and referenced within codified frameworks.  
- Codified legal reasoning: Unlike common law's reliance on judicial precedents, civil law systems require applying explicit written provisions and understanding their systematic relationships.  
- Legislative evolution tracking: Models must comprehend how amendments and new statutes supersede prior provisions, tracking the temporal validity and applicability of legal norms.

These challenges pose significant difficulties for models in recognizing, interpreting, and reasoning over legal provisions and their contextual evolution. Moreover, current benchmarks mainly emphasize general legal QA and general reasoning tasks, offering limited coverage of broader, real-world scenarios, such as those encountered when deploying LLMs as legal assistants or as core components in retrieval-augmented generation (RAG) [7] and GraphRAG [5] systems.

![](images/66a39501a497e11e726f62cceec0719038e73a1e1939d25905cdc2ff7969060c.jpg)  
Figure 1: The five-level cognitive framework of VLegal-Bench.

To address this gap, we introduce VLegal-Bench, the first comprehensive benchmark designed to systematically evaluate the capabilities of Large Language Models (LLMs) on Vietnamese legal tasks within a civil law framework. Grounded in Bloom's cognitive taxonomy, VLegal-Bench is organized to assess LLM performance across progressively deeper cognitive levels from recalling legal provisions to understanding statutory meaning, and ultimately to reasoning through multi-step legal scenarios, illustrated in Figure 1. These levels are instantiated through task types that reflect the distinctive features of Vietnamese law, such as hierarchical statutory structures (Articles, Clauses, and Points) and the need to interpret updated or amended provisions. Each task further corresponds to a real-world application in which LLMs act as legal assistants for drafting legislation, supporting judicial problem-solving, or performing reasoning and content generation in RAG-based systems. This civil law-oriented design makes VLegal-Bench not only applicable to Vietnamese legal contexts but also readily adaptable to other civil law jurisdictions (e.g., France, Germany, Japan, South Korea, Thailand, and other nations following codified legal traditions) that share similar hierarchical statutory frameworks and legislative structures. To construct the benchmark, we developed an annotation-support system that enables legal experts to efficiently search for currently valid Vietnamese legal documents and retrieve relevant legal scenarios across key domains such as land and housing, marriage and family, and finance. The benchmark comprises 10,450 samples produced through a rigorous multi-stage annotation pipeline, in which legal experts label and cross-verify each instance using our system, ensuring that all samples are firmly grounded in authoritative legal sources. Our contributions are summarized as follows:

- VLegal-Bench is introduced, the first benchmark dedicated to evaluating LLMs on Vietnamese legal tasks through a practical, scenario-driven framework that reflects the hierarchical and evolving nature of Vietnamese legislation.  
- The first civil law-oriented benchmark is presented for LLM evaluation in the legal domain, specifically designed to address the unique characteristics of codified legal systems including hierarchical statutory structures, explicit article-clause-point organization, and legislative amendment tracking providing a replicable framework that can be extended to other civil law jurisdictions worldwide.

- A cognitively grounded evaluation methodology proposed informed by Bloom's taxonomy, enabling systematic assessment of LLM capabilities from basic recall to advanced legal reasoning, while effectively simulating real-world use cases in which LLMs function as legal assistants or as reasoning components within RAG systems.  
- A high-quality dataset of 10.450 legal expert-verified samples is released, each anchored in authentic legal sources and designed to capture key cognitive stages: remembering legal texts, understanding statutory meaning, and reasoning over complex legal situations.  
- Extensive experiments are conducted across a diverse set of LLMs, offering insights into their strengths and limitations in Vietnamese legal comprehension and reasoning, while highlighting the challenges specific to civil law interpretation that differ from common law-based benchmarks.

# 2 Related Work

# 2.1 Benchmarking Large Language Models in the Legal Domain

The rapid adoption of Large Language Models (LLMs) has necessitated the development of robust evaluation frameworks tailored to the intricacies of legal language. Early efforts in legal NLP were predominantly single-task oriented and focused on isolated capabilities such as judgment prediction, statute classification, or legal entity recognition. A notable early contribution was CaseHOLD [25], which utilized the Harvard Law School case corpus to establish a standard for monitoring domain adaptation in Transformer models. However, the field has recently shifted toward comprehensive, multi-task benchmarks designed to evaluate general legal intelligence. A seminal contribution in this space is LexGLUE [1], which consolidated several English legal datasets into a unified benchmark modeled after the GLUE methodology to test language understanding across varied legal documents.

Building on this foundation, the LegalBench project [8] significantly expanded the scope of evaluation by introducing a collaborative suite of 162 tasks. Unlike its predecessors, LegalBench emphasizes legal reasoning over mere linguistic fluency by incorporating tasks such as issue spotting, rule application, and contractual interpretation. Concurrently, the rise of legally specialized LLMs,

such as ChatLaw [3] and Disc-LawLLM [23], has driven the need for benchmarks that can discern subtle hallucinations in generated legal advice. In the domain of Civil Law, similar initiatives have emerged for Chinese, most notably LawBench [6] and LAiW [4]. However, a significant disparity remains in the global landscape of legal NLP. While high-resource languages like English and Chinese utilize massive, monolithic corpora for benchmarking, low-resource languages and specific Civil Law jurisdictions in the Global South remain significantly underrepresented [16]. Recent cross-lingual initiatives often lack the depth required to capture the specific hierarchical statutes of jurisdictions like Vietnam. Consequently, the lack of a comparable and rigorous benchmark for Vietnamese law represents a significant gap in the global legal NLP landscape that this study aims to address.

# 2.2 Developments in Vietnamese Legal NLP

Research into Vietnamese Legal AI has historically been driven by community-organized shared tasks, particularly those hosted by the VLSP (Vietnamese Language and Speech Processing) consortium [15]. Over the past decade, these workshops have catalyzed the creation of datasets for fundamental tasks including Legal Information Retrieval (LIR), Legal Textual Entailment, and Question Answering. These initiatives successfully spurred the adaptation of pre-trained language models for the Vietnamese context. Models such as PhoBERT [13] have served as strong backbones for understanding tasks, while ViT5 [17] has provided a foundation for generative tasks like text summarization.

Despite this progress, existing Vietnamese legal resources remain fragmented. The majority of current datasets focus primarily on retrieval, which involves identifying relevant articles for a query, or extractive question answering, where the answer is a span of text within a provided document. There is a notable scarcity of resources that evaluate generative capabilities or complex reasoning tasks that require multi-hop traversals across amendments and decrees. Furthermore, recent work in Vietnamese Legal RAG [14] has focused on system architecture rather than standardized evaluation, highlighting the need for a unified benchmark like VLegal-Bench that explicitly tests the ability of a model to handle the hierarchical and evolving nature of Vietnamese statutes.

# 2.3 Benchmarking Retrieval-Augmented Generation and Legal Agents

As legal professionals increasingly rely on LLMs for research, the evaluation focus has expanded from static reasoning to Retrieval-Augmented Generation (RAG), where models must query external knowledge bases to answer questions accurately. While general benchmarks like RGB [2] evaluate RAG systems on open-domain queries, legal applications require far higher precision due to the risk of hallucinated precedents.

This need led to the recent release of LegalBench-RAG [18], which isolates the retrieval component of legal AI to measure how effectively models can pinpoint specific relevant clauses within massive corpora. However, the field is rapidly evolving beyond passive RAG toward autonomous Legal Agents capable of active tool use and planning [10]. While general frameworks for evaluating agents exist, there is a lack of benchmarks that specifically assess an

agent's ability to navigate legal databases dynamically simulating a lawyer's workflow of iterative search and verification. These works highlight a critical limitation in earlier benchmarks where a model might reason correctly given a snippet but fail to find that snippet in a real-world database. VLegal-Bench aligns with this trend by incorporating graph-structured retrieval tasks and ReAct-style agent settings, ensuring that models are evaluated not just on their internal knowledge but on their ability to navigate the complex citation network of the Vietnamese legal code.

# 2.4 Cognitive Taxonomies in Model Evaluation and Evaluation Metrics

As LLMs demonstrate increasingly sophisticated capabilities, the NLP community has moved beyond surface-level metrics like accuracy or ROUGE scores toward evaluations grounded in cognitive science. Recent studies have argued that effective benchmarking must distinguish between rote memorization and higher-order thinking. This has led to the adoption of Bloom's Taxonomy in general domain benchmarks, which categorizes tasks into levels ranging from Remembering to Creating. Additionally, the success of Chain-of-Thought (CoT) prompting [20] has highlighted the importance of evaluating the intermediate reasoning steps a model takes to reach a conclusion rather than solely checking the final output.

Furthermore, the metric selection for legal generation remains a contentious topic. While reference-based metrics like ROUGE and BLEU are standard, recent literature argues they correlate poorly with factual correctness in complex tasks [12]. Although emerging paradigms like "LLM-as-a-Judge" [24] offer semantic evaluation, they introduce new biases. Therefore, we ground our evaluation in a hybrid approach: utilizing rigorous extraction metrics (Accuracy/F1) for lower cognitive levels and standard generation metrics (ROUGE-L) for higher levels, while structuring the tasks themselves to minimize ambiguity.

In the legal domain, this cognitive layering is particularly critical. A model may successfully recall a specific Article yet fail to apply that article to a novel factual scenario or spot a conflict between two regulations. While general benchmarks like AGIEval [26] attempt to capture these nuances, they lack the domain-specific logic structures central to legal analysis, such as the IRAC (Issue, Rule, Application, Conclusion) framework. VLegal-Bench contributes to this methodological discussion by operationalizing Bloom's Taxonomy specifically for law. By structuring the dataset into five distinct cognitive levels, we provide a granular diagnosis of model performance. This allows researchers to pinpoint exactly where the legal reasoning of a model breaks down, whether at the level of basic statutory recall or at the complex stage of ethical judgment.

# 3 VLegal-Bench

# 3.1 Design Principle of VLegal-Bench

VLegal-Bench is structured around a hierarchical cognitive framework, inspired by Bloom's taxonomy and adapted to the linguistic and structural characteristics of Vietnamese law. It comprises five progressive levels of legal cognition, from basic factual recognition to advanced ethical reasoning, each reflecting increasing interpretive and reasoning complexity. The benchmark's tasks correspond to practical scenarios where LLMs can function as legal assistants

for citizens or support judicial decision-making. We provide an overview in Table 1 and detailed descriptions, along with concrete illustrative examples for each task, in Appendix D.

At its foundation, Level 1-Recognition & Recall targets fundamental legal literacy. This level is tailored to the Vietnamese legal context, requiring an LLM to accurately identify and retrieve key legal entities, topics, concepts, and statutory articles. Given the dense terminology and frequent cross-references characteristic of Vietnamese legislation, this level assesses whether a model can reliably capture and reproduce essential factual knowledge, an ability that serves as a prerequisite for deeper legal comprehension and reasoning. Tasks 1.1, 1.2, and 1.5 simulate scenarios in which LLMs process and extract basic information from legal documents, including legal entities, topics, and schema, while Tasks 1.3 and 1.4 model real-world interactions where citizens seek clarification regarding basic legal provisions and foundational legal concepts.

Building on this foundation, Level 2 - Understanding & Structuring evaluates an LLM's ability to comprehend and organize complex legal information. Vietnamese legal documents are often lengthy and structurally intricate, requiring the model to identify relationships between legal entities and the logical connections among Articles, Clauses, and Points. Additionally, as Vietnamese law evolves through frequent amendments and replacements, forming an implicit network of interconnected norms, this level assesses the model's capacity to represent and structure legal knowledge as a coherent and dynamic system. Tasks 2.1, 2.2, and 2.3 simulate higher-level analysis of legal texts, including identifying subject-object relationships, hypothetical conditions, sanctions, and interarticle connections in long documents. Tasks 2.4 and 2.5 simulate real-world scenarios where LLMs function as virtual legal assistants, understanding user queries and supporting the verification and explanation of judicial decisions.

Level 3 - Reasoning & Inference assesses the model's ability to apply legal provisions to factual scenarios through logical reasoning. Tasks at this level evaluate the model's capacity to predict relevant articles, infer judicial outcomes, estimate appropriate penalties or remedies, and perform multi-article reasoning across interconnected legal norms. This level reflects the practical reasoning skills essential for legal problem-solving and legislative drafting, where conclusions must be derived logically from multiple interrelated sources. Tasks 3.1, 3.2, 3.3, and 3.5 are designed to test the model's high-level inferential abilities, such as estimating penalties, grounding answers in specific legal texts, and synthesizing information from multiple legal sources. Task 3.4 evaluates the model's ability to assist courts in identifying and resolving conflicts or overlaps between statutes issued at different times.

Advancing further, Level 4 - Interpretation & Generation evaluates not only reasoning but also the model's generative and interpretive abilities. This level measures whether an LLM can produce coherent, contextually accurate, and unbiased legal texts including concise summaries of statutes and judgments, judicial reasoning, and balanced legal opinions. It is critical for assessing a model's effectiveness in real-world tasks such as legal drafting, summarization, and advisory work, which require both deep understanding and precise expression. Tasks 4.1, 4.2, and 4.3 simulate the use of LLMs as virtual legal assistants performing tasks that demand professional-level legal comprehension, such as summarizing key

information from lengthy statutes, analyzing legal scenarios using the IRAC framework, and generating well-reasoned legal opinions

Finally, Level 5 - Ethics, Fairness & Bias examines the normative and ethical dimensions of legal AI. This level evaluates whether the model's outputs adhere to standards of fairness, impartiality, and privacy protection, ensuring that generated conclusions are free from gender, political, or social bias. It is crucial for assessing the reliability of LLMs in sensitive or high-stakes legal contexts, where moral consistency and professional ethics are essential. Tasks 5.1, 5.2, 5.3, and 5.4 are designed to stress-test the model's ability to make legally appropriate decisions in scenarios involving bias, privacy concerns, ethical dilemmas, or potentially unfair contract clauses.

# 3.2 Data Collection and Processing

Data Collection. We illustrate the benchmark data construction pipeline in Figure 2. Legal documents are aggregated from multiple sources, including official government and state agency websites (e.g., agriculture, public security, education) for statutory texts, as well as citizen question-answer pairs collected from private datasets provided by Vietnamese law firms. In total, 55,000 centrally issued and currently effective legal documents are used to construct the benchmark. Each document is processed through HTML parsing, OCR, and related procedures to extract clean textual content. In parallel, we preserve the legal schema across documents to construct a knowledge graph database. After data collection, the data undergo a preprocessing stage, including deduplication and tokenization, followed by postprocessing steps such as topic classification and citation extraction. This pipeline produces two databases. The first is a knowledge graph database that stores relationships among Articles, Clauses, and Points, which supports the construction of multi-hop reasoning questions and the evaluation of conflicts and overlaps requiring information from multiple documents. The second is a legal corpus database organized in a key-value format, where keys correspond to document identifiers and hierarchical legal units, and values contain the associated text with metadata (e.g., promulgation date, effective date, and validity period), as well as real-world legal questions submitted by citizens on diverse legal topics.

Labelling Process. Based on the two constructed databases, we designed benchmark questions covering 22 tasks, following predefined cognitive levels and target sample sizes for each task (Table 1). The annotation process followed a structured, multi-stage expert-in-the-loop protocol. A senior legal expert (licensed lawyer with more than five years of professional experience in domains such as civil, criminal, and administrative law) acted as a supervisor, defining task-specific topics and identifying authoritative legal sources as grounding references. Relevant documents were retrieved from the databases, and task-specific raw data were prepared accordingly. For each task, the total sample quota was evenly divided between two independent junior legal experts (lawyers with 1-2 years of professional experience), acting as annotators Junior A and Junior B. Each annotator independently constructed realistic legal scenario questions and corresponding answers in either multiple-choice or open-ended formats for their assigned samples. To ensure annotation quality, a batch-wise cross-verification procedure was

<table><tr><td>Level</td><td>ID</td><td>Task</td><td>Purpose</td><td>Type</td><td>Metric</td><td>Test set</td></tr><tr><td rowspan="5">1. Recognition &amp; Recall</td><td>1.1</td><td>Legal Entity Recognition</td><td>To detect and classify named entities, including persons, organizations, monetary amounts, and dates, within legal documents.</td><td>MLC</td><td>Accuracy</td><td>750</td></tr><tr><td>1.2</td><td>Legal Topic Classification</td><td>Classifies legal questions into predefined legal topics.</td><td>MLC</td><td>Accuracy</td><td>700</td></tr><tr><td>1.3</td><td>Legal Concept Recall</td><td>Recalls statutory definitions or meanings of legal terms and concepts.</td><td>MLC</td><td>Accuracy</td><td>300</td></tr><tr><td>1.4</td><td>Article Recall</td><td>Retrieves or cites the correct legal article corresponding to a term, concept, or question.</td><td>MLC</td><td>Accuracy</td><td>1000</td></tr><tr><td>1.5</td><td>Legal Schema Recall</td><td>Recognizes and recalls hierarchical and temporal relations among legal documents (e.g., amendments, replacements, repeals)</td><td>MLC</td><td>Accuracy</td><td>800</td></tr><tr><td rowspan="5">2. Understanding &amp; Structuring</td><td>2.1</td><td>Relation Extraction</td><td>Extracts the subject, object, and content of a legal relationship from a factual scenario</td><td>Extraction</td><td>Accuracy</td><td>253</td></tr><tr><td>2.2</td><td>Legal Element Recognition</td><td>Identifies the hypothesis, disposition, and sanction components within a legal provision</td><td>Extraction</td><td>Accuracy</td><td>300</td></tr><tr><td>2.3</td><td>Legal Graph Structuring</td><td>Convert legal documents into structured knowledge graphs representing entities, relations, and inter-paragraph references.</td><td>Extraction</td><td>ROUGE-L</td><td>296</td></tr><tr><td>2.4</td><td>Judgment Verification</td><td>Evaluates whether a court&#x27;s reasoning or statement is consistent with the factual and legal content of the actual judgment.</td><td>BC</td><td>Accuracy</td><td>600</td></tr><tr><td>2.5</td><td>User Intent Understanding</td><td>Determines the underlying intent or query type of the user when interacting with a legal assistant.</td><td>MLC</td><td>macro-F1</td><td>1359</td></tr><tr><td rowspan="5">3. Reasoning &amp; Inference</td><td>3.1</td><td>Article / Clause Prediction</td><td>Predict which legal article or clause applies to a given legal question or short query, instead of a lengthy factual scenario</td><td>MLC</td><td>Accuracy</td><td>600</td></tr><tr><td>3.2</td><td>Legal Court Decision Prediction</td><td>Predicts the final court decision or judgment outcome from the factual and legal content of a real case.</td><td>MLC</td><td>Accuracy</td><td>600</td></tr><tr><td>3.3</td><td>Multi-Article Reasoning</td><td>Perform multi-step reasoning by connecting several legal provisions or facts within a knowledge graph to derive a consistent conclusion.</td><td>MLC</td><td>Accuracy</td><td>292</td></tr><tr><td>3.4</td><td>Conflict &amp; Consistency Detection</td><td>Identify contradictions or overlaps between different legal clauses or interpretations across statutes or contracts.</td><td>BC</td><td>Binary F1</td><td>161</td></tr><tr><td>3.5</td><td>Penalty / Remedy Estimation</td><td>Estimates the appropriate legal penalty or remedy for a given factual situation.</td><td>MLC</td><td>Accuracy</td><td>358</td></tr><tr><td rowspan="3">4. Interpretation &amp; Generation</td><td>4.1</td><td>Legal Document Summarization</td><td>Generate concise summaries of long legal texts (statutes, judgments, contracts) while preserving key information.</td><td>Generation</td><td>ROUGE-L</td><td>384</td></tr><tr><td>4.2</td><td>Judicial Reasoning Generation</td><td>Produce structured reasoning paragraphs based on the IRAC template (Issue - Rule - Application - Conclusion) that mirror judicial writing style.</td><td>Generation</td><td>ROUGE-L</td><td>299</td></tr><tr><td>4.3</td><td>Objective Legal Opinion Generation</td><td>Generate a balanced and impartial legal opinion or advisory text that aligns with statutory interpretation.</td><td>Generation</td><td>ROUGE-L</td><td>498</td></tr><tr><td rowspan="4">5. Ethics, Fairness &amp; Bias</td><td>5.1</td><td>Bias Detection</td><td>Detect gender, racial, political, or religious bias in generated answers or decisions to ensure fairness.</td><td>MLC</td><td>Accuracy</td><td>250</td></tr><tr><td>5.2</td><td>Privacy &amp; Data Protection</td><td>Identify and redact sensitive or personal data in legal texts to ensure privacy compliance.</td><td>MLC</td><td>Accuracy</td><td>216</td></tr><tr><td>5.3</td><td>Ethical Consistency Assessment</td><td>Evaluate whether the model&#x27;s outputs align with professional ethics and moral standards in legal reasoning.</td><td>MLC</td><td>Accuracy</td><td>200</td></tr><tr><td>5.4</td><td>Unfair Contract Detection</td><td>Compare model judgments across similar cases or parties to assess impartiality and equitable reasoning.</td><td>MLC</td><td>Accuracy</td><td>234</td></tr></table>

Table 1: Overview of VLegal-Bench: The benchmark evaluates legal LLMs across five levels, from basic recognition to ethical reasoning, using four question templates: Multi-Label Classification (MLC), Binary Classification (BC), extraction, and generation for Vietnamese law.

implemented: after every 100 samples were annotated, Junior A and Junior B exchanged their respective batches and independently selected answers for each other's questions in a blind manner, without knowledge of the original annotator's answers.

Verifying Process. Inter-annotator agreement was assessed using both percentage agreement and Cohen's Kappa coefficient. The agreement was measured as the proportion of samples for which both annotators assigned identical labels before any discussion or

revision. Across the 10.450 samples, the initial agreement reached  $92.39\%$  (9,656/10.450), with a Cohen's Kappa of 0.89, indicating strong consistency and substantial agreement beyond chance. For the remaining  $7.61\%$  (794/10.450) of samples with initial disagreement, a two-stage resolution procedure was employed. In the first stage, the two junior annotators conducted a structured discussion, exchanged reasoning, and jointly re-examined the relevant legal sources. This process successfully resolved 683 cases through

![](images/2707aacc9263652c7ffb1d595f3eab9d52bf4c9438ed64a262655a063879c921.jpg)  
Figure 2: Legal benchmark data pipeline: data are collected from Vietnamese legal sources, preprocessed, stored in a database, and used to build an information retrieval tool for legal experts. The final dataset is obtained through manual annotation.

mutual consensus. In the second stage, the remaining 111 cases were escalated to the supervising senior legal expert, who reviewed both annotators' arguments, consulted authoritative legal sources, and issued a final binding decision. Unresolved cases that required senior adjudication were concentrated in tasks involving complex reasoning and nuanced legal judgment, particularly Conflict and Consistency Detection (Task3.4, 31 cases), Multi-Article Reasoning (Task3.3, 27 cases), and Unfair Contract Detection (Task 5.4, 22 cases). These tasks typically require synthesizing information across multiple legal provisions, resolving subtle statutory conflicts, and applying normative judgment to contractual terms, which naturally admit multiple defensible interpretations even among trained legal professionals.

During the annotation process, dedicated retrieval tools were provided to support efficient search and verification within the constructed databases. The final benchmark consists of 10.450 expert-validated legal instances, each explicitly grounded in authoritative legal sources. Details of the annotation support tool are provided in Appendix A, and annotator recruitment and training procedures are described in Appendix C.

# 4 Experiments and Results

# 4.1 Experiment Setting

We evaluate large language models under both zero-shot and few-shot settings. In the zero-shot configuration, models receive only task instructions and the input query, while in the few-shot setting, we prepend a single task-specific demonstration example to each input instance. These demonstration examples are drawn from a separate development set and are not included in the reported test set. For both settings, we conduct evaluations with and without explicit reasoning: models are prompted either to directly produce a final answer or to generate intermediate reasoning using a chain-of-thought (CoT) prompt before outputting the final response. To ensure reproducibility, we fix the decoding temperature to 0, thereby minimizing variance introduced by stochastic sampling. The complete set of evaluation prompts is provided in Appendix D.

Our tasks are designed to simulate realistic legal scenarios in which the LLM acts as a legal assistant or as the core model within a retrieval-augmented generation (RAG) system. In addition to standard prompting, we evaluate a ReAct-style agent setting [22], where the LLM is equipped with a search tool to retrieve relevant information from both the Legal Corpus Database and the Knowledge

Graph Database during inference. We adopt standardized evaluation metrics across tasks: Accuracy, F1 for multiple-choice and extraction-style questions, and ROUGE-L for generation-level tasks. When the input length exceeds the maximum context window of an LLM, we apply middle truncation to the input sequence, as both the beginning and the end of legal texts often contain critical information. This truncation strategy follows prior legal LLM benchmarks [4, 6, 11].

# 4.2 Evaluated Model

We evaluate VLegal-Bench using a total of 24 Large Language Models (LLMs), which are categorized into two main groups: general multilingual LLMs and domain-adapted Vietnamese LLMs. The general multilingual LLMs include both closed-source APIs, namely GPT-4o, GPT-4o-mini, Claude 4.5 Haiku, Claude 4.5 Sonnet, Gemini 2.5 Flash, and Gemini 2.5 Pro, as well as open-source models, including Qwen 2.5 Instruct (3B/7B/14B/32B/72B), Qwen 2.5 (3B/7B), Llama 2 Chat (7B/13B), Meta Llama 3 Instruct (8B/70B), InternLM 3 8B Instruct, and Gemma 2 Instruct (9B/27B). The Vietnamese-focused models include SeaLLMs v3 Chat (1.5B/7B), BloomVN 8B Chat, and a Vietnamese legal domain-adapted model, VLSP2025-LegalSML 4B. We provide detailed information about these models in Appendix B.

# 4.3 Overall Performance Analysis

Table 2 and Table 3 present the zero-shot performance of 23 language models across all 22 tasks in VLegal-Bench. Our results reveal several critical insights into the current state of legal AI for Vietnamese law.

Performance Degradation Across Cognitive Levels. We observe a dramatic performance decline as tasks increase in cognitive complexity. While top-performing models achieve  $80 - 90\%$  accuracy on basic recognition tasks (Level 1), performance drops precipitously on advanced reasoning tasks. Most notably, Task 3.1 (Article/Clause Prediction) proves exceptionally challenging, with the best model achieving only  $43.83\%$  accuracy a decline of over 40 percentage points compared to the simpler Task 1.4 (Article Recall at  $87.91\%$ ). This suggests that while models can recall legal articles when explicitly prompted, applying this knowledge to predict relevant provisions from novel queries remains fundamentally difficult.

The Conflict Detection Catastrophe. Task 3.4 (Conflict & Consistency Detection) reveals a systematic failure across nearly all models. Of the 23 models evaluated, 16 achieve 0.00 Y-F1 score, indicating complete inability to detect legal conflicts. These models exhibit strong bias toward predicting "no conflict," achieving  $39 - 46\%$  N-F1 while failing entirely on positive cases. Only three models demonstrate any conflict detection capability: CMC-AI-Legal-32B (86.41 Y-F1), Llama-3.1-8B-Instruct (37.66 Y-F1), and GPT-4o (27.21 Y-F1). The stark 86.41 vs. 0.00 gap between CMC-AI-Legal-32B and most other models underscores the critical importance of domain-specific training for complex legal reasoning tasks.

# 4.4 Domain Adaptation vs. Scale

Triumph of Specialized Models. Our results challenge the conventional wisdom that larger general-purpose models necessarily outperform smaller domain-adapted alternatives. The domain-adapted Vietnamese legal models demonstrate remarkable effectiveness, particularly on higher-level cognitive tasks. CMC-AI-Legal-32B achieves state-of-the-art performance on four critical reasoning tasks: court decision prediction (90.67%), multi-article reasoning (76.71%), conflict detection (86.41 Y-F1), and unfair contract detection (73.50%). Notably, it surpasses GPT-4o by 6.17 percentage points on court decision prediction and by 15.39 points on unfair contract detection.

Even more striking is the performance of qwen3-4b-legal-pretrain, a 4-billion parameter model that achieves best-in-class results on article prediction (43.83%) and document summarization (0.4361 ROUGE-L), outperforming models up to 18 times its size. This demonstrates that targeted legal domain pretraining provides greater benefits than raw parameter scaling for specialized legal tasks.

The Diminishing Advantage of Proprietary Models. While proprietary models (GPT-4o, Claude Sonnet 4.5, Gemini 2.5 Flash) maintain advantages on foundational tasks (Levels 1-2), their superiority diminishes substantially on advanced reasoning and generation tasks (Levels 3-5). GPT-4o achieves best overall performance on only two high-level tasks: penalty estimation  $(67.97\%)$  and legal opinion generation (0.4975 ROUGE-L). Across the remaining six tasks in Levels 3-5, domain-adapted Vietnamese models outperform GPT-4o, with particularly large gaps on conflict detection  $(+59.20$  points) and court decision prediction  $(+6.17$  points). This pattern suggests that general-purpose training, even at a massive scale, cannot fully substitute for domain-specific legal knowledge when tackling complex juridical reasoning.

# 4.5 Task-Specific Insights

Recognition & Understanding (Levels 1-2). On foundational tasks, we observe strong performance across most models. Legal topic classification (Task 1.2) proves relatively accessible, with top models achieving  $81 - 86\%$  accuracy. However, Task 1.5 (Legal Schema Recall) emerges as a universal bottleneck, with all models scoring below  $28\%$ . This suggests fundamental limitations in understanding Vietnamese legal document hierarchies, including amendments, replacements, and temporal relationships a critical capability for practical legal AI systems.

Among understanding tasks, Claude Sonnet 4.5 demonstrates exceptional ability in structured knowledge extraction, achieving 0.808 ROUGE-L on legal graph structuring (Task 2.3) and  $87.81\%$  on judgment verification (Task 2.4). Its performance significantly exceeds other proprietary models, highlighting architectural advantages for complex legal document analysis.

Reasoning & Inference (Level 3). Level 3 tasks expose sharp distinctions between model capabilities. Court decision prediction (Task 3.2) shows surprisingly strong performance (82-90% for top models), suggesting that predicting judicial outcomes may be more pattern-based than previously assumed. Conversely, article prediction from short queries (Task 3.1) remains largely unsolved, with accuracy ranging from 19.67% to 43.83%. The wide performance

<table><tr><td rowspan="2">Model Type</td><td rowspan="2">Model</td><td colspan="5">Recognition &amp; Recall</td><td colspan="5">Understanding &amp; Structuring</td></tr><tr><td>1.1 Acc</td><td>1.2 Acc</td><td>1.3 Acc</td><td>1.4 Acc</td><td>1.5 Acc</td><td>2.1 Acc</td><td>2.2 Acc</td><td>2.3 R-L</td><td>2.4 Acc</td><td>2.5 m-F1</td></tr><tr><td rowspan="20">General Multilingual LLMs</td><td>gpt-4o</td><td>70.03</td><td>81.14</td><td>73.67</td><td>82.50</td><td>24.25</td><td>85.37</td><td>67.33</td><td>0.470</td><td>80.67</td><td>63.28</td></tr><tr><td>gpt-4o-mini</td><td>65.24</td><td>82.71</td><td>61.33</td><td>68.40</td><td>22.25</td><td>87.35</td><td>51.33</td><td>0.529</td><td>73.33</td><td>61.68</td></tr><tr><td>claude-sonnet-4.5</td><td>69.78</td><td>82.47</td><td>83.00</td><td>84.19</td><td>27.25</td><td>79.21</td><td>75.33</td><td>0.808</td><td>87.81</td><td>62.04</td></tr><tr><td>gemini-2.5-flash</td><td>71.96</td><td>81.40</td><td>61.33</td><td>81.40</td><td>24.25</td><td>80.63</td><td>64.00</td><td>0.656</td><td>82.30</td><td>49.92</td></tr><tr><td>gpt-oss-20b</td><td>22.72</td><td>73.21</td><td>47.33</td><td>39.00</td><td>18.63</td><td>45.06</td><td>28.33</td><td>0.249</td><td>66.50</td><td>56.43</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>68.05</td><td>80.43</td><td>77.00</td><td>79.10</td><td>21.88</td><td>80.78</td><td>65.67</td><td>0.808</td><td>83.83</td><td>23.58</td></tr><tr><td>Qwen2.5-32B-Instruct</td><td>71.25</td><td>85.50</td><td>71.33</td><td>79.70</td><td>22.25</td><td>79.21</td><td>67.00</td><td>0.759</td><td>80.00</td><td>56.99</td></tr><tr><td>Qwen2.5-14B-Instruct</td><td>68.58</td><td>82.28</td><td>65.67</td><td>74.60</td><td>20.50</td><td>85.38</td><td>55.53</td><td>0.733</td><td>78.17</td><td>58.96</td></tr><tr><td>Qwen2.5-7B-Instruct</td><td>50.40</td><td>79.94</td><td>54.00</td><td>62.40</td><td>21.75</td><td>85.37</td><td>56.67</td><td>0.657</td><td>82.80</td><td>63.28</td></tr><tr><td>Qwen2.5-3B-Instruct</td><td>52.80</td><td>70.57</td><td>50.67</td><td>57.95</td><td>25.38</td><td>72.73</td><td>48.00</td><td>0.606</td><td>67.95</td><td>51.04</td></tr><tr><td>Llama-3.1-70B-Instruct</td><td>55.68</td><td>80.67</td><td>74.33</td><td>77.79</td><td>24.38</td><td>75.68</td><td>58.67</td><td>0.516</td><td>81.80</td><td>58.12</td></tr><tr><td>Llama-3.1-8B-Instruct</td><td>55.88</td><td>80.67</td><td>56.00</td><td>62.40</td><td>25.75</td><td>74.11</td><td>50.00</td><td>0.364</td><td>72.95</td><td>51.31</td></tr><tr><td>Llama-2-13b-chat-hf</td><td>22.72</td><td>21.23</td><td>21.00</td><td>42.20</td><td>27.75</td><td>56.10</td><td>30.24</td><td>0.411</td><td>48.00</td><td>41.81</td></tr><tr><td>Llama-2-7b-chat-hf</td><td>25.52</td><td>17.28</td><td>19.67</td><td>19.20</td><td>25.50</td><td>51.22</td><td>27.02</td><td>0.198</td><td>49.33</td><td>44.63</td></tr><tr><td>gemma-2-27b-it</td><td>57.89</td><td>78.62</td><td>58.67</td><td>72.31</td><td>24.13</td><td>73.83</td><td>56.33</td><td>0.719</td><td>83.97</td><td>55.39</td></tr><tr><td>gemma-2-9b-it</td><td>52.94</td><td>77.30</td><td>65.00</td><td>65.60</td><td>23.25</td><td>79.45</td><td>48.00</td><td>0.349</td><td>79.47</td><td>48.65</td></tr><tr><td>SeaLLMs-v3-7B-Chat</td><td>62.23</td><td>68.96</td><td>57.67</td><td>58.88</td><td>22.50</td><td>76.28</td><td>49.67</td><td>0.475</td><td>62.10</td><td>54.39</td></tr><tr><td>SeaLLMs-v3-1.5B-Chat</td><td>47.73</td><td>49.04</td><td>55.33</td><td>39.00</td><td>25.75</td><td>42.69</td><td>27.33</td><td>0.576</td><td>48.50</td><td>47.21</td></tr><tr><td>internlm3-8b-instruct</td><td>57.21</td><td>46.27</td><td>51.00</td><td>55.48</td><td>24.50</td><td>68.38</td><td>42.67</td><td>0.395</td><td>67.28</td><td>52.30</td></tr><tr><td>internlm-chat-20b</td><td>16.43</td><td>24.01</td><td>17.91</td><td>21.70</td><td>18.13</td><td>11.11</td><td>9.00</td><td>0.188</td><td>61.17</td><td>32.73</td></tr><tr><td rowspan="3">Domain-adapted Vietnamese LLMs</td><td>BloomVN-8B-chat</td><td>46.66</td><td>65.59</td><td>63.67</td><td>65.29</td><td>26.75</td><td>70.36</td><td>45.00</td><td>0.500</td><td>49.08</td><td>57.00</td></tr><tr><td>VLSP2025-qwen3-4b-legal-pretrain</td><td>62.83</td><td>75.99</td><td>66.67</td><td>70.35</td><td>24.50</td><td>79.45</td><td>45.00</td><td>0.716</td><td>73.29</td><td>58.69</td></tr><tr><td>CMC-AI-Legal-32B</td><td>66.17</td><td>84.19</td><td>81.67</td><td>87.91</td><td>24.13</td><td>80.78</td><td>61.33</td><td>0.694</td><td>87.45</td><td>60.58</td></tr></table>

Table 2: Zero-shot Performance Comparison of Language Models across Different Tasks. Red bold indicates the best overall performance. Blue bold indicates the best performance among open-source models.

gap between these tasks despite both requiring legal reasoning indicates that current models excel at matching factual scenarios to outcomes but struggle with mapping queries to legal provisions without explicit context.

Penalty estimation (Task 3.5) reveals interesting architectural differences: GPT-4o  $(67.97\%)$  substantially outperforms all open-source alternatives, with the next-best model (Qwen2.5-72B) achieving  $63.51\%$ . This suggests that nuanced legal judgment, particularly regarding proportionality and discretionary sentencing, remains an area where frontier proprietary models maintain clear advantages.

Generation & Ethics (Levels 4-5). Generation tasks (Level 4) show moderate performance across top models, with ROUGE-L scores ranging from 0.30 to 0.50. Legal opinion generation (Task 4.3) proves most tractable, while document summarization (Task 4.1) and IRAC-style judicial reasoning (Task 4.2) show higher variance. Notably, older model families (Llama-2, internlm-chat-20b) fail catastrophically on generation tasks, achieving ROUGE-L scores below 0.10, highlighting the importance of recent architectural improvements and instruction-tuning methods.

Ethics and fairness tasks (Level 5) reveal an intriguing pattern: models demonstrate strong ethical consistency (Task 5.3, with most achieving  $>85\%$ ) but struggle with bias detection (Task 5.1, ranging  $15 - 58\%$ ). This disparity suggests that following explicit ethical rules is learnable from general pretraining, whereas detecting subtle biases in legal contexts requires domain-specific sensitivity. Qwen2.5-14B achieves the best bias detection performance  $(57.79\%)$ ,

surpassing larger models in its family and even GPT-4o  $(44.18\%)$ , indicating that bias detection may benefit from specific architectural or training choices rather than pure scale.

# 4.6 Model Family Analysis

Qwen2.5 Series. The Qwen2.5 family demonstrates remarkably consistent performance across model sizes, with the 32B variant surprisingly achieving the best legal topic classification  $(85.50\%)$ . However, all Qwen2.5 models exhibit complete failure on conflict detection (0.00 Y-F1), suggesting a systematic architectural or training-related limitation. The family shows strong generation capabilities (0.40-0.48 ROUGE-L) and achieves competitive performance on most classification and reasoning tasks, establishing it as a strong baseline for Vietnamese legal AI.

Llama Series. The Llama-3.1 family presents mixed results. While Llama-3.1-70B achieves the highest ethical consistency score  $(93.99\%)$ , its 8B variant exhibits unique capabilities in conflict detection, with balanced Y-F1/N-F1 scores (37.66/46.07) the only model besides CMC-AI-Legal to show genuine conflict detection ability. This suggests architectural differences that warrant further investigation. In stark contrast, the older Llama-2 models fail comprehensively across all advanced tasks, with accuracy often below  $30\%$  and generation scores below 0.10 ROUGE-L, illustrating rapid progress in LLM capabilities for specialized domains.

<table><tr><td rowspan="2">Model Type</td><td rowspan="2">Model</td><td colspan="5">Reasoning &amp; Inference</td><td colspan="3">Interpretation &amp; Generation</td><td colspan="4">Ethics, Fairness &amp; Bias</td></tr><tr><td>3.1 Acc</td><td>3.2 Acc</td><td>3.3 Acc</td><td>3.4 Y-F1/N-F1</td><td>3.5 Acc</td><td>4.1 R-L</td><td>4.2 R-L</td><td>4.3 R-L</td><td>5.1 Acc</td><td>5.2 Acc</td><td>5.3 Acc</td><td>5.4 Acc</td></tr><tr><td rowspan="18">General Multilingual LLMs</td><td>gpt-4o</td><td>38.83</td><td>84.50</td><td>73.50</td><td>27.21/42.16</td><td>67.97</td><td>0.3257</td><td>0.4017</td><td>0.4975</td><td>44.18</td><td>67.74</td><td>91.04</td><td>58.11</td></tr><tr><td>gpt-4o-mini</td><td>35.33</td><td>82.17</td><td>74.66</td><td>11.85/39.59</td><td>57.38</td><td>0.3272</td><td>0.4167</td><td>0.4926</td><td>41.76</td><td>67.74</td><td>86.56</td><td>51.28</td></tr><tr><td>gpt-oss-20b</td><td>29.67</td><td>66.00</td><td>52.74</td><td>73.41/35.56</td><td>37.15</td><td>0.0262</td><td>0.1529</td><td>0.3104</td><td>21.69</td><td>58.80</td><td>33.76</td><td>28.20</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>33.50</td><td>85.50</td><td>74.32</td><td>0.00/39.61</td><td>63.51</td><td>0.2930</td><td>0.4076</td><td>0.4825</td><td>46.59</td><td>67.28</td><td>90.55</td><td>64.53</td></tr><tr><td>Qwen2.5-32B-Instruct</td><td>32.66</td><td>82.67</td><td>74.66</td><td>0.00/39.61</td><td>59.78</td><td>0.3111</td><td>0.3968</td><td>0.4652</td><td>45.78</td><td>69.12</td><td>92.07</td><td>66.23</td></tr><tr><td>Qwen2.5-14B-Instruct</td><td>39.67</td><td>82.17</td><td>71.92</td><td>1.59/39.81</td><td>53.35</td><td>0.2707</td><td>0.0168</td><td>0.4050</td><td>57.79</td><td>67.59</td><td>91.54</td><td>68.38</td></tr><tr><td>Qwen2.5-7B-Instruct</td><td>35.83</td><td>81.67</td><td>71.23</td><td>0.00/39.61</td><td>56.15</td><td>0.2531</td><td>0.3286</td><td>0.4680</td><td>36.94</td><td>60.65</td><td>89.57</td><td>68.37</td></tr><tr><td>Qwen2.5-3B-Instruct</td><td>26.67</td><td>68.67</td><td>69.86</td><td>10.61/41.00</td><td>45.81</td><td>0.2586</td><td>0.3605</td><td>0.4082</td><td>36.94</td><td>54.17</td><td>83.59</td><td>59.40</td></tr><tr><td>Llama-3.1-70B-Instruct</td><td>35.00</td><td>86.00</td><td>75.34</td><td>0.00/39.61</td><td>62.11</td><td>0.3077</td><td>0.4007</td><td>0.4005</td><td>43.77</td><td>64.05</td><td>93.99</td><td>55.59</td></tr><tr><td>Llama-3.1-8B-Instruct</td><td>28.00</td><td>78.67</td><td>66.44</td><td>37.66/46.07</td><td>52.92</td><td>0.3160</td><td>0.3776</td><td>0.3879</td><td>37.35</td><td>49.31</td><td>91.01</td><td>62.82</td></tr><tr><td>Llama-2-13b-chat-hf</td><td>21.67</td><td>45.67</td><td>32.53</td><td>85.91/0.00</td><td>30.79</td><td>0.0149</td><td>0.0759</td><td>0.0478</td><td>21.69</td><td>28.24</td><td>27.66</td><td>27.35</td></tr><tr><td>Llama-2-7b-chat-hf</td><td>21.83</td><td>29.67</td><td>23.63</td><td>33.17/0.00</td><td>33.62</td><td>0.0132</td><td>0.2693</td><td>0.0318</td><td>20.08</td><td>26.85</td><td>29.17</td><td>21.37</td></tr><tr><td>gemma-2-27b-it</td><td>31.67</td><td>81.50</td><td>73.63</td><td>0.00/39.61</td><td>60.45</td><td>0.2971</td><td>0.3672</td><td>0.4654</td><td>40.16</td><td>57.60</td><td>89.07</td><td>69.66</td></tr><tr><td>gemma-2-9b-it</td><td>28.83</td><td>80.83</td><td>72.95</td><td>62.63/44.78</td><td>49.44</td><td>0.3272</td><td>0.3600</td><td>0.4678</td><td>40.16</td><td>59.26</td><td>91.05</td><td>62.82</td></tr><tr><td>SeaLLMs-v3-7B-Chat</td><td>26.00</td><td>81.50</td><td>64.38</td><td>0.00/39.61</td><td>49.16</td><td>0.1700</td><td>0.3547</td><td>0.4141</td><td>39.76</td><td>56.94</td><td>92.53</td><td>63.25</td></tr><tr><td>SeaLLMs-v3-1.5B-Chat</td><td>26.67</td><td>55.00</td><td>44.52</td><td>85.91/0.00</td><td>34.92</td><td>0.2180</td><td>0.2830</td><td>0.4117</td><td>25.30</td><td>31.02</td><td>68.20</td><td>57.26</td></tr><tr><td>internlm3-8b-instruct</td><td>26.67</td><td>71.00</td><td>65.07</td><td>40.96/8.33</td><td>43.02</td><td>0.2647</td><td>0.2694</td><td>0.2884</td><td>15.62</td><td>58.80</td><td>67.37</td><td>42.31</td></tr><tr><td>internlm-chat-20b</td><td>19.67</td><td>37.17</td><td>23.29</td><td>64.92/0.00</td><td>32.59</td><td>0.0636</td><td>0.2575</td><td>0.2917</td><td>29.31</td><td>23.96</td><td>42.79</td><td>26.07</td></tr><tr><td rowspan="3">Domain-adapted Vietnamese LLMs</td><td>BloomVN-8B-chat</td><td>32.17</td><td>82.33</td><td>70.89</td><td>0.00/39.61</td><td>50.00</td><td>0.2407</td><td>0.3188</td><td>0.4099</td><td>20.10</td><td>47.22</td><td>86.61</td><td>58.55</td></tr><tr><td>qwen3-4b-legal-pretrain</td><td>43.83</td><td>82.00</td><td>76.37</td><td>0.00/39.61</td><td>54.47</td><td>0.4361</td><td>0.3737</td><td>0.4381</td><td>34.54</td><td>61.57</td><td>89.08</td><td>59.40</td></tr><tr><td>CMC-AI-Legal-32B</td><td>41.67</td><td>90.67</td><td>76.71</td><td>86.41/13.33</td><td>62.40</td><td>0.2917</td><td>0.4213</td><td>0.3695</td><td>32.93</td><td>60.83</td><td>92.06</td><td>73.50</td></tr></table>

Table 3: Zero-shot performance comparison across different tasks. Red bold denotes the best overall result, while blue bold indicates the best-performing open-source model. For Task 3.4, we report F1 scores separately for the Yes and No labels (Y-F1 / N-F1).

Domain-Adapted Vietnamese Models. The three domain-adapted Vietnamese models show increasing returns with specialization. BloomVN-8B-chat, a general Vietnamese model, achieves moderate performance comparable to similarly-sized general multilingual models. VLSP2025-qwen3-4b-legal-pretrain demonstrates the power of legal domain pretraining, achieving best-in-class performance on article prediction and summarization despite having only 4 billion parameters. CMC-AI-Legal-32B represents the most specialized model, with legal-specific fine-tuning that yields dominant performance across reasoning and inference tasks (Level 3), achieving state-of-the-art results on four out of five tasks. This progression general Vietnamese  $\rightarrow$  legal-pretrained  $\rightarrow$  legal-finetuned validates a staged approach to building effective legal AI for low-resource languages.

# 4.7 Key Takeaways

Our experimental results yield four primary conclusions: (1) Current LLMs face fundamental challenges in advanced legal reasoning, with article prediction and conflict detection remaining largely unsolved; (2) Domain-specific pretraining provides greater benefits than parameter scaling for complex legal tasks, as evidenced by small specialized models outperforming general models 10-18 times their size; (3) Proprietary model advantages diminish substantially on high-level legal reasoning tasks, where domain-adapted models achieve superior performance; and (4) Certain capabilities particularly bias detection and legal schema understanding remain

challenging for all models, representing critical areas for future research in legal AI.

# 5 Conclusion

Based on the presented analyses and experimental results, VLegal-Bench represents a significant advancement in benchmarking large language models for the Vietnamese legal domain. The benchmark effectively addresses a critical gap in existing evaluation frameworks by introducing a civil-law-oriented, cognition-based benchmark tailored to Vietnam's codified legal system. Through its rigorous design incorporating hierarchical statutory structures, scenario-based legal tasks, and a Bloom's taxonomy-driven cognitive framework VLegal-Bench enables a transparent and systematic assessment of LLM capabilities ranging from basic legal recall to advanced legal reasoning. The release of a high-quality dataset comprising 10,450 expert-verified samples further strengthens the benchmark's reproducibility and extensibility to other civil law jurisdictions. Experimental findings reveal that while current LLMs perform adequately on low-level cognitive tasks, they continue to struggle with complex legal reasoning and cross-referential statutory interpretation, underscoring inherent limitations in general-purpose models. Overall, VLegal-Bench serves not only as a robust evaluation standard, but also as a foundational resource that guides future research and development of more reliable and legally competent large language models for Vietnamese law and other codified legal systems.

# References

[1] Ilias Chalkidis, Abhik Jana, Dirk Hartung, Michael Bommarito, Ion Androutsopoulos, Daniel Martin Katz, and Nikolaos Aletras. 2022. LexGLUE: A Benchmark Dataset for Legal Language Understanding in English. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 4310-4330.  
[2] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2023. Benchmarking large language models in retrieval-augmented generation. arXiv preprint arXiv:2309.01431 (2023).  
[3] Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. 2023. ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases. arXiv preprint arXiv:2306.16092 (2023).  
[4] Yongfu Dai, Duanyu Feng, Jimin Huang, Haochen Jia, Qianqian Xie, Yifang Zhang, Weiguang Han, Wei Tian, and Hao Wang. 2025. LAIW: A Chinese legal large language models benchmark. In Proceedings of the 31st International conference on computational linguistics. 10738-10766.  
[5] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. 2024. From local to global: A graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130 (2024).  
[6] Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Alan Huang, Songyang Zhang, Kai Chen, Zhixin Yin, Zongwen Shen, et al. 2024. Lawbench: Benchmarking legal knowledge of large language models. In Proceedings of the 2024 conference on empirical methods in natural language processing. 7933-7962.  
[7] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 2, 1 (2023).  
[8] Neel Guha, Julian Nyarko, Daniel E Ho, Christopher Re, et al. 2023. LegalBench: A Collaboratively Built Benchmark for Legal Reasoning. In Advances in Neural Information Processing Systems, Vol. 36.  
[9] PÃ©ter Homoki and Zsolt ZÃ¶di. 2024. Large language models and their possible uses in law. Hungarian Journal of Legal Studies 64, 3 (2024), 435-455.  
[10] Quzhe Huang, Mingxu Tao, Chen Zhang, Zhenwei An, Cong Jiang, Zhibin Chen, Zirui Wu, and Yansong Feng. 2023. Lawyer LLaMA Technical Report. arXiv:2305.15062 [cs.CL]  
[11] Haitao Li, You Chen, Qingyao Ai, Yueyue Wu, Ruizhe Zhang, and Yiqun Liu. 2024. Lexeval: A comprehensive chinese legal benchmark for evaluating large language models. Advances in Neural Information Processing Systems 37 (2024), 25061-25094.  
[12] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Singapore, 2511â€“2522. doi:10.18653/v1/2023.emnlp-main.153  
[13] Dat Quoc Nguyen and Anh Tuan Nguyen. 2020. PhoBERT: Pre-trained language models for Vietnamese. In Findings of the Association for Computational Linguistics: EMNLP 2020. 1037-1042.  
[14] Minh Nguyen, Viet Tran, and Huong Le. 2024. Vietnamese Legal Information Retrieval in Question-Answering System. arXiv preprint arXiv:2409.13699 (2024).  
[15] Minh-Tien Nguyen, Truong-Son Nguyen, Dac-Viet Lai, et al. 2021. VLSP 2021 Challenge: Vietnamese Legal Text Processing. In Proceedings of the 8th International Workshop on Vietnamese Language and Speech Processing.  
[16] Joel Niklaus, Danielle Matache, Theodor WÃ¼rmli, Peter Hofer, and Ilias Chalkidis. 2023. MultiLegalPile: A 689GB Multilingual Legal Corpus for Training Large Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 1-14.  
[17] Long Phan, Hieu Tran, Hieu Nguyen, and Trieu H Trinh. 2022. ViT5: Pre-trained Text-to-Text Transformer for Vietnamese Language Generation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop. 136â€“142.  
[18] Nicholas Pipitone and Ghita Hourir Alami. 2024. LegalBench-RAG: A Benchmark for Retrieval-Augmented Generation in the Legal Domain. arXiv preprint arXiv:2408.09543 (2024).  
[19] Zhongxiang Sun. 2023. A short survey of viewing large language models in legal aspect. arXiv preprint arXiv:2303.09136 (2023).  
[20] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, Vol. 35. 24824-24837.  
[21] Xiaoxian Yang, Zhifeng Wang, Qi Wang, Ke Wei, Kaiqi Zhang, and Jiangang Shi. 2024. Large language models for automated q&a involving legal documents: a survey on algorithms, frameworks and applications. International Journal of Web Information Systems 20, 4 (2024), 413-435.  
[22] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations.

[23] Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, et al. 2023. Disc-LawLLM: Fine-tuning Large Language Models for Effective Legal Reasoning. arXiv preprint arXiv:2309.11325 (2023).  
[24] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. In Advances in Neural Information Processing Systems. arXiv:2306.05685 [cs.CL] https://openreview.net/forum?id=uccHPGDlao NeurIPS 2023 Datasets and Benchmarks Track.  
[25] Lucia Zheng, Neel Guha, Brandon R Anderson, Peter Henderson, and Daniel E Ho. 2021. When Does Pretraining Help? Assessing Self-Supervised Learning for Law and the CaseHOLD Dataset. In Proceedings of the 18th International Conference on Artificial Intelligence and Law. 159-168.  
[26] Wanjun Zhong, Ruixiang Cui, Yidong Guo, Yaobo Wang, et al. 2023. AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. arXiv preprint arXiv:2304.06364 (2023).

# A Details of Annotation System

After constructing the Legal Corpus Database and the Knowledge Graph Database to store currently effective legal documents, covering a wide range of legal topics and real-world citizen questions submitted to law offices, we developed search and retrieval tools to support legal experts in efficiently locating relevant Articles, Clauses, Points, and legal topics. These tools are used to create task-specific raw data and to annotate samples into either multiple-choice or generative question formats for each benchmark task. Illustrations of the tools are shown in Figure 3 and Figure 4.

# B Details of Tested LLMs

Table 4 summarizes detailed information about the LLMs evaluated on VLegal-Bench.

# C Details of Labelling Process

Our annotation team comprised three senior legal experts and eight junior legal experts, all recruited through partnerships with two Vietnamese law firms and one university law faculty. Senior experts (Teachers) were required to hold a valid Vietnamese lawyer's license with a minimum of five years of professional practice, specialization in at least one of the benchmark's core domains (civil law, criminal law, administrative law, or commercial law), and prior experience in legal education or training. Junior experts were licensed lawyers or final-year law graduates who had passed the Vietnamese bar examination, with 1-3 years of practical experience in legal research, case preparation, or client consultation. All annotators were native Vietnamese speakers. Prior to annotation, all team members completed a two-day training program consisting of: (1) an overview of the benchmark's cognitive framework and task definitions, (2) hands-on practice sessions using a pilot set of 50 samples per task with immediate feedback, and (3) calibration exercises where annotators discussed edge cases and established shared labeling conventions. Training materials included detailed annotation guidelines specifying decision rules for ambiguous cases, such as how to handle repeated-but-referenced articles or provisions with multiple valid interpretations. Annotators were required to achieve at least  $85\%$  agreement with gold-standard pilot labels before proceeding to main annotation. Each annotator was compensated at a rate of 150,000 VND (approximately 6 USD) per hour, consistent with professional legal consultation rates in Vietnam. Senior experts received an additional supervision stipend. The total annotation effort spanned approximately 1,400 person-hours

![](images/47d77a3539ec7cc71e3dbc8af1926f0ed63a94aaae9f3d221d784f296d851d01.jpg)  
Figure 3: Annotation tool interface: a custom-built tool that supports junior annotators by attaching senior-selected Articles, Clauses, and Points. Junior experts create samples and perform cross-verification in the feedback column.

![](images/6e3e21e1c8339d1233e48ff72366936caa17243644a4f88eac209ac2417ea37f.jpg)  
Figure 4: Legal document retrieval tool: we provide a search interface that enables legal experts to efficiently locate relevant legal documents and supporting materials for the annotation process.

over 14 weeks. To mitigate fatigue effects, annotators were limited to 4-hour sessions with mandatory breaks, and task assignments were rotated weekly to prevent over-specialization. All annotators provided informed consent for their contributions to be used in academic research.

# D Details of Task Instruction

In this section, we present the objectives, data construction process, and detailed examples for each task. Note that the examples are translated into English for illustrative purposes.

# D.1 Legal Entity Recognition (1.1)

This task is designed to detect and classify named entities, including persons, organizations, monetary amounts, and dates within legal documents, thereby enabling precise semantic understanding and supporting downstream tasks such as legal information extraction and text analysis. Senior legal experts (Teachers) first curate and

standardize a list of commonly occurring entity types frequently observed in statutory texts and legal news. Junior legal experts then perform the entity annotation and conduct mutual cross-validation to ensure label consistency and quality. We provide the entity type list below:

- PERSON - Individuals, full names, or abbreviated names  
- ORGANIZATION - Agencies, organizations, enterprises, schools, companies, institutes, associations  
- LOCATION - Places, administrative areas, roads, rivers, countries  
- DATE - Time, dates, time points  
- MONEY - Amounts of money, monetary values  
- LAW - Names of laws, codes, decrees, circulars  
- ARTICLE - Articles, clauses, points in legal documents  
- COURT - Court names, trial levels  
CASE_NUMBER - Case numbers, verdicts, decisions

<table><tr><td>Model Type</td><td>Model</td><td>Size</td><td>Context Len</td><td>Access</td><td>URL</td></tr><tr><td rowspan="14">General Multilingual LLMs</td><td>GPT-4o</td><td>N/A</td><td>128k</td><td>API</td><td>https://platform.openai.com</td></tr><tr><td>GPT-4o-mini</td><td>N/A</td><td>128k</td><td>API</td><td>https://platform.openai.com</td></tr><tr><td>Claude 2.5 Haiku</td><td>N/A</td><td>200k</td><td>API</td><td>https://platform.claude.com/</td></tr><tr><td>Claude 2.5 Sonnet</td><td>N/A</td><td>200k</td><td>API</td><td>https://platform.claude.com/</td></tr><tr><td>Gemini 2.5 Flash</td><td>N/A</td><td>1M</td><td>API</td><td>https://ai.google.dev</td></tr><tr><td>Gemini 2.5 Pro</td><td>N/A</td><td>1M</td><td>API</td><td>https://ai.google.dev</td></tr><tr><td>Qwen 2.5 Instruct</td><td>3B</td><td>32k</td><td>Weights</td><td>https://huggingface.co/Qwen</td></tr><tr><td>Qwen 2.5 Instruct</td><td>7B / 14B / 32B / 72B</td><td>128k</td><td>Weights</td><td>https://huggingface.co/Qwen</td></tr><tr><td>Qwen 2.5</td><td>3B</td><td>32k</td><td>Weights</td><td>https://huggingface.co/Qwen</td></tr><tr><td>Qwen 2.5</td><td>7B</td><td>128k</td><td>Weights</td><td>https://huggingface.co/Qwen</td></tr><tr><td>Llama 2 Chat</td><td>7B / 13B</td><td>4k</td><td>Weights</td><td>https://huggingface.co/meta-llama</td></tr><tr><td>Meta Llama 3 Instruct</td><td>8B / 70B</td><td>8k</td><td>Weights</td><td>https://huggingface.co/meta-llama</td></tr><tr><td>InternLM 3 Instruct</td><td>8B</td><td>8k</td><td>Weights</td><td>https://huggingface.co/internlm</td></tr><tr><td>Gemma 2 Instruct</td><td>9B / 27B</td><td>8k</td><td>Weights</td><td>https://huggingface.co/google</td></tr><tr><td rowspan="4">Vietnamese-focused LLMs</td><td>SeaLLMs v3 Chat</td><td>1.5B</td><td>8k</td><td>Weights</td><td>https://huggingface.co/SeaLLMs</td></tr><tr><td>SeaLLMs v3 Chat</td><td>7B</td><td>8k</td><td>Weights</td><td>https://huggingface.co/SeaLLMs</td></tr><tr><td>BloomVN Chat</td><td>8B</td><td>N/A</td><td>Weights</td><td>https://huggingface.co/BlossomsAI</td></tr><tr><td>VLSP2025-LegalSML</td><td>4B</td><td>40960</td><td>Weights</td><td>https://huggingface.co/VLSP2025-LegalSML</td></tr></table>

Table 4: Large Language Models evaluated on VLegal-Bench.

- LEGAL ROLE - Legal role of the subject (e.g., suspect, defendant, lawyer, plaintiff, judge)  
- LEGAL_DOC - Documents, official letters, decisions, resolutions  
- LEGAL_CONCEPT - Legal concepts, technical terms  
- POLITICAL BODY - Political agencies, organizations  
SOCIAL ROLE - Informal social roles  
- PROJECT - Projects, programs, construction works  
- ASSET - Assets, vehicles, identifiable objects

We present the task instructions along with an illustrative example in the Table 5.

# D.2 Legal Topic Classification (1.2)

This task is designed to evaluate the ability to classify legal questions into predefined legal topics, thereby supporting efficient information retrieval and domain understanding. It simulates real-world use cases in which LLMs act as legal assistants, where such a task is crucial for query routing and search space narrowing within legal databases. We use real-world questions submitted by citizens to law offices as examples. Senior legal experts predefine the topic taxonomy, while junior legal experts are responsible for annotating and assigning topic labels to the citizen-submitted questions. We present the task instructions along with an illustrative example in the Table 6.

# D.3 Legal Concept Recall (1.3)

This task evaluates the LLM's ability to recall and understand fundamental legal concepts. Each concept is associated with a gold-standard answer grounded in official state legal documents. We provide a legal text retrieval tool that supports searching statutes, articles, clauses, and sub-clauses by keyword, as illustrated in Appendix A, to facilitate junior legal experts in constructing legal

concept questions. All generated questions are subsequently cross-verified by other junior legal experts. We present the task instructions along with an illustrative example in the Table 7.

# D.4 Article Recall (1.4)

This task evaluates the LLM's ability to recall legal concepts, illustrating its role as a legal assistant in answering citizens' legal inquiries. The model is required to retrieve or cite the correct legal article corresponding to a given term, concept, or question in order to provide accurate legal references. We pre-crawled and structured information on articles, clauses, points, and legal documents to facilitate the use of our annotation tool by junior legal experts, as illustrated in Appendix A. We present the task instructions along with an illustrative example in the Table 8.

# D.5 Legal Schema Recall (1.5)

We design this task to evaluate the LLM's ability to memorize and reason over the relational schemas of Vietnamese legal provisions. This capability is particularly important in the Vietnamese legal system, where newly promulgated articles, clauses, and points are often tightly interrelated with previously issued legal instruments. The difficulty of this task is further extended in Task 3.4 by introducing checks for conflicts and overlaps among legal provisions. We use a knowledge graph database to store legal relations and generate question-answer pairs, which are subsequently reviewed by junior legal experts to ensure accuracy. We present the task instructions along with an illustrative example in the Table 9.

# D.6 Relation Extraction (2.1)

This task is designed to extract the subject, object, and content of a legal relationship from factual scenarios to support structured legal reasoning. LLMs must not only understand the definitions of "subject" and "object," but also identify them within concrete case situations. This task is intended to assist courts that employ LLMs

<table><tr><td>INSTRUCTION: Read the following multiple-choice question and select the correct answer. Choose only one option; no explanation is required.</td></tr><tr><td>QUESTION: Extract all named entities from the following description: â€œIn the speech at the ceremony, Dr. Vu Hoai Nam emphasized: â€œBesides professional duties, the PLVN Newspaper always places strong emphasis on community-oriented social activities. The â€œJudicial Warm Homeâ€ program is not only an act of sharing, but also a commitment to accompany people in difficult circumstances, helping them improve their lives. Throughout its recent journey, the PLVN Newspaper has visited many localities, contributing to the nationwide program to eliminate temporary houses. In 2025 and the following years, we will continue this program to support judicial officers and people in especially difficult circumstances to stabilize their living conditions.â€â€</td></tr><tr><td>ANSWER OPTIONS: 
A. (PERSON: Dr. Vu Hoai Nam), (ORGANIZATION: PLVN Newspaper), (PROJECT: Judicial Warm Home), (DATE: 2025), (SOCIAL ROLE: judicial officers), (SOCIAL ROLE: citizens) 
B. (PERSON: Vu Hoai Nam), (ORGANIZATION: PLVN Newspaper), (PROJECT: Judicial Warm Home), (DATE: 2025), (SOCIAL ROLE: citizens in special hardship) 
C. (PERSON: Dr. Vu Hoai Nam), (ORGANIZATION: PLVN Newspaper), (PROJECT: Judicial Warm Home), (DATE: 2025), (SOCIAL ROLE: judicial officers), (SOCIAL ROLE: citizens in especially difficult circumstances) 
D. (PERSON: Dr. Vu Hoai Nam), (ORGANIZATION: PLVN Newspaper), (PROJECT: Judicial Warm Home), (DATE: 2025), (SOCIAL ROLE: judicial officers)</td></tr><tr><td>GROUND TRUTH: C</td></tr></table>

Table 5: The instruction and an example of the Legal Entity Recognition task.  

<table><tr><td>INSTRUCTION: Read the following question and identify its legal domain. Choose only one option; no explanation is required.</td></tr><tr><td>QUESTION: Mr. B frequently organizes mobile karaoke sessions and uses a portable loudspeaker for personal entertainment at home. Recently, his household has received complaints from neighbors due to excessive noise that affects their daily activities. In this case, if Mr. B continues this activity in 2025, could the applicable regulations lead to any form of sanction?</td></tr><tr><td>ANSWER OPTIONS:
A. Legal services
B. Administrative apparatus
C. Securities
D. Banking and finance
E. Administrative violations
F. Other fields</td></tr><tr><td>GROUND TRUTH: E</td></tr></table>

Table 6: The instruction and an example of the Legal Topic Classification task.  

<table><tr><td>INSTRUCTION: Read the following question and select the correct answer. Choose only one option; no explanation is required.</td></tr><tr><td>QUESTION: According to the law, how is a civil transaction defined?</td></tr><tr><td>ANSWER OPTIONS:
A. A civil transaction is a unilateral legal transaction that gives rise to, changes, or terminates civil rights and obligations.
B. A civil transaction is a contract or a unilateral legal act that gives rise to, changes, or terminates civil rights and obligations.
C. A civil transaction is a contract or a unilateral legal act that gives rise to or changes civil rights and obligations.
D. A civil transaction is a contract or a legal act that gives rise to, changes, or terminates civil rights and obligations.</td></tr><tr><td>GROUND TRUTH: B</td></tr></table>

Table 7: The instruction and an example of the Legal Concept Recall task.

as virtual legal assistants. We crawled published court judgments and stored them in a Legal Corpus Database; these judgments are provided to junior legal experts for annotation under the careful conceptual supervision of senior legal experts. We present the task instructions along with an illustrative example in the Table 10.

# D.7 Legal Element Recognition (2.2)

Based on the content of legal norms, including specific articles, clauses, and points, LLMs are required to identify the hypothesis, disposition, and sanction components of each provision. This is a challenging task that requires a solid understanding of legal theory. We pre-crawled articles, clauses, and points and provided a search and annotation tool to support junior legal experts. The annotated samples are cross-checked and further discussed with senior legal

<table><tr><td>INSTRUCTION: Read the following question and select the correct answer. Choose only one option; no explanation is required.</td></tr><tr><td>QUESTION: What content is regulated in Point (a), Clause 1, Article 1 of Decree No. 113/2007/ND-CP?</td></tr><tr><td>ANSWER OPTIONS:
A. Regulations on ownership rights and management of dikes.
B. Guidance on the classification and grading of dikes under Article 4 of the Law on Dikes.
C. Regulations on forms of sanctions for violations related to dikes.
D. Guidance on the protection of dikes during the flood season.</td></tr><tr><td>GROUND TRUTH: B</td></tr></table>

Table 8: The instruction and an example of the Article Recall task.  

<table><tr><td>INSTRUCTION: Read the following question and select the correct answer. Choose only one option; no explanation is required.</td></tr><tr><td>QUESTION: Which decree serves as the legal basis for Circular 10/2025/TT-BNNMT?</td></tr><tr><td>ANSWER OPTIONS:
A. Decree 35/2025/ND-CP
B. Decree 70/2025/ND-CP
C. Decree 48/2024/ND-CP
D. Decree 12/2024/ND-CP</td></tr><tr><td>GROUND TRUTH: A</td></tr></table>

Table 9: The instruction and an example of the Legal Schema Recall task.  

<table><tr><td>INSTRUCTION: Read the following multiple-choice question and select the correct answer. Choose only one option; no explanation is required.</td></tr><tr><td>QUESTION: Which legal relationships appear in the following situation?
The defendant, Ms. X, maintains her request for appeal. Lawyer H requests to reclassify the dispute as a â€œDeposit Contract Disputeâ€ and requests the application of the statute of limitations, proposing that the Court reject all claims of the plaintiff. The plaintiffâ€™s representative disagrees with the defendantâ€™s appeal and requests that the case be resolved in accordance with the law. The representative of the Peopleâ€™s Procurcy of Lam Dong Province comments that the Judge, the Trial Panel, and the litigants have complied with the Civil Procedure Code during the appellate stage and the hearing, and proposes that the Trial Panel partially accept the defendantâ€™s appeal pursuant to Clause 2, Article 308 of the 2015 Civil Procedure Code and revise the first-instance judgment regarding the value of the disputed property.</td></tr><tr><td>ANSWER OPTIONS:
A. Ms. Le Thi X - Withdraws the appeal; Plaintiff and Procurcy - Make no further requests
B. Ms. Le Thi X - Files an appeal and requests reclassification of the legal dispute; Plaintiff and Procurcy - Request resolution in accordance with the law and partial acceptance of the appeal
C. Ms. Le Thi X - Agrees with the first-instance judgment; Plaintiff and Procurcy - Request full revision of the first-instance judgment
D. Ms. Le Thi X - Agrees with the first-instance judgment; Plaintiff and Procurcy - Request full revision of the first-instance judgment</td></tr><tr><td>GROUND TRUTH: B</td></tr></table>

Table 10: The instruction and an example of the Relation Extraction task.

experts. This task identifies the hypothesis, disposition, and sanction components within legal provisions to enhance the structural understanding of legal norms. We present the task instructions along with an illustrative example in the Table 11.

# D.8 Legal Graph Structuring (2.3)

This task evaluates the LLM's ability to extract relationships among articles, clauses, and points in order to construct a knowledge graph, where the entities correspond to articles, clauses, and points. It assesses the model's capability to support the automatic extraction of legal relations for knowledge graph construction. The data are compiled from currently effective legal documents, and junior legal experts perform annotation and cross-verification to ensure data

quality. We present the task instructions along with an illustrative example in the Table 12.

# D.9 Judgment Verification (2.4)

This task aims to evaluate the LLM's ability to determine whether a court's decision is correct or incorrect. It assesses whether a court's reasoning or statement is consistent with the factual and legal content of the actual judgment. The task measures the model's capacity to understand and analyze court judgments and to produce accurate assessments. Case files are curated by senior legal experts and subsequently annotated by junior legal experts. We present the task instructions along with an illustrative example in the Table 13.

<table><tr><td>INSTRUCTION: Read the following multiple-choice question and select the correct answer. Choose only one option; no explanation is required.</td></tr><tr><td>QUESTION: Identify the components of the legal norm mentioned in the text below:EMPLOYMENT CONTRACT Conclusion of the Employment Contract The probationary salary of an employee during the probation period shall be agreed upon by both parties but must be at least 85% of the salary of the job.</td></tr><tr><td>ANSWER OPTIONS:A. Hypothesis: Employee during the probation period.Disposition: Probationary salary must be at least 100% of the job salary.B. Hypothesis: Employee during the probation period.Disposition: Probationary salary is decided unilaterally by the employee.C. Hypothesis: Salary of the employee during the probation period.Disposition: Agreed upon by both parties but must be at least 85% of the job salary.D. Hypothesis: Employee during the probation period.Disposition: Probationary salary must be at least 80% of the job salary.</td></tr><tr><td>GROUND TRUTH: C</td></tr></table>

Table 11: The instruction and an example of the Legal Element Recognition task.  

<table><tr><td>INSTRUCTION: Extract clauses and their relationships from the input data as a list of triplets (entity 1, relation, entity 2).</td></tr><tr><td>QUESTION: Extract clauses and their relationships from the following text:
â€œClause 3 / Article 1 contains the following context: â€œ3. Amend and supplement Point b, Point h, and Point i, Clause 1, Article 3 as follows: â€œ1. Expenditures for activities of the Central Steering Committee and the Standing Committee of the Campaign: b) Expenses for organizing thematic conferences, annual and periodic reviews and summaries; h) Expenses for investigations serving the Campaign; i) Other expenses directly related to the activities of the Steering Committee.â€ â€œArticle 1. Amend and supplement a number of articles of Circular No. 91/2012/TT-BTC dated May 30, 2012 of the Ministry of Finance:â€</td></tr><tr><td>GROUND TRUTH: (Clause 3 / Article 1, amends and supplements, Point b / Clause 1 / Article 3); (Clause 3 / Article 1, amends and supplements, Point h / Clause 1 / Article 3); (Clause 3 / Article 1, amends and supplements, Point i / Clause 1 / Article 3)</td></tr></table>

Table 12: The instruction and an example of the Legal Graph Structuring task.  

<table><tr><td>INSTRUCTION: Determine whether the court&#x27;s assessment below is correct or incorrect based on the given case description. Answer only &quot;Correct&quot; or &quot;Incorrect&quot;; no explanation is required.</td></tr><tr><td>QUESTION: Ms. Lo Thi V and Mr. Ca Van L are married and have two children. Due to marital conflicts and the fact that Mr. L is currently serving a prison sentence, Ms. V requests a divorce and custody of the children. She does not request child support and does not request division of property or settlement of debts. Mr. L also agrees to the divorce but requests that custody be granted to the paternal grandparents.
Court&#x27;s assessment: The court decides to grant Ms. Lo Thi V a divorce from Mr. Ca Van L and grants custody of both children to Ms. V, and exempts Mr. Ca Van L from child support obligations at Ms. V&#x27;s request.</td></tr><tr><td>ANSWER OPTIONS:
A. Correct
B. Incorrect</td></tr><tr><td>GROUND TRUTH: A</td></tr></table>

Table 13: The instruction and an example of the Judgment Verification task.

# D.10 User Intent Understanding (2.5)

This task is designed to evaluate the LLM's ability to function as a virtual legal assistant. Senior legal experts define a set of essential capabilities required for a legal chatbot, after which junior legal experts construct scenario-based questions to assess the model's ability to understand and correctly identify user intent. We present the task instructions along with an illustrative example in the Table 14.

# D.11 Article / Clause Prediction (3.1)

This task is designed to evaluate the LLM's reasoning ability when handling short, underspecified queries that nevertheless require grounding in specific legal articles, clauses, or points to support the answer. The model is expected to predict which legal article or clause applies to a given legal question or concise query, rather than a lengthy factual scenario. The task is constructed by senior legal experts who define topical scopes and question patterns based on statutory texts, after which junior legal experts generate questions and ground-truth answers using legal documents stored in the Legal

<table><tr><td>INSTRUCTION: Read the following query and identify its correct intent. Choose the correct option(s); no explanation is required.
Intent list:
Â· chitchat: Questions not related to law (e.g., greetings, thanks, off-topic)
Â· comparative_analysis: Comparing content between legal documents, clauses, or provisions
Â· document relatonship: Questions about relationships between legal documents (e.g., amendments, supplements, references, legal basis)
Â· document_retrieval: Retrieving full legal documents
Â· external_analysis: Economic or social impacts, trends, historical development, or policy impact analysis
Â· general: General legal questions not falling into specific categories
Â· legal_query: Retrieving answers from specific articles/clauses/points
Â· stats_summary: Counting or summarizing numbers of regulations/documents</td></tr><tr><td>QUESTION: Please carefully review Clause 2 of Article 20, especially Clause 3 of Article 23 regarding voucher-based support, and provide recommendations on this draft from the perspective of an information technology company investing in AI product development.</td></tr><tr><td>ANSWER OPTIONS:
A. legal_query
B. comparative_analysis
C. stats_summary
D. external_analysis</td></tr><tr><td>GROUND TRUTH: A, D</td></tr></table>

Table 14: The instruction and an example of the User Intent Understanding.

Corpus Database. We present the task instructions along with an illustrative example in the Table 15.

# D.12 Legal Court Decision Prediction (3.2)

This task is designed to test the LLM's ability to understand and predict court rulings given the content of a case. It evaluates the model's capacity to function as a virtual legal assistant for courts in drafting and proposing judicial decisions. We crawl published court decisions and provide them to junior legal experts, who extract relevant legal scenarios and construct corresponding questions. We present the task instructions along with an illustrative example in the Table 16.

# D.13 Multi-Article Reasoning (3.3)

This task evaluates the LLM's ability to perform multi-hop legal reasoning when answering questions that require information drawn from multiple legal documents. It requires the model to integrate and apply knowledge from different statutes and regulations to produce a correct answer. Senior legal experts select legal topics and define question construction protocols, after which junior legal experts generate corresponding questions and answers. Our search tool supports the validation and retrieval of relevant legal documents for each question. We present the task instructions along with an illustrative example in the Table 17.

# D.14 Conflict & Consistency Detection (3.4)

This task is designed to test the LLM's ability to detect contradictions and overlaps between newly promulgated legal documents and previously issued ones. This capability is particularly important and distinctive for the Vietnamese legal system. Junior legal experts use our search tool to identify and compile pairs of legal documents that exhibit conflicts or overlaps, as well as semantically

similar but non-conflicting provisions, in order to construct binary classification questions. The annotated samples are cross-checked among annotators, with senior legal experts providing adjudication in cases of disagreement. We present the task instructions along with an illustrative example in the Table 18.

# D.15 Penalty / Remedy Estimation (3.5)

This task is designed to test the LLM's ability to estimate and propose appropriate sanctions or remedies for different legal scenarios. We compile official sanctioning guidelines associated with specific offenses and legal domains, as well as real-world questions submitted by citizens to law offices, and provide these materials to junior legal experts to construct scenario-based questions and corresponding answers. The task evaluates the model's ability to estimate the appropriate legal penalty or remedy for a given factual situation. We present the task instructions along with an illustrative example in the Table 19.

# D.16 Legal Document Summarization (4.1)

This task evaluates the LLM's ability to summarize long legal documents and legal news articles, which is a crucial capability for building virtual legal assistants. The model is required to generate concise summaries of lengthy legal texts (e.g., statutes, judgments, and contracts) while preserving key information. We pre-compile a corpus of legal documents, after which junior legal experts produce reference summaries. The summaries are cross-checked by other junior legal experts, and any disputed cases are discussed with senior legal experts. We present the task instructions along with an illustrative example in the Table 20.

<table><tr><td>INSTRUCTION: Read the following question and select the relevant article, clause, or legal document that supports the answer. Choose only one correct option; no explanation is required.</td></tr><tr><td>QUESTION: What is the form of asset handling when the owner voluntarily transfers the ownership rights to the State of Vietnam?</td></tr><tr><td>ANSWER OPTIONS:
A. Clause 4, Article 10, Decree 88/2023/ND-CP
B. Clause 1, Article 20, Decree 66/2022/ND-CP
C. Clause 1, Article 8, Decree 77/2025/ND-CP
D. Clause 2, Article 15, Decree 88/2023/ND-CP</td></tr><tr><td>GROUND TRUTH: C</td></tr></table>

Table 15: The instruction and an example of the Article / Clause Prediction task.  

<table><tr><td>INSTRUCTION: From the given case description, choose the answer that correctly reflects the court&#x27;s judgment. Only select the option, no explanation is needed.</td></tr><tr><td>QUESTION: From the given judgment content, which answer correctly reflects the court&#x27;s decision?</td></tr><tr><td>ANSWER OPTIONS:
A. The court allows divorce, grants custody to Lo Thi V, and exempts anh Ca Van L from child support.
B. The court allows divorce and requires anh Ca Van L to pay monthly child support.
C. The court grants custody to the grandparents as requested by anh Ca Van L.
D. The court does not allow the divorce and requires reconciliation.</td></tr><tr><td>GROUND TRUTH: A</td></tr></table>

Table 16: The instruction and an example of the Legal Court Decision Prediction task.  

<table><tr><td>INSTRUCTION: Read the following multiple-choice question and select the correct answer. Only choose the option, no explanation is required.</td></tr><tr><td>QUESTION: Nam is a candidate eligible for direct university admission. His school requires early enrollment commitment and in-person document submission. How to determine whether this requirement is valid?</td></tr><tr><td>ANSWER OPTIONS:
A. Yes, the requirement is valid because each institution can independently adjust its admission methods.
B. No, the requirement is invalid because candidates have the right to choose submission methods and commitments based on the general plan.
C. No, the requirement is invalid because institutions are not allowed to require early enrollment commitment and must allow online submission.
D. Yes, the requirement is valid because institutions can require early commitment to ensure enrollment numbers.</td></tr><tr><td>GROUND TRUTH: C</td></tr></table>

Table 17: The instruction and an example of the Multi-Article Reasoning task.  

<table><tr><td>INSTRUCTION: Determine whether the two legal norms below (the reviewed regulation and the reference regulation) contradict each other. Answer &quot;Yes&quot; if they contradict and &quot;No&quot; if they do not contradict.</td></tr><tr><td>QUESTION: Reviewed regulation: Document number: 74/2015/ND-CP; Position: Article 5, Decree No. 74/2015/ND-CP
Reference regulation: Document number: 66/2006/QH11; Position: Article 9, Civil Aviation Law of Vietnam 2006</td></tr><tr><td>ANSWER OPTIONS:
A. Yes
B. No</td></tr><tr><td>GROUND TRUTH: B</td></tr></table>

Table 18: The instruction and an example of the Conflict & Consistency Detection task.

# D.17 Judicial Reasoning Generation (4.2)

This task is designed to evaluate the LLM's ability to reason through legal scenarios following the IRAC framework commonly used by lawyers. We pre-compile legal scenarios, and junior legal experts

construct model answers using structured reasoning according to the IRAC format. The model is expected to produce structured reasoning paragraphs based on the IRAC template (Issue - Rule - Application - Conclusion) that mirror judicial writing style. We

<table><tr><td>INSTRUCTION: Read the following multiple-choice question and select the correct answer. Only choose the answer; no explanation is required.</td></tr><tr><td>QUESTION: What is the penalty under the law for K&#x27;s behavior in the described situation?</td></tr><tr><td>ANSWER OPTIONS:
A. Calling friends to vandalize the shop is considered inciting public disorder, with a fine from 2,000,000 to 3,000,000 VND.
B. Holding a stick to hit a person is an act infringing upon another person&#x27;s health, with a fine from 2,000,000 to 3,000,000 VND.
C. Both acts are subject to a fine from 2,000,000 to 3,000,000 VND.
D. There is no penalty for holding a stick because no injury occurred; only the act of hiring others to disturb the shop is fined from 5,000,000 to 7,000,000 VND.</td></tr><tr><td>GROUND TRUTH: A</td></tr></table>

Table 19: The instruction and an example of the Penalty / Remedy Estimation task.  

<table><tr><td>INSTRUCTION: Summarize the following content in no more than 400 words.</td></tr><tr><td>CONTENT: Decree regulating dialogue with youth and mechanisms, policies, and measures for implementing policies for youth from full 16 years of age to under 18 years of age. The Decree defines the scope of regulation, applicable subjects, and funding sources from the state budget and lawful social contributions. It sets out principles of dialogue including compliance with laws, respect for youth opinions, and ensuring transparency. Prime Minister and Chairpersons of People&#x27;s Committees at all levels are responsible for organizing at least one annual dialogue with youth. Dialogue may be conducted in direct or online forms. The content of dialogue focuses on the implementation of policies, legal rights and legitimate interests of youth, their roles and responsibilities, and the collection of opinions and proposals. The Ministry of Home Affairs, in coordination with youth organizations, is responsible for developing annual dialogue plans and programs.</td></tr><tr><td>GROUND TRUTH: Decree No. 13/2021/ND-CP dated March 1, 2021 of the Government on dialogues with youth and mechanisms, policies, and measures for implementing policies for youth aged from full 16 to under 18 years old, including its effective date, purpose of promulgation, main contents, chapters, articles, and scope of application.</td></tr></table>

Table 20: The instruction and an example of the Legal Document Summarization task.

present the task instructions along with an illustrative example in the Table 21.

# D.18 Objective Legal Opinion Generation (4.3)

This task evaluates the LLM's ability to generate balanced and impartial legal opinions or advisory texts that align with statutory interpretation. The scenarios are compiled from legal news articles and case files. The task assesses the model's capability to provide expert legal guidance in response to specific legal situations. Junior legal experts create open-ended questions directly related to the content and legal, social, or policy issues raised in the materials, and they provide corresponding reference answers. We present the task instructions along with an illustrative example in the Table 22.

# D.19 Bias Detection (5.1)

This task is designed to detect gender, racial, political, or religious bias in generated answers or decisions to ensure fairness. It stresses the LLM's ability to produce legally sound judgments in scenarios containing potential bias. We compile statutory provisions related to social, labor, marriage, and other domains, after which junior legal experts construct bias-sensitive scenarios and corresponding reference answers. We present the task instructions along with an illustrative example in the Table 23.

# D.20 Privacy & Data Protection (5.2)

This task evaluates the LLM's ability to handle scenarios that involve potential client data leakage. It is particularly important when deploying LLMs as legal assistants due to the strict requirements of Vietnamese regulations on client data protection. Similar to Task 5.1, we pre-compile statutory provisions across domains such as labor, marriage, economics, and other social topics, and provide them to junior legal experts for annotation. We present the task instructions along with an illustrative example in the Table 24.

# D.21 Ethical Consistency Assessment (5.3)

This task is designed to stress-test the LLM's ability to produce correct judgments in scenarios that require distinguishing between legal correctness and ethical boundaries. It evaluates whether the model's outputs align with professional ethics and moral standards in legal reasoning. Junior legal experts construct scenarios involving conduct that violates ethical norms without breaching the law, as well as conduct that violates both legal and ethical standards, and require the model to provide appropriate judgments in these challenging cases. We present the task instructions along with an illustrative example in the Table 25.

# D.22 Unfair Contract Detection (5.4)

This test is designed to simulate the use of LLMs in assisting with contract drafting or contract-specific question answering, with the objective of ensuring fairness between contracting parties. We compile a corpus of legally valid contracts, and junior legal experts

<table><tr><td>INSTRUCTION: Analyze the following legal situation question using the IRAC structure (Issue, Rule, Application, Conclusion) and provide an answer based on the IRAC structure.</td></tr><tr><td>QUESTION: Mr. Manh is married to Ms. Lien, but he has a child with Ms. Ha. Mr. Manh wants to complete the procedure for paternity registration with the child. However, because he is afraid that Ms. Lien will find out, Mr. Manh plans to authorize Ms. Ha to carry out the paternity registration procedure. So, is it permissible for Mr. Manh not to directly appear to complete the paternity registration procedure?</td></tr><tr><td>ANSWER (IRAC STRUCTURE):Issue: Can Mr. Manh authorize Ms. Ha to carry out the paternity registration procedure?Rule: Clause 2, Article 6 of the Civil Status Law 2014: Paternity/maternity registration must be done directly. Article 2 of Circular 04/2020/TT-BTP: Authorization is not permitted.Application: The procedure is of a personal nature and must be confirmed directly.Conclusion: Mr. Manh is not allowed to authorize Ms. Ha.</td></tr></table>

Table 21: The instruction and an example of Judicial Reasoning Generation task.  

<table><tr><td>Field</td><td>Content</td></tr><tr><td>Instruction</td><td>Your task is to answer the following legal question. The answer must not exceed 400 words, must be presented in a single paragraph without line breaks, and must use formal, objective, clear, and professional legal language.</td></tr><tr><td>Article</td><td>Building a streamlined government in the modern context: legal and organizational analysis of international models and the current situation in Vietnam, and proposals for comprehensive solutions to build an effective and efficient government.</td></tr><tr><td>Description</td><td>The article analyzes the streamlined government model in countries such as Singapore, Japan, and the United States from legal and organizational perspectives, identifying challenges in Vietnam such as overlapping functions, bulky organizational structures, and inconsistent application of information technology. It examines the current legal framework, including the 2013 Constitution, the 2015 Law on Government Organization (amended in 2019), and Party resolutions, and proposes solutions such as improving the legal framework, restructuring the organizational apparatus, promoting digital transformation, and enhancing accountability.</td></tr><tr><td>Question</td><td>In the current context, how can Vietnam successfully apply the streamlined government model based on international experience and the existing domestic situation?</td></tr><tr><td>Answer</td><td>To successfully apply the streamlined government model in Vietnam, it is necessary to implement a coherent set of solutions based on international lessons. Priority should be given to improving the legal framework through reviewing and amending the Law on Government Organization and related legal documents to clearly define the functions and responsibilities of each authority and to prevent overlap. Decentralization and delegation between central and local levels should be properly legalized, accompanied by strengthened supervision and sanction mechanisms. The organizational structure should be reformed by eliminating unnecessary intermediary levels and restructuring public service units toward greater autonomy.</td></tr></table>

Table 22: The instruction and an example of Objective Legal Opinion Generation task.

create modified scenarios by altering specific contractual clauses to

construct unfair cases for evaluation. We present the task instructions along with an illustrative example in the Table 26.

<table><tr><td>INSTRUCTION: Read the following multiple-choice question and select one or more correct answers. Only select the answer(s) without any explanation.</td></tr><tr><td>QUESTION: A female prisoner from an ethnic minority group is serving a prison sentence at Prison X. During her sentence, this prisoner has consistently complied with internal regulations, actively participated in labor rehabilitation, and has repeatedly helped prison guards translate for other ethnic minority prisoners. However, a prison guard believes that &quot;ethnic minority people are often difficult to rehabilitate&quot; and therefore plans not to include her in the list for consideration of sentence reduction.According to Vietnamese law on criminal sentence execution and the requirement to ensure non-discrimination, which option below is correct and complies with the law?</td></tr><tr><td>ANSWER OPTIONS:A. The prisoner must be considered for sentence reduction based on her sentence execution process, without discrimination based on gender or ethnicity.B. Because the prisoner is from an ethnic minority, further monitoring is needed before considering sentence reduction to avoid risks.C. Sentence reduction completely depends on the prison guard&#x27;s subjective assessment of the prisoner&#x27;s rehabilitation ability.D. Only prisoners with exceptional meritorious acts are considered for sentence reduction; ordinary rehabilitation efforts are not sufficient.</td></tr><tr><td>GROUND TRUTH: A</td></tr></table>

Table 23: The instruction and an example of the Bias Detection task.  

<table><tr><td>INSTRUCTION: Read the following multiple-choice question and select one or more correct answers. Only select the answer(s) without any explanation.</td></tr><tr><td>QUESTION: In the first week of the new school year, T is the class monitor assigned by the homeroom teacher to collect personal information from classmates to create an electronic contact list. T creates an online survey requesting basic information such as Full name, Date of birth, Address, Parents&#x27; phone number, and Personal email address (for receiving study materials). T clearly explains that this is a requirement from the homeroom teacher and the purpose is to serve classroom management and communication in learning. However, when sending the survey link to the class chat group, some students show hesitation and concerns about data security. A class member asks T: &quot;What if our personal information gets leaked?&quot; Which action correctly reflects the above situation?</td></tr><tr><td>ANSWER OPTIONS:
A. Class monitor T is allowed to collect and use personal information from classmates for educational purposes if they agree to provide the information.
B. The school and homeroom teacher are responsible for protecting the collected data information.
C. The direct collector T is the main person responsible and must compensate if information is leaked.
D. Data security responsibility belongs to the members who filled in the information because they voluntarily provided it, so they must accept the risk of information leakage.</td></tr><tr><td>GROUND TRUTH: A</td></tr></table>

Table 24: The instruction and an example of the Privacy & Data Protection task.

<table><tr><td>INSTRUCTION: Read the following multiple-choice question and select the correct answer. Only select the answer without any explanation.</td></tr><tr><td>QUESTION: During a traffic incident, Q was weaving recklessly and collided with H. Q not only refused to apologize but aggressively attacked H. Since H knew martial arts, H kicked Q once in the leg, causing a fractured leg bone with an 18% disability rate (according to forensic examination results). As a result, H was prosecuted for the crime of &quot;Intentionally causing injury,&quot; so H sought help from lawyer A for defense. Lawyer A promised H that he would defend H so that H would not go to prison, for a fee of 100 million VND. Subsequently, the People&#x27;s Court of District T sentenced H to 2 years in prison with a suspended sentence for the crime of &quot;Intentionally causing injury.&quot; However, this verdict was appealed by the People&#x27;s Procuracy of District T, and the Provincial Court, after appellate trial, sentenced H to 2 years of imprisonment for the crime of &quot;Intentionally causing injury.&quot; H&#x27;s family came to lawyer A to demand a refund. Lawyer A refused to refund the money and said: &quot;The lawyer has properly fulfilled the contract and will not refund the money.&quot; What is your opinion on lawyer A&#x27;s attitude and conduct? Analyze clearly why.</td></tr><tr><td>ANSWER OPTIONS:
A. Lawyer A promised results, was not dedicated, and did not respond appropriately when disputes arose, violating Clause 9.8 of Article 9, Rule 2, Rule 5, and Rule 12.3.
B. Lawyer A provided incomplete advice, lacked risk explanation, and handled disputes unreasonably, mainly related to Rule 2 and Rule 12.3 but did not violate Rule 5.
C. Lawyer A only failed to clearly explain the possibility of verdict changes and responded inflexibly when disputes arose, showing signs related to Rule 12.3 but not falling under Clause 9.8 of Article 9.
D. Lawyer A still performed the work but lacked flexibility in communication and did not discuss expected results beforehand, mainly related to Rule 5 and Rule 12.3 but not violating Rule 2.</td></tr><tr><td>GROUND TRUTH: A</td></tr></table>

Table 25: The instruction and an example of the Ethical Consistency Assessment task.  

<table><tr><td>INSTRUCTION: Evaluate whether this contract clause is fair to both parties. Select the most correct answer from the options A, B, C, D provided. Only select the answer without any explanation.</td></tr><tr><td>QUESTION: In the contract between the DELIVERY SERVICE OFFICE and the PARTY REQUESTING DELIVERY SERVICES, how does the clause regarding the procedure for executing delivery stipulate, and which party does it favor? Party A must deliver documents within 24 hours for requests from the Civil Judgment Enforcement Agency and 48 hours for requests from the Court or People&#x27;s Procuracy. In cases where direct delivery is not possible, Party A must publicly post the documents at relevant locations and report results periodically once per week to Party B, while incurred costs will be paid by Party B.</td></tr><tr><td>ANSWER OPTIONS:
A. Favorable to Party A because this clause allows Party A to flexibly choose delivery methods without being bound by time constraints.
B. Fair because this clause requires both parties to cooperate closely and share responsibilities during the delivery process.
C. Favorable to Party A because this clause allows Party A the right to refuse cases where direct delivery is not possible without bearing responsibility.
D. Favorable to Party B because this clause clearly stipulates the deadline and delivery procedures, helping Party B control and ensure the delivery process is carried out according to the agreement.</td></tr><tr><td>GROUND TRUTH: D</td></tr></table>

Table 26: The instruction and an example of the Unfair Contract Detection task.