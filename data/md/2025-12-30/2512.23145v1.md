# Reservoir Computing inspired Matrix Multiplication-free Language Model

Takumi Shiratsuchi*, Yuichiro Tanaka*†, and Hakaru Tamukoh*†

*Graduate School of Life Science and Systems Engineering, Kyushu Institute of Technology, Japan

$\dagger$ Research Center for Neuromorphic AI Hardware, Kyushu Institute of Technology, Japan

Abstract—Large language models (LLMs) have achieved state-of-the-art performance in natural language processing; however, their high computational cost remains a major bottleneck. In this study, we target computational efficiency by focusing on a matrix multiplication free language model (MatMul-free LM) and further reducing the training cost through an architecture inspired by reservoir computing. Specifically, we partially fix and share the weights of selected layers in the MatMul-free LM and insert reservoir layers to obtain rich dynamic representations without additional training overhead. Additionally, several operations are combined to reduce memory accesses. Experimental results show that the proposed architecture reduces the number of parameters by up to  $19\%$ , training time by  $9.9\%$ , and inference time by  $8.0\%$ , while maintaining comparable performance to the baseline model.

Index Terms—large language model, reservoir computing.

# I. INTRODUCTION

Large language models (LLMs), for example, Llama3 [1] and DeepSeek-V3 [2], achieve performance comparable to humans in various tasks such as sentence generation and dialogue [3]. Consequently, companies and public institutions are rapidly deploying them in real-world products and services. Researchers have also demonstrated strong results when they apply LLMs to legal case retrieval [4], [5], code generation with automatic debugging [6], medical question-answering systems [7], [8], and robot action planning [9]-[12].

Despite these advances, the computational cost of both training and inference remains prohibitively high for most computing environments. For example, training a practical large-scale model, Llama3 405 billion (B) [1], requires approximately 50 days even when using 16,000 NVIDIA H100 graphics processing units (GPUs), each equipped with 80 GB of memory. Inference is also computationally expensive: generating 256 tokens with a context length of 4,096 tokens using 16 H100 GPUs requires approximately one second of computation.

A primary factor contributing to the high computational cost of LLM training and inference is the large memory footprint arising from the massive number of parameters and their bit widths. For instance, inference of Llama3 405 B without quantization requires approximately 750 GB of memory solely for storing model parameters. To mitigate this issue, previous studies have proposed methods to reduce the computational and memory costs of LLMs, including quantization of pretrained models [13], [14] and low-rank decomposition [15].

However, these approaches provide only limited reductions in parameter count and memory usage.

This study focuses on the matrix multiplication-free language model (MatMul-free LM) [16] to address the memory usage problem. MatMul-free LM is a type of LLM where the weights are quantized to ternary values  $\{+1, 0, -1\}$  through quantization-aware training (QAT) [17], [18], [19], leading to a significant reduction in memory usage [16]. Additionally, MatMul-free LM replaces attention mechanism used in most LLMs with a matrix multiplication-free linear gated recurrent unit (MLGRU). This change contributes to a further decrease in computational costs. Although MatMul-free LM reduces the bit width of individual weights, the problem of its large number of parameters remains, limiting its effectiveness. In addition, pre-training all parameters in the model incurs substantial computational costs.

To solve these issues, this study introduces the concept of reservoir computing (RC) [20]. RC is a computationally efficient recurrent neural network (RNN) where both the input and reservoir layers have fixed weights, while only the output layer is trainable [20]. RC is known to perform well despite the small number of trainable parameters [21]. Furthermore, research on efficient implementation methods using dedicated hardware and physical dynamics has advanced considerably, and is expected to yield practical developments [22], [23].

Here, we propose an RC inspired MatMul-free LM (RC MatMul-free LM), applying RC to MLGRU in MatMul-free LM. This research is the first attempt to introduce an RC-extended component into LLM. The proposed RC MatMul-free LM reduces not only the bit width of each parameter but also the total number of parameters by fixing and sharing parameters, based on the findings reported in previous studies [24], [25], which show that performance degradation remains small even when parameters are shared across layers.

# II. RELATED WORK

# A. Architecture of LLM

Fig. 1 shows an architecture of generic LLM and transformer-based LLM [26]. As shown on the left side of Fig. 1, LLM mainly consists of six components: embedding layer, normalization (Norm) layer, token-mixer, channel-mixer, head, and residual connections that directly add the inputs [27]. The token-mixer and channel-mixer account for the majority of parameters. Almost all LLMs employ a feed-forward network (FFN) as the channel-mixer. In contrast, there are various

![](images/bf603114d6f09561d2421b2939f67894d5fd4fc15c1eeadbe4e0c235df99cb7a.jpg)  
Fig. 1. Left : architecture of general LLM right : architecture of transformer-based LLM

architectures for the token-mixer. LLMs, with the most widely used architecture, such as Llama3 [1], adopt the transformer [26] architecture and use attention mechanism as the token-mixer, as shown on the right side of Fig. 1. Collectively, the component consisting of the FFN, attention, Norm, and residual connection is referred to as the transformer block. Despite its prevalence, the attention mechanism suffers from the issue that the computational cost scales quadratically with the input sequence length. To address this problem, alternatives have been proposed, such as Mamba [28], which employs a state space model instead of the attention mechanism; Receptance Weighted Key Value (RWKV) [29], which replaces attention mechanism with an RNN; and a hierarchically gated linear RNN (HGRN) [30].

HGRN reproduces the attention mechanism by RNN. While simple RNNs cannot adequately capture the long-term dependencies that the attention mechanism can, HGRN employs a hierarchically designed forget gate to capture both short and long-term dependencies. Specifically, the lower bound of the forget gate is trained through the cumax activation function [31] and is constrained to increase monotonically with depth: layers nearer the embeddings receive a small lower bound, while layers nearer the output head receive a large one. Consequently, the lower layers whose forget gates take small values retain past states only briefly and capture short-term dependencies, whereas the upper layers whose gates take larger values retain past states for much longer and capture long-term dependencies [30].

# B. Quantization and Other Model Compression for LLMs

In LLMs, a scaling law [32] indicates that improving performance requires increasing model parameters, training data,

and computational resources, thereby necessitating greater memory usage and longer processing times for both training and inference. To address the increasing memory usage of inference associated with performance, quantization of trained LLMs has been widely considered. Specifically, quantization of most weights to int8 [13] or int4 [14], and quantization using QLoRA [33] to 2-bit or 3-bit NormalFloat data types [34]–[36], where each value represents a brain floating-point (bfloat) 16 [37] value, have been investigated. Furthermore, OPTQ [38] quantizes the weights to 3-bit per layer and utilizes a kernel optimized for 3-bit quantized weights to minimize memory accesses and achieve fast inference. However, there are limitations to these methods, such as the trade-off between quantization error and memory usage, and the need for float16 or bfloat16 operations to maintain accuracy, leading to increased computation time in inference.

QAT [17]–[19] is effective for constructing high-performance binary or ternary LLMs. As research into developing high-performance binary and ternary LLMs using QAT, Wang et al. showed that when a 1-bit model is scaled, the performance of the model with quantization approaches that of the model without quantization [39]. Ma et al. showed that BitNet with 3.9B parameters quantized to three values performs better than Llama [40], [41] with 3B parameters without quantization, using more memory for inference [42]. However, the training process requires the use of float16 and other data types, and the computational cost is enormous, especially for pre-training.

There are also methods for accelerating LLMs and reducing their memory usage by fixing and sharing parameters. Reservoir transformer [25] has shown that the parameters of some transformer blocks can be fixed without training and shared by transformer blocks in different layers, thereby reducing the model size. Instead of fixed transformer blocks, an architecture using bi-directional GRU (BiGRU) [43] with all weights fixed, called BiGRU reservoir, is also proposed. However, it only introduces an untrained BiGRU immediately after the transformer block, and no consistent performance improvement has been observed. A lite bidirectional encoder representations from transformers (ALBERT) [24] showed that sharing the attention mechanism parameters in each layer caused little performance degradation. However, no studies have confirmed this property for models combining RNNs and transformers, including RWKV.

Other methods for reducing parameters include approximating the weight matrix of a trained LLM with multiple low-dimensional matrices, called low-rank decomposition [15], and reducing weights of low importance, called pruning [44], but their effectiveness is limited.

# C. MatMul-freeLM

MatMul-free LM [16] is one of the extremely lightweight LLMs that enables the implementation of matrix multiplication using logical operations by quantizing the parameters to ternary values  $\{+1, 0, -1\}$ .

Fig. 2 shows the architecture of MatMul-free LM. The input

![](images/dbe73e4d33ddef80d8c2d3db2ca9d18eaa11ec3f1dd31cbdb6dc3436a6f934b8.jpg)  
Fig. 2. MatMul-free LM

![](images/60751ae48f7a3c13d8f9dc42380b649a6e49f93d2bd319e5b32e2f4eb3cd93eb.jpg)  
Fig. 3. Repeating module of MLGRU

tokens are first embedded into vectors, just like in a standard transformer. These vectors are then fed into the decoder block, consisting of an MLGRU, a gated linear unit (GLU) [45], [46], root mean square (RMS) Norm [47], and residual connections. The decoder block is repeated N times (where N is the number of layers). Finally, the processed vectors are passed through a linear layer with ternary weights (called the heads) and then converted into tokens based on the logits.

Fig. 3 shows the repeating module of MLGRU in MatMul-free LM, and its computation is defined from (1) to (7).

$$
\boldsymbol {f} _ {t} = \sigma \left(\boldsymbol {x} _ {t} \circledast \boldsymbol {W} _ {f} + \boldsymbol {b} _ {f}\right), \tag {1}
$$

$$
\boldsymbol {f} _ {t} ^ {\prime} = \gamma^ {k} + \left(\mathbf {1} - \gamma^ {k}\right) \odot \boldsymbol {f} _ {t}, \tag {2}
$$

$$
\boldsymbol {c} _ {t} = \tau \left(\boldsymbol {x} _ {t} * \boldsymbol {W} _ {c} + \boldsymbol {b} _ {c}\right), \tag {3}
$$

$$
\boldsymbol {h} _ {t} = \boldsymbol {f} _ {t} ^ {\prime} \odot \boldsymbol {h} _ {t - 1} + \left(\boldsymbol {1} - \boldsymbol {f} _ {t} ^ {\prime}\right) \odot \boldsymbol {c} _ {t}, \tag {4}
$$

$$
\boldsymbol {g} _ {t} = \sigma \left(\boldsymbol {x} _ {t} * \boldsymbol {W} _ {g} + \boldsymbol {b} _ {g}\right), \tag {5}
$$

$$
\boldsymbol {o} _ {t} ^ {\prime} = \boldsymbol {g} _ {t} \odot \boldsymbol {h} _ {t}, \tag {6}
$$

$$
\boldsymbol {o} _ {t} = \boldsymbol {o} _ {t} ^ {\prime} * \boldsymbol {W} _ {o} + \boldsymbol {b} _ {o}. \tag {7}
$$

In Fig. 3, the symbols  $\odot$  and  $\circ$  represent element-wise multiplication and ternary matrix multiplication, respectively. Let the dimensionality of the vector be  $d$ , the index of the layer is  $k$ , the vector  $\pmb{x}_t \in \mathbb{R}^d$  is input from the previous component at time step  $t$ , and  $\pmb{h}_t \in \mathbb{R}^d$  is the internal state vector. The matrices  $\pmb{W}_c, \pmb{W}_f, \pmb{W}_o, \pmb{W}_g \in \mathbb{R}^{d \times d}$  are ternary weight matrices, while  $\pmb{b}_c, \pmb{b}_f, \pmb{b}_o, \pmb{b}_g \in \mathbb{R}^d$  are the corresponding biases terms. The vector  $\mathbf{1}$  refers to a vector where all components are one. The vectors  $\pmb{c}_t, \pmb{f}_t, \pmb{f}_t', \gamma^k, \pmb{g}_t, \pmb{o}_t', \pmb{o}_t \in \mathbb{R}^d$  represent input vector, forget gate, updated forget gate, lower bound vector

![](images/132c8a5885ae1956dce1129a4973af484f338f0e03eeaa18240af9665ce29df9.jpg)  
Fig. 4. GLU

for the k-th layer, output gate, intermediate output, and final output, respectively.  $\sigma$  denotes the sigmoid activation function, and  $\tau$  denotes the sigmoid-weighted linear unit (SiLU) activation function [48]. Compared to the traditional equations of long short-term memory (LSTM) or GRU, many dependencies on the previous hidden state  $h_{t-1}$  are intentionally removed in this architecture to improve execution speed and computational efficiency. Nevertheless, the model still has a large number of parameters and requires considerable computational cost.

Equation (2) shows the calculation for adjusting  $f_{t}$  to  $f_{t}^{\prime}$  so that it exceeds the lower bound. To obtain the variable  $\gamma^{k}$ , the model employs the cumax activation function [31] during training to calculate the forget gate values of the upper layers closer to 1, as in HGRN [30]. This cumax activation function, used only during training to calculate the lower bounds, is defined in (8) to (10).

$$
\boldsymbol {P} = \operatorname {S o f t m a x} (\boldsymbol {\Gamma}, \dim = 0), \tag {8}
$$

$$
\gamma^ {k} = \operatorname {c u m a x} (\Gamma) = [ \operatorname {C u m s u m} (\boldsymbol {P}, \dim = 0) ] _ {k}, \tag {9}
$$

$$
\left[ \operatorname {C u m s u m} (z) \right] _ {k} = \left(\Sigma_ {i = 1} ^ {k} z _ {i}\right) - z _ {1}. \tag {10}
$$

The matrix  $\Gamma \in \mathbb{R}^{N\times d}$  is a trainable weight matrix used to compute  $\gamma^k$ , where  $N$  is the total number of layers. The intermediate output is represented by  $P\in \mathbb{R}^{N\times d}$ . The cumulative sum (cumsum), as defined in (10), is used in this calculation.  $z$  denotes vectors that are used as inputs of cumsum. During inference, the model uses the  $\gamma$  values precomputed and stored in memory, performing only the update process described in (2).

Fig. 4 shows the GLU component, which is used as the FFN in this architecture, and (11) to (14) define its computational process.

$$
\boldsymbol {s} _ {t} = \chi_ {t} * \boldsymbol {W} _ {s}, \tag {11}
$$

$$
\boldsymbol {u} _ {t} = \chi_ {t} * \boldsymbol {W} _ {u}, \tag {12}
$$

$$
\boldsymbol {p} _ {t} = \tau (\boldsymbol {s} _ {t}) \odot \boldsymbol {u} _ {t}, \tag {13}
$$

$$
\boldsymbol {q} _ {t} = \boldsymbol {p} _ {t} * \boldsymbol {W} _ {q}, \tag {14}
$$

In the GLU, the following three-step process is performed:

1) Map inputs of the GLU,  $\chi_t \in \mathbb{R}^d$  to higher dimensions into  $s_t$  and  $\pmb{u}_t \in \mathbb{R}^l$  through the gate projection and up projection using  $\pmb{W}_s$ ,  $\pmb{W}_u \in \mathbb{R}^{d \times l}$ , respectively.

![](images/cc1773d2937a48be837bd87f37a7f02a00f9088f3dbbf4a843236ffec085a0d4.jpg)  
Fig. 5. Repeating module of ternary LI ESN

2) Perform pointwise multiplication between  $\pmb{g}_t$ , obtained by applying the SiLU activation function, and  $\pmb{u}_t$  applied to obtain representation  $\pmb{p}_t$ .  
3) Map  $\pmb{p}_t$  into  $\pmb{q}_t\in \mathbb{R}^d$  through a down projection using  $W_{q}\in \mathbb{R}^{d}$

# D. RC

RC is a type of RNN that consists of only three layers: input layer, reservoir layer, and output (readout) layer. In contrast to deep learning, RC offers significantly lower training costs as it optimizes only the output layer parameters. Despite this simplicity, RC achieves competitive performance in various tasks. Furthermore, incorporating the concept of the leaky integrator (LI) model enables the adjustment of the internal state dynamics of the reservoir to suit the task.

Fig. 5 and (15) to (17) show a repeating module and processing of the ternary LI echo state network (ESN) [20], a type of RC, respectively. In Fig. 5,  $\text{串} ^ { \prime }$  denotes a ternary matrix multiplication with fixed weights, while  $\text{串}$  denotes a ternary matrix multiplication with trainable weights.

$$
\boldsymbol {c} _ {t} = \tanh  \left(\boldsymbol {x} _ {t} * \overline {{\boldsymbol {W}}} _ {c} + \boldsymbol {h} _ {t - 1} * \frac {\overline {{\boldsymbol {W}}} _ {r}}{\lambda_ {r} ^ {m a x}} + \boldsymbol {b} _ {c}\right), \tag {15}
$$

$$
\boldsymbol {h} _ {t} = f \boldsymbol {h} _ {t - 1} + (1 - f) \boldsymbol {c} _ {t}, \tag {16}
$$

$$
\boldsymbol {o} _ {t} = \boldsymbol {h} _ {t} * \boldsymbol {W} _ {o} + \boldsymbol {b} _ {o}. \tag {17}
$$

Here,  $\overline{W}_c$  is a ternary matrix with fixed weights,  $\overline{W}_r$  is a ternary sparse matrix with fixed weights, and  $f$  represents the forget gate.  $\lambda_r^{max}$  is the maximum eigenvalue of  $\overline{W}_r, c_t, o_t \in \mathbb{R}^d$  are input vector and output vector, respectively, and  $h_t \in \mathbb{R}^l$  is the internal state vector.  $b_c, b_o \in \mathbb{R}^d$  are the biases. As shown from (15) to (17), the LI ESN is differentiable.

RC possesses echo state property, which ensures reproducibility in time-series signal processing. In other words, the reservoir dynamics exhibit a fading memory behavior, meaning that the input history uniquely determines the current state, while the influence of remote past inputs gradually decays. This property is satisfied in ESNs with tanh activation if the spectral radius, the maximum absolute eigenvalue of  $\overline{W}_r$  is

![](images/87cd9d8c3fdd83eed1a5a0c7e50a045495cf71b501580b3a083885b291d2ab7a.jpg)  
Fig. 6. RC MatMul-free LM

![](images/8ceb3c7529c948500c1e660fff369edc898e333a1e5782550ce8eaafcf4b8d9d.jpg)  
Fig. 7. Repeating module of MLGRU-RC

less than 1 [20]. Therefore, division by  $\lambda_r^{max}$  is applied to scale the spectral radius [49].

Due to the fading memory characteristics in reservoirs and the absence of gating mechanisms, a simple ESN often struggles to train long-term dependencies. To mitigate this limitation, the gated ESN [50], which introduces gating mechanism into the ESN, has been proposed. This model can be categorized into two types: one that trains only the output layer, and the other where the weights of the gate layer are trained by gradient descent. Both architectures exhibit superior performance relative to a simple ESN in natural language processing tasks including long-term dependencies.

# III. PROPOSED METHOD

# A. RC MatMul-free LM

In this study, we propose RC MatMul-free LM, in which MLGRU of MatMul-free LM is replaced with MLGRU-RC inspired by RC. Fig. 6 shows the architecture of this model. As shown in Fig. 6, we name the decoder block of RC MatMul-free LM as the RCformer block. Fig. 7 shows the repeating module of MLGRU-RC. The ternary matrix multiplication indicated by  $\circ^{\prime}$  in Fig. 7 uses fixed random weights and shares the weights across all layers. This aims to reduce the number of parameters and computation time. In addition, this approach attempts to improve performance by adding a dependency on the internal state  $h_{t - 1}$  to the processing of MatMul-free LM.  $\circ$  is a ternary logical matrix multiplication, which is updated via backpropagation similar to MatMul-free LM.

![](images/01b760100d56f3451ad0c880e6bd4c4e7383c6b39b3c53a61bc9a8b5b76560e1.jpg)  
Fig. 8. Recurrent kernel of MatMul-free LM

The proposed method modifies (3) of the processing of MLGRU, shown in (1) to (7), according to (18).

$$
\boldsymbol {c} _ {t} = \tau \left(\boldsymbol {x} _ {t} * \overline {{\boldsymbol {W}}} _ {c} + \boldsymbol {h} _ {t - 1} * \frac {\overline {{\boldsymbol {W}}} _ {r}}{\lambda_ {r} ^ {m a x}} + \boldsymbol {b} _ {c}\right), \tag {18}
$$

Here,  $\overline{W}_r$  is a fixed sparse ternary weight matrix. Also,  $\lambda_r^{max}$  is the maximum eigenvalue of  $\overline{W}_r$ . This approach omits the computation of the gradients  $\frac{\partial L}{\partial \overline{W}_c}$  and  $\frac{\partial L}{\partial \overline{W}_r}$  with respect to  $\overline{W}_c$  and  $\overline{W}_r$  when training this network, which lets  $L$  as the loss function.

# B. Kernel Optimization of RC MatMul-free LM

RC MatMul-free LM reduced unnecessary memory read and write by fusing operations, which includes activation functions into the recurrent processing to achieve acceleration. Fig. 8 shows a kernel of MatMul-free LM before improvement, which executes the operations corresponding to the area enclosed by the purple dashed line in Fig. 7. Similarly, Fig. 9 shows a kernel of RC MatMul-free LM after improvement. Blocks highlighted in blue and outlined by black borders in Figs. 8 and 9 are parameter variables, and arrows connected to the blocks represent the read and write operations of the parameters. As shown in Fig. 8, MatMul-free LM utilizes two kernels that cause extra reads and writes of  $\boldsymbol{f}_t$  and  $\boldsymbol{c}_t$ . In RC MatMul-free LM, as shown in Fig. 9, this approach integrated these operations into a single kernel. This integration reduces the redundancy of memory access and the overhead associated with kernel launches. Since the reads of  $\lambda_r^{max}$  and  $W_r$  shown in Fig. 9 are not recurrent, their impact on processing time is almost negligible.

Furthermore, this improvement enables Triton [51] to optimize all operations shown in Fig. 9. Consequently, this approach also reduces the processing time for matrix multiplication with  $W_r$  and calculations of an activation function.

# C. GRC MatMul-freeLM

To further reduce training time and the number of parameters, we propose a gated RC MatMul-free LM (GRC MatMul-free LM), which fixed  $W_{f}$  and  $W_{g}$  and shared them across

![](images/59dc4c5fa515136f8d38fc35b8727791055f099507ef5bebebf7ea2ea702fb35.jpg)  
Fig. 9. Recurrent kernel of RC MatMul-free LM

all layers. In GRC MatMul-free LM, we extend (1) and (5) of RC MatMul-free LM, which are formulated as (19) and (20).

$$
\boldsymbol {f} _ {t} = \sigma \left(\boldsymbol {x} _ {t} * \overline {{\boldsymbol {W}}} _ {\boldsymbol {f}} + \boldsymbol {b} _ {f}\right), \tag {19}
$$

$$
\boldsymbol {g} _ {t} = \sigma \left(\boldsymbol {x} _ {t} * \overline {{\boldsymbol {W}}} _ {\boldsymbol {g}} + \boldsymbol {b} _ {g}\right), \tag {20}
$$

Similar to  $\overline{W}_c$ , fixed ternary weight matrices  $\overline{W}_f$  and  $\overline{W}_g$  are initialized with random values; consequently, computing their corresponding gradients  $\frac{\partial L}{\partial \overline{W}_f}$  and  $\frac{\partial L}{\partial \overline{W}_g}$  is not required.

Although fixing weights in RNNs with only the same-dimensional mappings generally tends to degrade performance, we consider that fixing  $W_{f}$  retains its function as a forget gate. That is because  $f_{t}^{\prime}$  monotonically increases as the network becomes deeper, resulting in the shallow and deep layers capturing short-term and long-term dependencies, respectively. Furthermore,  $f_{t}^{\prime}$  is tuned coarsely via  $\gamma^{k}$  that is computed from trainable  $\Gamma$ . Therefore, we adopt this extension. Regarding  $g_{t}$ , we also apply this extension. This is inspired by the observation that fixing and sharing all attention mechanism parameters makes minimal impact on performance [24]. Fig. 10 shows the repeating module of MLGRU-RC in GRC MatMul-freeLM. As with RC MatMul-free LM, we also optimize the kernel in GRC MatMul-free LM.

# IV. EXPERIMENT

In this experiment, we developed training programs for RC MatMul-free LM and GRC MatMul-free LM using PyTorch [52] and Triton [51]. Table I summarizes the experimental settings. In Table I, sparsity denotes the sparsity rate of  $\overline{W}_r$ , and context size shows the maximum number of tokens the model can process at once. The reason for using a smaller context size compared to that in previous work [16] is to compare model performance attributable to the architecture within a shorter training time. SlimPajama dataset shown in Table I is a large-scale natural language dataset containing 627B tokens. For this experiment, we used randomly sampled

![](images/dc6fdb1e2eacebffa2ee4387b6b2545a1f10cd135e66c8b4a9202a98c5fb70a3.jpg)  
Fig. 10. Repeating module of gated MLGRU-RC

data from the 627B tokens. Regarding random seeds for CUDA, we used the same value across all experiments.

For the benchmark tasks, we adopted ARC Easy (ARCe), ARC Challenge (ARCc) [53], HellaSwag (Hs) [54], Openbook QA (OQ) [55], PIQA (PQ) [56], and Winogrande (WGe) [57]. We evaluated these tasks using LM Evaluation Harness [58], a commonly used framework for consistent evaluation of multiple benchmark tasks in LLM performance assessment.

# A. Learning Rate

Since MatMul-free LM [16] reported that 0.01 is the best learning rate, this study explored parameters around this value and adopted  $\frac{0.01}{\sqrt[8]{8}}$  as the learning rate at which training converged with batch size 256 in this experiment. We used a cosine scheduler as the learning rate scheduler.

# B. Result

In this experiment, we evaluated two types of weights from the xavier uniform [60] distribution and averaged the results across these runs. Tables II, III, and IV show the details of the models and computation time, the loss of models, and the benchmark scores with their corresponding averages, respectively. In Table II, the values in parentheses under size indicate the number of fixed parameters that are not updated during training. For memory usage of parameters, we calculated the memory usage required for parameters during inference. We computed the memory size of ternary parameters as  $\log_23$ -bit.

Table II confirms that improving MatMul-free LM to RC MatMul-free LM or GRC MatMul-free LM reduces the computation time for both training and inference. Specifically, for training, RC MatMul-free LM reduces the time to  $3.9\%$ , and GRC MatMul-free LM reduces it to  $9.9\%$ . For inference, RC MatMul-free LM and GRC MatMul-free LM reduce the time to  $6.1\%$  and  $8.0\%$ , respectively. Regarding memory usage, 62.5 MB out of the total memory consumption shown in the table corresponds to the memory usage for the embedding layer. Meanwhile, Table III and IV show that the performance differences between MatMul-free LM, RC MatMul-free LM, and GRC MatMul-free LM are minimal, confirming that the improvements maintain model performance.

# V. DISCUSSION

Table II shows a reduction in model size, training time, and inference time. We attribute this decrease to parameter fixing, parameter sharing, and kernel optimizations, all of which reduce the number of backpropagation operations and memory accesses. Although the ratio appears small, scaling RC MatMul-free LM to a size comparable to the practical Llama3 405 B would translate that reduction into a saving of 2 to 5 days of training time.

Conversely, Tables III and IV show that MatMul-free LM suffers almost no performance degradation even when fixing and sharing its parameters, similar to the full-bit transformer. This result implies that RNN-based LLM or any ternary LLM will likewise lose little accuracy under parameter fixing or sharing, and that even a token-mixer component inspired by RC could remain functional.

Because this study confirms the effectiveness of parameter fixing for QAT of ternary LLMs, we expect to shorten training time further by reducing memory accesses through the same kernel optimizations used at inference time [38]. Specifically, the frozen linear functions use  $\log_2$  3-bit rather than 16-bit weights; by packing weights and reading or writing them together, we can reduce the number of memory accesses to approximately  $\frac{1}{10}$  times that of the 16-bit case. We can achieve this improvement by adapting the ternary optimized linear functions already employed during inference.

We confirmed that the network can still train when we use a component inspired by RC as the token-mixer. Future work will therefore explore incorporating existing acceleration and performance enhancement techniques for RC into RC MatMul-free LM.

The layers whose parameters we froze act as random, same-dimensional mappings, so there remains room for further performance improvement. We need to consider introducing higher-dimensional mappings. However, a naive introduction increases the number of parameters in the unshared output layers and hence the total model size. So we must devise alternatives for those layers.

Although we did not verify it experimentally, the fact that the only changes are parameter fixing and the addition of new layers—while maintaining comparable performance—suggests that RC MatMul-free LM, like MatMul-free LM, should obey the usual scaling laws.

We trained RC MatMul-free LM and performed inference on GPUs using PyTorch [52] and Triton [51]. Because RC lends itself to physical implementation, and RC MatMul-free LM was developed with field programmable gate array deployment in mind, we likewise expect physical or circuit implementations of RC MatMul-free LM.

# VI. CONCLUSION

In this study, we propose RC MatMul-free LM, which introduces the concept of RC into MatMul-free LM to reduce the number of parameters and memory usage, one of the main causes of the high computational cost that hinders the practical deployment of large language models. In the

# TABLEI

# SETTING OF EXPERIMENTS

<table><tr><td>Dataset</td><td>Tokenizer</td><td>Number of Token (Train / Eval)</td><td>Machine</td><td>Sparsity of reservoir layer</td><td>Context Size</td></tr><tr><td>SlimPajama [59]</td><td>Mistral</td><td>15 B/541 M</td><td>One H100 GPU</td><td>85%</td><td>128</td></tr></table>

TABLE II SIZE AND RUNTIME OF EACH MODEL  

<table><tr><td>Models</td><td>Total size [M] (Size of fixed [M])</td><td>Memory usage of parameters [MB]</td><td>Train runtime [h]</td><td>Eval runtime [min]</td></tr><tr><td>MatMul-free LM [16]</td><td>374</td><td>127</td><td>73.61</td><td>43.68</td></tr><tr><td>RC MatMul-free LM (III-A)</td><td>351(2)</td><td>122</td><td>70.77</td><td>41.00</td></tr><tr><td>GRC MatMul-free LM (III-C)</td><td>303(4)</td><td>113</td><td>66.32</td><td>40.18</td></tr></table>

TABLE III LOSS OF EACH MODEL  

<table><tr><td>Models</td><td>Train loss</td><td>Eval loss</td></tr><tr><td>MatMul-free LM [16]</td><td>3.291</td><td>2.995</td></tr><tr><td>RC MatMul-free LM (III-A)</td><td>3.349</td><td>3.048</td></tr><tr><td>GRC MatMul-free LM (III-C)</td><td>3.476</td><td>3.153</td></tr></table>

TABLE IV BENCHMARK SCORE OF EACH MODEL  

<table><tr><td>Models</td><td>ARCc</td><td>ARCe</td><td>Hs</td><td>OQ</td><td>PQ</td><td>WGe</td><td>Avg.</td></tr><tr><td>MatMul-free LM [16]</td><td>23.8</td><td>42.4</td><td>34.0</td><td>29.6</td><td>63.4</td><td>48.8</td><td>40.3</td></tr><tr><td>RC MatMul-free LM (III-A)</td><td>23.3</td><td>42.1</td><td>32.2</td><td>27.3</td><td>62.0</td><td>49.7</td><td>39.4</td></tr><tr><td>GRC MatMul-free LM (III-C)</td><td>23.5</td><td>41.5</td><td>30.6</td><td>29.0</td><td>61.2</td><td>50.7</td><td>39.4</td></tr></table>

proposed RC MatMul-free LM, we replace MLGRU in the previous MatMul-free LM with MLGRU-RC, an RC-based component, thereby achieving inter-layer parameter fixing and sharing.

Experiments on the large-scale natural-language dataset SlimPajama demonstrate that, compared with the previous MatMul-free LM, the proposed RC MatMul-free LM reaches comparable performance while reducing the parameter count by up to  $19\%$  and shortening training and inference times by  $9.9\%$  and  $8.0\%$ , respectively.

These results constitute a significant step toward resolving the computational cost problem of LLMs.

# ACKNOWLEDGMENTS

This research is based on results obtained from a project, JPNP16007, commissioned by the New Energy and Industrial Technology Development Organization (NEDO). This work received support from JSPS KAKENHI Grant Number 23H03468, as well as from JST ALCANext Grant Number JPMJAN23F3.

# REFERENCES

[1] A. Grattafori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, et al., “The Llama 3 Herd of Models,” 2024, arXiv: 2407.21783.  
[2] DeepSeek-AI, A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, et al., "DeepSeek-V3 Technical Report," 2025, arXiv: 2412.19437.

[3] Q. Khraisha, S. Put, J. Kappenberg, A. Warraitch, and K. Hadfield, "Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages," Research synthesis methods, pp. 616–626, 2024.  
[4] A. Louis, G. van Dijck, and G. Spanakis, "Interpretable Long-Form Legal Question Answering with Retrieval-Augmented Large Language Models," Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, pp. 22266-22275, Mar. 2024.  
[5] B. Padiu, R. Iacob, T. Rebedea, and M. Dascalu, “To what extent have llms reshaped the legal domain so far? a scoping literature review,” Information, vol. 15, no. 11, p. 662, 2024.  
[6] T. Ridnik, D. Kredo, and I. Friedman, "Code Generation with Alpha-Codium: From Prompt Engineering to Flow Engineering," 2024, arXiv: 2401.08500.  
[7] Y. Zheng, W. Gan, Z. Chen, Z. Qi, Q. Liang, and P. S. Yu, "Large Language Models for Medicine: A Survey," 2024, arXiv: 2405.13055.  
[8] M. Liu, W. Hu, J. Ding, J. Xu, X. Li, L. Zhu, Z. Bai, X. Shi, B. Wang, H. Song, P. Liu, X. Zhang, S. Wang, K. Li, H. Wang, T. Ruan, X. Huang, X. Sun, and S. Zhang, "MedBench: A Comprehensive, Standardized, and Reliable Benchmarking System for Evaluating Chinese Medical Large Language Models," Big Data Mining and Analytics, vol. 7, no. 4, pp. 1116-1128, 2024.  
[9] F. Zeng, W. Gan, Y. Wang, N. Liu, and P. S. Yu, “Large Language Models for Robotics: A Survey,” 2023, arXiv: 2311.07226.  
[10] K. Yamao, D. Kanaoka, K. Isomoto, A. Mizutani, Y. Tanaka, and H. Tamukoh, "Development of A SayCan-based Task Planning System Capable of Handling Abstract Nouns," in Proceedings of International Conference on Artificial Life & Robotics (ICAROB2024), pp. OS15-4, ALife Robotics, 2024.  
[11] K. Yamao, D. Kanaoka, K. Isomoto, and H. Tamukoh, "A General Purpose Service Robot System Capable of Handling Commands Containing

Abstract Nouns," in 2024 IEEE International Conference on Robotics and Automation (ICRA), 2024. p.ThBL-EX.11, 2024.  
[12] Y. Yano, A. Mizutani, Y. Fukuda, D. Kanaoka, T. Ono, and H. Tamukoh, "Unified Understanding of Environment, Task, and Human for Human-Robot Interaction in Real-World Environments," in 2024 33rd IEEE International Conference on Robot and Human Interactive Communication (ROMAN), pp. 224–230, 2024.  
[13] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale," 2022, arXiv: 2208.07339.  
[14] J. Koo, D. Park, S. Jung, and J. Kung, “OPAL: Outlier-Preserved Microscaling Quantization Accelerator for Generative Large Language Models,” 2024, arXiv: 2409.05902.  
[15] C. Moar, F. Tahmasebi, M. Pellauer, and H. Kwon, "Characterizing the Accuracy - Efficiency Trade-off of Low-rank Decomposition in Language Models," 2024, arXiv: 2405.06626.  
[16] R.-J. Zhu, Y. Zhang, E. Siferman, T. Sheaves, Y. Wang, D. Richmond, P. Zhou, and J. K. Eshraghian, "Scalable MatMul-free Language Modeling," 2024, arXiv: 2406.02528.  
[17] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko, “Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference,” pp. 2704–2713, 06 2018.  
[18] Y. Bhalgat, J. Lee, M. Nagel, T. Blankevoort, and N. Kwak, "LSQ+: Improving low-bit quantization through learnable offsets and better initialization," 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 2978-2985, 2020.  
[19] N. Wang, C.-C. C. Liu, S. Venkataramani, S. Sen, C.-Y. Chen, K. El Maghraoui, V. V. Srinivasan, and L. Chang, "Deep Compression of Pre-trained Transformer Models," in Advances in Neural Information Processing Systems (S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, eds.), vol. 35, pp. 14140-14154, Curran Associates, Inc., 2022.  
[20] H. Jaeger, "Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the echo state network approach", vol. 5. Citeseer, 2002.  
[21] P. Vlachas, J. Pathak, B. Hunt, T. Sapsis, M. Girvan, E. Ott, and P. Koumoutsakos, "Backpropagation algorithms and Reservoir Computing in Recurrent Neural Networks for the forecasting of complex spatiotemporal dynamics," Neural Networks, vol. 126, pp. 191-217, 2020.  
[22] Y. Usami, B. van de Ven, D. G. Mathew, T. Chen, T. Kotooka, Y. Kawashima, Y. Tanaka, Y. Otsuka, H. Ohoyama, and H. Tamukoh, "In-materio reservoir computing in a sulfonated polyaniline network," Advanced Materials, vol. 33, no. 48, p. 2102688, 2021.  
[23] K. Yoshioka, Y. Tanaka, and H. Tamukoh, "LUTNet-RC: Look-Up Tables Networks for Reservoir Computing on an FPGA," in 2023 International Conference on Field Programmable Technology (ICFPT), pp. 170-178, 2023.  
[24] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, “ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,” 2020, arXiv: 1909.11942.  
[25] S. Shen, A. Baevski, A. Morcos, K. Keutzer, M. Auli, and D. Kiela, "Reservoir Transformers," in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4294-4309, 2021.  
[26] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need," in Advances in Neural Information Processing Systems 30 (NIPS 2017), p. 6000-6010, 2017.  
[27] W. Yu, M. Luo, P. Zhou, C. Si, Y. Zhou, X. Wang, J. Feng, and S. Yan, "MetaFormer is Actually What You Need for Vision," in 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10809-10819, 2022.  
[28] A. Gu and T. Dao, “Mamba: Linear-Time Sequence Modeling with Selective State Spaces,” in First Conference on Language Modeling, 2024.  
[29] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, S. Biderman, H. Cao, X. Cheng, M. Chung, L. Derczynski, X. Du, M. Grella, K. Gv, X. He, H. Hou, P. Kazienko, J. Kocon, J. Kong, B. Koptyra, H. Lau, J. Lin, K. S. I. Mantri, F. Mom, A. Saito, G. Song, X. Tang, J. Wind, S. Wozniak, Z. Zhang, Q. Zhou, J. Zhu, and R.-J. Zhu, "RWKV: Reinventing RNNs for the Transformer Era," in Findings of the

Association for Computational Linguistics: EMNLP 2023, pp. 14048-14077, 2023.  
[30] Z. Qin, S. Yang, and Y. Zhong, "Hierarchically Gated Recurrent Neural Network for Sequence Modeling," 2023, arXiv: 2311.04823.  
[31] Y. Shen, S. Tan, A. Sordoni, and A. C. Courville, “Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks,” ArXiv, vol. abs/1810.09536, 2018.  
[32] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, "Scaling Laws for Neural Language Models," 2020, arXiv: 2001.08361.  
[33] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, "QLORA: efficient finetuning of quantized LLMs," in Proceedings of the 37th International Conference on Neural Information Processing Systems, pp. 10088-10115, 2023.  
[34] B. Liao, C. Herold, S. Khadivi, and C. Monz, "ApiQ: Finetuning of 2-Bit Quantized Large Language Model," in Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (Y. Al-Onaizan, M. Bansal, and Y.-N. Chen, eds.), (Miami, Florida, USA), pp. 20996-21020, Association for Computational Linguistics, Nov. 2024.  
[35] M. Kim, S. Lee, W. Sung, and J. Choi, "RA-LoRA: Rank-Adaptive Parameter-Efficient Fine-Tuning for Accurate 2-bit Quantized Large Language Models," in Findings of the Association for Computational Linguistics ACL 2024, pp. 15773-15786, 2024.  
[36] H. Qin, X. Ma, X. Zheng, X. Li, Y. Zhang, S. Liu, J. Luo, X. Liu, and M. Magno, "Accurate LoRA-finetuning quantization of LLMs via information retention," in Proceedings of the 41st International Conference on Machine Learning, ICML'24, JMLR.org, 2024.  
[37] Google Cloud, "Bfloat16: The Secret to High Performance on Cloud TPUs." https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud,tpus?hl=en, 2019. Accessed: 2025-04-09.  
[38] E. Frantar, S. Ashkboos, T. Hoefler, and D.-A. Alistarh, “OPTQ: Accurate post-training quantization for generative pre-trained transformers,” in 11th International Conference on Learning Representations, 2023.  
[39] H. Wang, S. Ma, L. Dong, S. Huang, H. Wang, L. Ma, F. Yang, R. Wang, Y. Wu, and F. Wei, "BitNet: Scaling 1-bit Transformers for Large Language Models," 2023, arXiv: 2310.11453.  
[40] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, and F. Azhar, "Llama: Open and efficient foundation language models," 2023, arXiv: 2302.13971.  
[41] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov and T. Scialom, "Llama 2: Open Foundation and Fine-Tuned Chat Models," 2023, arXiv: 2307.09288.  
[42] S. Ma, H. Wang, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, R. Wang, J. Xue, and F. Wei, "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits," 2024, arXiv: 2402.17764.  
[43] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation," in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1724–1734, 2014.  
[44] M. Sun, Z. Liu, A. Bair, and J. Z. Kolter, "A Simple and Effective Pruning Approach for Large Language Models," 2024, arXiv: 2306.11695.  
[45] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, "Language modeling with gated convolutional networks," in International conference on machine learning, pp. 933-941, 2017.  
[46] N. Shazeer, “Glu variants improve transformer,” 2020, arXiv: 2002.05202.  
[47] B. Zhang and R. Sennrich, Root mean square layer normalization. Red Hook, NY, USA: Curran Associates Inc., 2019.

[48] S. Elfwing, E. Uchibe, and K. Doya, "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning," Neural Networks, vol. 107, 01 2018.  
[49] K. Honda and H. Tamukoh, “A hardware-oriented echo state network and its FPGA implementation,” Journal of Robotics, Networking and Artificial Life, vol. 7, no. 1, pp. 58–62, 2020.  
[50] D. Di Sarli, C. Gallicchio, and A. Micheli, "On the effectiveness of Gated Echo State Networks for data exhibiting long-term dependencies," Computer Science and Information Systems, vol. 19, pp. 63-63, 01 2021.  
[51] OpenAI, "Triton: An open-source deep learning compiler." https://github.com/openai/triton, 2021.  
[52] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, "PyTorch: An Imperative Style, High-Performance Deep Learning Library," 2019, arXiv: 1912.01703.  
[53] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord, “Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge,” 2018, arXiv: 1803.05457.  
[54] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, "HellaSwag: Can a Machine Really Finish Your Sentence?", in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791-4800, 2019.  
[55] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering,” in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2381-2391, 2018.  
[56] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, “PIQA: Reasoning about Physical Commonsense in Natural Language,” 2019, arXiv: 1911.11641.  
[57] K. Sakaguchi, R. Bras, C. Bhagavatula, and C. Yejin, "WinoGrande: An Adversarial Winograd Schema Challenge at Scale," Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, pp. 8732-8740, 2020.  
[58] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, A. Le Noac'h, H. Li, K. McDonell, N. Muen-nighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou, "A framework for few-shot language model evaluation," 2024.  
[59] D. Soboleva, F. Al-Khateeb, R. Myers, J. R. Steeves, J. Hestness, and N. Dey, "SlimPajama: A 627B token cleaned and deduplicated version of RedPajama." https://cerebras.ai/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, June 2023.  
[60] X. Glorot and Y. Bengio, “Understanding the difficulty of training deep feedforward neural networks,” in Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (Y. W. Teh and M. Titterington, eds.), vol. 9 of Proceedings of Machine Learning Research, (Chia Laguna Resort, Sardinia, Italy), pp. 249–256, PMLR, 13–15 May 2010.