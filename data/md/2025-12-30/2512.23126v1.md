# InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization

Yu Li, Tian Lan, Zhengling Qi  
George Washington University

# Abstract

Direct Preference Optimization (DPO) and its variants have become the standard for aligning Large Language Models (LLMs) due to their simplicity and offline stability. However, we identify two fundamental limitations that undermine the reliability and optimality of DPO and its equivalents. First, the optimized policy derived by current methods lacks invariance: It changes with respect to modeling choices such as the scalarization function (e.g., logistic function in Bradley-Terry model) or the reference policy. This may yield brittle behaviors of the learned policy as an artifact of parameterization rather than a reflection of true human preference, while an optimal policy must satisfy the invariance property. Second, we show that the derived policy of most existing methods is theoretically suboptimal because it fails to fully capitalize on the comparative information embedded in the pairwise preference data. This restricts the model's ability to "compare and contrast" responses, illuminating a unique opportunity for triggering intrinsic self-reflection. In this work, we propose a novel family of Intrinsic Self-reflective Preference Optimization (InSPO) methods, which address these two limitations. We first derive a globally optimal policy that conditions on both the context and the alternative response under the pairwise preference data setting, which explicitly formalize the novel notion of self-reflection. Then we theoretically demonstrate that this formulation is superior to standard DPO and RLHF targets and guarantees invariance to the choice of scalarization and reference policy. Practically, InSPO operationalizes this target as a plug-and-play enhancement for DPO-family algorithms, decoupling the alignment goal from modeling constraints without requiring complex architectural changes. Crucially, our method incurs zero extra inference overhead, as the self-reflective mechanism is distilled into the policy during training and does not require generating alternative responses during deployment. Comprehensive experiments demonstrate that InSPO (implemented on a range of DPO-family algorithms) achieves consistent improvements in win rates and length-controlled metrics, validating that InSPO with self-reflection leads to more robust and human-aligned LLMs.

# 1 Introduction

Large language models (LLMs) are fine-tuned after pretraining through post-training alignment. This stage typically employs two complementary families of techniques [Kumar et al., 2025]. The first technique is supervised fine-tuning (SFT), which fine-tunes the model to generate the correct response given an instruction, typically by minimizing the negative log-likelihood of the target tokens. SFT effectively imparts formatting conventions, stylistic preferences, and basic task-following behaviors [Dong et al., 2023]. The second technique is Reinforcement Learning from Human Feedback (RLHF), which aligns the model by learning from human pairwise preference data [Ouyang et al., 2022]. This standard pipeline operates in two stages: first, a reward model is fitted to approximate human preferences, where the Bradley-Terry (BT) model is imposed to link the human preference probability with the reward contrast between two responses via the logistic function. Second, the LLM is fine-tuned by maximizing the learned reward with a penalty on the Kullback-Leibler (KL) divergence between the trainable policy and the reference one. The KL penalty is used to stabilize training and prevent overfitting, which is motivated by Proximal Policy Optimization

(PPO) [Schulman et al., 2017]. While RLHF shows promising results, it requires iterative on-policy sampling, coordinated training of reward, and careful hyperparameter tuning. The complexity and computational overhead often complicate the deployment in practice [Mei et al., 2023, Yu et al., 2025].

Direct Preference Optimization (DPO), in contrast, simplifies this process by removing both the explicit reward model and the online rollout phase. Given a preference pair where one response is preferred over another for a given context, DPO associates a latent preference score with log-likelihood ratios between the trainable policy and a reference model and directly maximizes the margin that favors the preferred response under a logistic loss. This equivalent reformulation preserves the KL regularization implicit in RLHF while operating purely offline on the static preference pairs. The success of DPO has inspired a growing family of reward-free preference optimization methods that refine various aspects of the optimization objective, such as adjusting the loss curvature [Azar et al., 2024], incorporating length normalization [Meng et al., 2024], or modifying the reference formulation [Ethayarajh et al., 2024], to further improve DPO.

Despite the impressive performance of RLHF and DPO-based approaches, we show that these existing methods are subject to two fundamental limitations that undermine the reliability and optimality of the learned policy. First, the derived optimal policy lacks invariance to modeling choices including the scalarization function (e.g., the logistic function in the BT model) and the reference policy. Ideally, the aligned behavior of an LLM should reflect the ground-truth human preference distribution, independent of the mathematical formulation used to process those preferences. However, as we formally demonstrate, the putative "optimal" target in current frameworks shifts with these design choices. Consequently, methods such as DPO and RLHF often produce behavior that reflects parameterization artifacts rather than genuine human preferences.

Second, these methods are theoretically suboptimal because they fail to exploit the comparative information embedded in pairwise preference data, effectively leaving the model's inherent capacity for self-reflection untapped. In particular, current approaches cast response generation as an isolated maximization problem, disregarding the fundamentally contextual nature of human preference, which is shaped by interactions between responses rather than their standalone quality [Tversky and Simonson, 1993]. By being oblivious to explicitly conditioning on an alternative response during the training, existing methods limit its ability to "compare and contrast", a mechanism we term intrinsic self-reflection, thereby capping the potential quality of the fine-tuned policy. These two limitations raise a fundamental question for LLM preference optimization:

# A Key Question

How can we construct a new preference optimization framework that is both invariant to modeling choices and capable of fully exploiting the comparative nature of human preferences?

In this work, we propose a new perspective on preference optimization that fully leverages the comparative information embedded in pairwise preference data, which is a rich signal that can be effectively capitalized using a novel notion of self-reflection. To this end, we first derive a globally optimal policy that conditions on not only the context but also the alternative response. Then, we mathematically demonstrate that this formulation yields a target superior to standard RLHF and DPO, while enjoying invariance to the choice of scalarization and reference policy. The results enable us to develop a novel framework that unlocks Intrinsic Self-reflective Preference Optimization (InSPO) for existing methods using the standard pairwise preference data. Our InSPO is a plug-and-play enhancement for the DPO family that is both theoretically rigorous and computationally efficient. It effectively decouples the alignment goal from arbitrary modeling constraints and unlocks the intrinsic self-reflection capability. Crucially, the deployment of our method remains standard and does not require generating an alternative response at the inference time, as the self-reflective mechanism is distilled into the policy during training and does not require generating alternative responses during deployment. Lastly, we validate through comprehensive experiments on a range of DPO-family algorithms, showing consistent improvements in win rates and controlled metrics across benchmarks; our analysis reveals that the proposed method induces dense reward shaping and scales effectively with model capacity, all with

![](images/d0fcb56ec9b0be9373c511a8ad1bb1569e72125c09230ae1eafec62984a9fc60.jpg)  
Figure 1: From pairwise preference to our proposed InSPO. Standard DPO (left) learns a suboptimal policy  $\bar{\pi}$  from response comparisons where both the preferred and dispreferred responses are evaluated based solely on the prompt. INsPO (right) unleashes intrinsic self-reflection through symmetric cross-conditioning for learning an optimal policy  $\pi^{*}$ : the policy generates the preferred response while seeing the dispreferred one as context, and vice versa, allowing the model to leverage alternative responses as in-context guidance for improvement. Green terms highlight the self-reflection mechanism in our new objectives.

zero inference overhead. See Figure 1 for an overview of our method.

# 2 Background and Notations

To align an LLM with human preference, most existing pipelines first collect pairwise preference data generated by a pre-trained or SFT policy  $\pi_{\mathrm{ref}}$ . Each sample in the preference data consists of a context  $x \in \mathcal{X}$  generated by some distribution  $\rho$ , and two responses  $y_w \in \mathcal{Y}$  and  $y_\ell \in \mathcal{Y}$ , where  $\mathcal{X}$  and  $\mathcal{Y}$  are the context and response spaces respectively. After collecting such preference data, one can apply RLHF or DPO, two primary approaches, to fine-tune an LLM.

RLHF consists of two steps. In the first step, it uses the Bradley-Terry (BT) probabilistic model to understand the human preference on  $(x,y_w,y_\ell)$  and learn a reward function. Specifically, BT model assumes that

$$
\mathbb {P} \left(y _ {w} \succ y _ {\ell} \mid x\right) = \sigma \left(r \left(x, y _ {w}\right) - r \left(x, y _ {\ell}\right)\right), \tag {1}
$$

where  $y_{w} \succ y_{\ell}$  indicates  $y_{w}$  is preferred to  $y_{\ell}$ ,  $\sigma(z) = 1/(1 + e^{-z})$ , and  $r$  is the unknown reward function to evaluate the quality of each response to the context  $x$ . Then given preference data, one can implement maximum likelihood estimation (MLE) to estimate the reward function. In the second step, a policy optimization is executed to find a better LLM policy that maximizes the learned reward. For example, PPO [Ouyang et al., 2022] solves

$$
\max  _ {\pi} \mathbb {E} _ {x \sim \rho , y \sim \pi} [ r (x, y) ] - \beta D _ {\mathrm {K L}} (\pi \| \pi_ {\text {r e f}}), \tag {2}
$$

where  $\beta > 0$  is a regularization parameter that controls the strength of the KL divergence toward the reference policy  $\pi_{\mathrm{ref}}$ , and the divergence  $D_{\mathrm{KL}}(\pi \| \pi_{\mathrm{ref}})$  is defined as

$$
D _ {\mathrm {K L}} (\pi \parallel \pi_ {\mathrm {r e f}}) = \mathbb {E} _ {x \sim \rho} \Big [ \mathrm {K L} \big (\pi (\cdot \mid x) \parallel \pi_ {\mathrm {r e f}} (\cdot \mid x) \big) \Big ].
$$

As an alternative approach to RLHF, DPO shows that solving (2) is equivalent to modeling

$$
r (x, y _ {w}) - r (x, y _ {\ell}) = \beta \left(\log \frac {\pi_ {\theta} (y _ {w} \mid x)}{\pi_ {\mathrm {r e f}} (y _ {w} \mid x)} - \log \frac {\pi_ {\theta} (y _ {\ell} \mid x)}{\pi_ {\mathrm {r e f}} (y _ {\ell} \mid x)}\right).
$$

Then based on the MLE loss derived under BT model (1), DPO fine-tunes an LLM via solving

$$
\max  _ {\pi} \mathbb {E} _ {(x, y _ {w}, y _ {\ell}) \sim \mathcal {D}} \left[ \log \sigma \left(\beta \log \frac {\pi \left(y _ {w} \mid x\right)}{\pi_ {\operatorname {r e f}} \left(y _ {w} \mid x\right)} - \beta \log \frac {\pi \left(y _ {\ell} \mid x\right)}{\pi_ {\operatorname {r e f}} \left(y _ {\ell} \mid x\right)}\right) \right], \tag {3}
$$

where  $\mathcal{D}$  is the joint distribution of  $(x, y_w, y_\ell)$ . Here without loss of generality, we assume  $y_w$  is always preferred to  $y_\ell$  after rearrangement of the preference data.

As established in Proposition 1 of Azar et al. [2024], both methods unify under a general preference optimization framework with the scalarization function  $\Psi(q) = \log(q / (1 - q))$ . The general objective is formulated as:

$$
\max  _ {\pi} \mathbb {E} _ {x \sim \rho} \mathbb {E} _ {y \sim \pi (\cdot | x), y ^ {\prime} \sim \pi_ {\mathrm {r e f}} (\cdot | x)} \left[ \Psi \left(\mathbb {P} \left(y \succ y ^ {\prime} \mid x\right)\right) \right]. \tag {4}
$$

Here,  $\Psi :[0,1]\to \mathbb{R}$  can be any non-decreasing function. Within this framework, existing methods seek an optimal policy  $\bar{\pi}$  restricted to the class of context-conditioned policies  $\bar{\Pi} = \{\pi :\mathcal{X}\rightarrow \mathcal{Y}\}$  defined as

$$
\bar {\pi} \in \operatorname {a r g m a x} _ {\pi \in \Pi} \mathcal {V} (\pi),
$$

where the value of a policy  $\mathcal{V}(\pi)$  is defined as

$$
\mathcal {V} (\pi) \triangleq \mathbb {E} _ {x \sim \rho} \mathbb {E} _ {y \sim \pi (\cdot | x), y ^ {\prime} \sim \pi_ {\mathrm {r e f}} (\cdot | x)} \left[ \Psi \big (\mathbb {P} (y \succ y ^ {\prime} \mid x) \big) \right].
$$

To conclude this section, we assume that we have a dataset of  $n$  pairwise preferences  $\mathcal{D}_n = \{(x^{(i)},y_w^{(i)},y_\ell^{(i)})\}_{i = 1}^n$ , where in  $i$ -th sample, a prompt/context  $x^{(i)}$  is drawn from distribution  $\rho$ , and two responses  $(y_w^{(i)},y_\ell^{(i)})$  are generated by the reference policy  $\pi_{\mathrm{ref}}$ , labeled such that  $y_w^{(i)}\succ y_\ell^{(i)}$ .

# 3 Limitations of Existing Methods

In this section, we identify two critical limitations of existing frameworks by investigating the properties of the restricted optimal policy  $\bar{\pi}$ : (i) Is  $\bar{\pi}$  invariant to the scalarization function  $\Psi$  and the reference distribution  $\pi_{\mathrm{ref}}$ ? (ii) Is  $\bar{\pi}$  theoretically optimal? We demonstrate below that the answer to both questions is negative.

# 3.1 Is  $\bar{\pi}$  invariant to  $\Psi$  and  $\pi_{\mathrm{ref}}$ ?

Ideally, a robust alignment framework should yield an optimal policy invariant to the choice of the scalarization function  $\Psi$  and the reference policy  $\pi_{\mathrm{ref}}$ . This invariance property is critical for modeling robustness and disentanglement from the reference. Specifically, human preferences are fundamentally ordinal. The optimal policy should reflect the underlying ranking of responses, rather than being an artifact of the specific choices of mathematical transformation, i.e.,  $\Psi$ , used to process the preference probabilities or the reference policy, i.e.,  $\pi_{\mathrm{ref}}$ . In addition, the choice of  $\Psi$ , which is often selected for numerical stability or concavity rather than semantic relevance, should not dictate the final behavior of the model. If the optimal fine-tuned policy changes based on modeling choices, the alignment process may become brittle and inconsistent. Lastly, a principled optimization objective should decouple the learned preferences from the reference policy  $\pi_{\mathrm{ref}}$ . Dependence on the reference policy implies that the "optimal" behavior is relative and transient, rather than converging toward a global optimal policy that maximizes human preference. However, the following proposition establishes that the existing target  $\bar{\pi}$  fails to satisfy this condition.

Proposition 3.1. The form of  $\bar{\pi}$  is not invariant to  $\Psi$  and  $\pi_{\mathrm{ref}}$ .

In the proof of Proposition 3.1, we provide counter-examples demonstrating that  $\bar{\pi}$  shifts when either  $\Psi$  or  $\pi_{\mathrm{ref}}$  is varied respectively. This lack of invariance raises a fundamental question about the quality of the resulting policy  $\bar{\pi}$ . If the "optimal" solution shifts based on different choices  $\Psi$  and  $\pi_{\mathrm{ref}}$ , it suggests that  $\bar{\pi}$  is an artifact of the objective function rather than a faithful reflection of human preferences. Consequently, in the following, we demonstrate that the current target  $\bar{\pi}$  is, in fact, technically suboptimal.

# 3.2 Is  $\bar{\pi}$  Optimal?

In this subsection, we investigate the theoretical optimality of  $\bar{\pi}$  in terms of maximizing  $\mathcal{V}(\pi)$ . While  $\bar{\pi}$  maximizes  $\mathcal{V}(\pi)$  over  $\bar{\Pi}$ , we show that it is generally suboptimal compared to the globally optimal policy  $\pi^{*}$ , defined as:

$$
\pi^ {*} \in \operatorname {a r g m a x} _ {\pi \in \Pi} \mathcal {V} (\pi), \tag {5}
$$

where  $\Pi = \{\pi : \mathcal{X} \times \mathcal{Y} \to \mathcal{Y}\}$  represents a broader policy class that conditions not only on the context  $x$  but also on an auxiliary response  $y'$ . The following theorem establishes that  $\pi^*$  is not only superior to  $\bar{\pi}$  but also possesses the desirable invariance property.

Theorem 3.2. (i)  $\pi^{*}$  is invariant to  $\Psi$  and  $\pi_{\mathrm{ref}}$ ; (ii) The following inequality holds, which implies  $\pi^{*}$  is superior to  $\bar{\pi}$ .

$$
\begin{array}{l} \mathcal {V} (\pi^ {*}) = \mathbb {E} _ {x \sim \rho} \mathbb {E} _ {y \sim \pi^ {*} (\cdot | x, y ^ {\prime}), y ^ {\prime} \sim \pi_ {\mathrm {r e f}} (\cdot | x)} [ \Psi (\mathbb {P} (y \succ y ^ {\prime} \mid x)) ] \\ \geq \mathcal {V} (\bar {\pi}) = \mathbb {E} _ {x \sim \rho} \mathbb {E} _ {y \sim \bar {\pi} (\cdot | x), y ^ {\prime} \sim \pi_ {\mathrm {r e f}} (\cdot | x)} \left[ \Psi \big (\mathbb {P} (y \succ y ^ {\prime} \mid x) \big) \right]. \\ \end{array}
$$

(ii) Furthermore, given a fixed  $\Psi$ ,  $\pi^*$  coincides with  $\bar{\pi}$  if the transformed preference probability satisfies the condition that  $\Psi\left(\mathbb{P}(y\succ y'\mid x)\right)\propto c(x,y) - c(x,y')$  for some function  $c$ .

Theorem 3.2 implies that the superiority of  $\pi^{*}$  over  $\bar{\pi}$  stems from its dependence on the comparator response  $y'$ , which effectively triggers a novel notion of intrinsic self-reflection within the LLM. This capacity for self-reflection, which existing fine-tuning methods lack, is a critical property for enhancing alignment. Furthermore, this mechanism renders  $\pi^{*}$  invariant to both  $\Psi$  and  $\pi_{\mathrm{ref}}$ , ensuring that the policy targets the ground-truth human preference probability rather than optimization artifacts as  $\bar{\pi}$ . While Theorem 3.2 (iii) suggests that self-reflection yields no improvement if the preference function is separable, the separable condition is restrictive as it requires a correctly specified (link) function  $\Psi$ . In other words, in the current framework of RLHF and DPO,  $\bar{\pi}$  is optimal only if the BT model (1) is correctly specified. More importantly, existing literature [Tversky and Simonson, 1993, Bordalo et al., 2013] demonstrates that human preferences are inherently non-separable and determined by the interaction between options. It is the context  $x$ , and the self-reflection by comparing  $y$  with  $y'$  that fundamentally shape the choice. Consequently, to leverage this comparative property, we aim to learn  $\pi^{*}$  from the preference data  $\mathcal{D}_n$ , and since our preference data are paired, we do not consider a larger categories of  $\Pi$  beyond pairwise comparison.

# 4 Intrinsic Self-reflective Preference Optimization

In this section, we introduce our InSPO method for learning  $\pi^{*}$ . Thanks to the invariant property of  $\pi^{*}$  as shown in Theorem 3.2 (i), we consider  $\Psi(q) = \log(q/(1 - q))$ . First of all, we impose the following choice model.

$$
\mathbb {P} \left(y _ {w} \succ y _ {\ell} \mid x\right) = \sigma \left(2 \left(r \left(x, y _ {w}, y _ {\ell}\right) - \beta \log Z \left(x, y _ {\ell}\right)\right)\right), \tag {6}
$$

for some generic reward function  $r:\mathcal{X}\times \mathcal{Y}\times \mathcal{Y}\to \mathbb{R}$  and  $Z(x,y^{\prime})\coloneqq \sum_{y}\pi_{\mathrm{ref}}(y\mid x)\exp \Bigl (\frac{1}{\beta} r(x,y,y^{\prime})\Bigr).$  Then by (5),

$$
\begin{array}{l} \pi^ {*} \in \mathrm {a r g m a x} _ {\pi} 2 \mathbb {E} _ {x \sim \rho , y \sim \pi (\cdot | x, y ^ {\prime}), y ^ {\prime} \sim \pi_ {\mathrm {r e f}} (\cdot | x)} \left[ r (x, y, y ^ {\prime}) - \beta \log Z (x, y ^ {\prime}) \right] \\ = \operatorname {a r g m a x} _ {\pi} \mathbb {E} _ {x \sim \rho , y \sim \pi (\cdot | x, y ^ {\prime}), y ^ {\prime} \sim \pi_ {\mathrm {r e f}} (\cdot | x)} \left[ r (x, y, y ^ {\prime}) \right], \\ \end{array}
$$

which is independent of  $Z$ . Therefore model assumption in (6) is mild as  $r$  is unspecified and can be generic. Then following the paradigm of RLHF, we can estimate  $\pi^*$  via

$$
\max _ {\pi} \mathbb {E} _ {x \sim \rho , y \sim \pi (\cdot | x, y ^ {\prime}), y ^ {\prime} \sim \pi_ {\mathrm {r e f}} (\cdot | x)} \left[ r (x, y, y ^ {\prime}) \right] - \beta D _ {\mathrm {K L}} (\pi \| \pi_ {\mathrm {r e f}}), \tag {7}
$$

where

$$
D _ {\mathrm {K L}} (\pi \parallel \pi_ {\mathrm {r e f}}) = \mathbb {E} _ {x \sim \rho , y ^ {\prime} \sim \pi_ {\mathrm {r e f}} (\cdot | x)} \Big [ \mathrm {K L} \big (\pi (\cdot \mid y ^ {\prime}, x) \parallel \pi_ {\mathrm {r e f}} (\cdot \mid x) \big) \Big ].
$$

While this is a promising approach, the reward function  $r$  may be hard to estimate and PPO is known for instability. In the following, we propose a family of DPO-based approaches for learning  $\pi^*$ .

To begin with, we have the following proposition that establishes the connection between the general reward function  $r$  and  $\pi_r$ , which is denoted as an optimal solution to (7).

Theorem 4.1. Solving the optimization problem (7) gives

$$
r (x, y, y ^ {\prime}) = \beta \left[ \log \frac {\pi_ {r} (y | x , y ^ {\prime})}{\pi_ {\mathrm {r e f}} (y | x)} + \log Z (x, y ^ {\prime}) \right]. \tag {8}
$$

Furthermore,  $\pi_r$  can be obtained by solving

$$
\max  _ {\pi} \mathbb {E} _ {(x, y _ {w}, y _ {\ell}) \sim \mathcal {D}} \left[ \log \sigma \left(\beta \left(\log \frac {\pi (y _ {w} | x , y _ {\ell})}{\pi_ {\mathrm {r e f}} (y _ {w} | x)} - \log \frac {\pi (y _ {\ell} | x , y _ {w})}{\pi_ {\mathrm {r e f}} (y _ {\ell} | x)}\right)\right) \right]. \tag {9}
$$

Then based on Theorem 3.2 and the preference dataset  $\mathcal{D}_n$ , InSPO estimates  $\pi^*$  via

$$
\min  _ {\theta} - \frac {1}{n} \sum_ {i = 1} ^ {n} \left[ \log \sigma \left(\beta \left(\log \frac {\pi_ {\theta} \left(y _ {w} ^ {(i)} \mid x , y _ {\ell} ^ {(i)}\right)}{\pi_ {\mathrm {r e f}} \left(y _ {w} ^ {(i)} \mid x\right)} - \log \frac {\pi_ {\theta} \left(y _ {\ell} ^ {(i)} \mid x , y _ {w} ^ {(i)}\right)}{\pi_ {\mathrm {r e f}} \left(y _ {\ell} ^ {(i)} \mid x\right)}\right)\right) \right], \tag {10}
$$

where we parametrize the trainable policy  $\pi$  by  $\theta$ . Similar to DPO, the proposed InSPO based on (10) aligns an LLM by directly shifting probability mass toward human preferred responses and away from dispreferred responses without performing RL, without rollouts, and without training a reward model. On top of it, our method leverages the self-reflection property embedded in the preference data to further calibrate the policy towards the preferred response by contrasting with the dispreferred one. Therefore, this approach enables the learning of  $\pi^{*}$ . Denote the solution to (10) as  $\widehat{\pi}$ .

During the inference, instead of first generating  $y'$  from  $\pi_{\mathrm{ref}}$  for self-reflection, which incurs an overhead cost, we directly deploy  $\widehat{\pi}$  given a testing query  $x$ . This is plausible because  $\widehat{\pi}(\cdot \mid x, y')$  and the deployed one  $\widehat{\pi}(\cdot \mid x)$  generate the responses based on the embeddings of  $(x, y')$  and  $x$  respectively, which has the same dimension thanks to the transformer architecture. From this perspective, one can understand that during the training process, our InSPO activates the self-reflection via conditioning on  $(x, y')$ , which indeed leverages more diverse embeddings for learning a better fine-tuned policy than that under  $x$  only. Since self-reflective mechanism is distilled into the policy during training, InSPO may not require generating alternative responses during the inference time. The training procedure is outlined in Algorithm 1.

It is worth noting that our proposed formulation for  $\pi^{*}$  is method-agnostic and can seamlessly integrate with standard preference optimization techniques. Table 1 illustrates the InSPO variants of six representative preference optimization methods, including DPO, demonstrating how each is adapted to unlock the self-reflective capability. In the next section, we study the empirical performance of each method.

Algorithm 1: Self-reflection based DPO  
1: Input:  $\mathcal{D}_n = \{(x,y_w,y_\ell)\}_{i=1}^n$ ,  $\pi_{\mathrm{ref}}, \beta$  Output:  $\widehat{\pi}$   
2: Initialize:  $\pi_\theta \gets \pi_{\mathrm{ref}}$   
3: for  $(x,y_w,y_\ell) \sim \mathcal{D}_n$  do  
4:  $r_w, r_\ell \gets \log \pi_{\mathrm{ref}}(y_w|x), \log \pi_{\mathrm{ref}}(y_\ell|x)$   
5:  $p_w, p_\ell \gets \log \pi_\theta(y_w|x, y_\ell), \log \pi_\theta(y_\ell|x, y_w)$   
6:  $m \gets \beta(p_w - r_w - p_\ell + r_\ell)$   
7:  $\theta \gets \theta - \eta \nabla_\theta[-\log \sigma(m)]$   
8: end for  
9: return  $\widehat{\pi}$

Table 1: InSPO variants of six representative preference optimization methods. The Original formulations of DPO-based approaches are in black, while INSPO-enhanced terms are in green.  

<table><tr><td>Method</td><td>Objective Function for Minimization</td></tr><tr><td rowspan="2">DPO [Rafailov et al., 2023]</td><td>- log σ(β log πθ(yw|x)/πref(yw|x) - β log πθ(yl|x)/πref(yl|x))</td></tr><tr><td>- log σ(β log πθ(yw|yℓ,x)/πref(yw|x) - β log πθ(yl|yw,x)/πref(yl|x))</td></tr><tr><td rowspan="2">IPO [Azar et al., 2024]</td><td>( log πθ(yw|x)/πref(yw|x) - log πθ(yl|x)/πref(yl|x) - 1/2τ)2</td></tr><tr><td>( log πθ(yw|yℓ,x)/πref(yw|x) - log πθ(yl|yw,x)/πref(yl|x) - 1/2τ)2</td></tr><tr><td rowspan="2">RDPO [Park et al., 2024]</td><td>- log σ(β[log πθ(yw|x)/πref(yw|x) - log πθ(yl|x)/πref(yl|x)] + α(|yw| - |yl|)</td></tr><tr><td>- log σ(β[log πθ(yw|yℓ,x)/πref(yw|x) - log πθ(yl|yw,x)/πref(yl|x)] + α(|yw| - |yl|)</td></tr><tr><td rowspan="2">ORPO [Hong et al., 2024]</td><td>- log pθ(yw|x) - λ log σ(logit(pθ(yw|x)) - logit(pθ(yl|x)))</td></tr><tr><td>- log pθ(yw|yℓ,x) - λ log σ(logit(pθ(yw|yℓ,x)) - logit(pθ(yl|yw,x)))</td></tr><tr><td rowspan="2">SimPO [Meng et al., 2024]</td><td>- log σ(β1/yw log πθ(yw|x) - β1/yell log πθ(yl|x) - γ)</td></tr><tr><td>- log σ(β1/yw log πθ(yw|yℓ,x) - β1/yell log πθ(yl|yw,x) - γ)</td></tr></table>

$p_{\theta}(y|\cdot) = \exp (\frac{1}{|y|}\log \pi_{\theta}(y|\cdot));\beta ,\tau ,\alpha ,\lambda ,\gamma$  : hyperparameters.

# 5 Experiments

# 5.1 Experimental Setup

Models and data. We employ Mistral-7B-Instruct-v0.2 and Llama-3-8B-Instruct as base models, initializing all methods from identical checkpoints within each family. Training data is sourced from UltraFeedback [Cui et al., 2023], containing approximately 60K preference pairs  $(x; y_w, y_\ell)$  with  $y_w \succ y_\ell$  after dedduplication and

![](images/f62ba46ef665019a956c2a0461026cf5302aa6945112cd37d4b80afa09d49d56.jpg)  
(a) Preference Accuracy

![](images/15b4fccec3632116e34710c334c43f31a72859a3940998a53bbd27e21a59f2ed.jpg)  
(b) Reward  
Figure 2: Training dynamics of INSPO methods (solid) versus baselines (dashed). Our INSPO exhibits stable optimization with smooth loss convergence, consistent accuracy improvement, and enhanced reward margins without requiring additional on-policy rollouts.

![](images/6552160f1b43c9704fa871d56e036b787ba973b0a53025cb3fd95c1874576d5d.jpg)  
(c) Training Loss

Table 2: Benchmark performance on AlpacaEval2, Arena-Hard, and MT-Bench for Mistral-Instruct (7B) and Llama-3-Instruct (8B). Shaded rows denote continuation-conditioned variants with deltas computed relative to their corresponding baselines. Bold values indicate the highest performance in each metric. All methods employ DPO-family objectives with identical training configurations.  

<table><tr><td rowspan="3">Method</td><td colspan="5">Mistral-Instruct (7B)</td><td colspan="5">Llama-3-Instruct (8B)</td></tr><tr><td colspan="2">AlpacaEval2</td><td colspan="2">Arena-Hard</td><td>MT-Bench</td><td colspan="2">AlpacaEval2</td><td colspan="2">Arena-Hard</td><td>MT-Bench</td></tr><tr><td>LC (%)</td><td>WR (%)</td><td>WR (%)</td><td>GPT-4o</td><td>GPT-4</td><td>LC (%)</td><td>WR (%)</td><td>WR (%)</td><td>GPT-4o</td><td>GPT-4</td></tr><tr><td>Baseline</td><td>17.1</td><td>14.7</td><td>12.6</td><td>6.2</td><td>7.5</td><td>26.0</td><td>25.3</td><td>22.3</td><td>6.9</td><td>8.1</td></tr><tr><td colspan="11">Reward-Free Preference Optimization</td></tr><tr><td>RRHF</td><td>25.3</td><td>24.8</td><td>18.1</td><td>6.5</td><td>7.6</td><td>31.3</td><td>28.4</td><td>26.5</td><td>6.7</td><td>7.9</td></tr><tr><td>SLiC-HF</td><td>24.1</td><td>24.6</td><td>18.9</td><td>6.5</td><td>7.8</td><td>26.9</td><td>27.5</td><td>26.2</td><td>6.8</td><td>8.1</td></tr><tr><td>CPO</td><td>23.8</td><td>28.8</td><td>22.6</td><td>6.3</td><td>7.5</td><td>28.9</td><td>32.2</td><td>28.8</td><td>7.0</td><td>8.0</td></tr><tr><td>KTO</td><td>24.5</td><td>23.6</td><td>17.9</td><td>6.4</td><td>7.7</td><td>33.1</td><td>31.8</td><td>26.4</td><td>6.9</td><td>8.2</td></tr><tr><td colspan="11">Preference Optimization with Self-Reflection Conditioning</td></tr><tr><td>IPO</td><td>20.3</td><td>20.3</td><td>16.2</td><td>6.4</td><td>7.8</td><td>35.6</td><td>35.6</td><td>30.5</td><td>7.0</td><td>8.3</td></tr><tr><td>+ INS</td><td>28.8 ↑8.5</td><td>27.9 ↑7.6</td><td>18.5 ↑2.3</td><td>6.5 ↑0.1</td><td>7.9 ↑0.1</td><td>37.7 ↑2.1</td><td>41.3 ↑5.7</td><td>32.8 ↑2.3</td><td>7.1 ↑0.1</td><td>8.5 ↑0.2</td></tr><tr><td>ORPO</td><td>24.5</td><td>24.9</td><td>20.8</td><td>6.4</td><td>7.7</td><td>28.5</td><td>27.4</td><td>25.8</td><td>6.8</td><td>8.0</td></tr><tr><td>+ INS</td><td>24.6 ↑0.1</td><td>24.7 ↓0.2</td><td>21.5 ↑0.7</td><td>6.3 ↓0.1</td><td>7.8 ↑0.1</td><td>35.7 ↑7.2</td><td>40.3 ↑12.9</td><td>25.5 ↓0.3</td><td>6.8 -0.0</td><td>8.0 -0.0</td></tr><tr><td>R-DPO</td><td>27.3</td><td>24.5</td><td>16.1</td><td>6.2</td><td>7.5</td><td>41.1</td><td>37.8</td><td>33.1</td><td>7.0</td><td>8.0</td></tr><tr><td>+ INS</td><td>29.1 ↑1.8</td><td>27.6 ↑3.1</td><td>19.0 ↑2.9</td><td>6.4 ↑0.2</td><td>7.6 ↑0.1</td><td>41.0 ↓0.1</td><td>43.8 ↑6.0</td><td>35.0 ↑1.9</td><td>7.1 ↑0.1</td><td>8.1 ↑0.1</td></tr><tr><td>DPO</td><td>26.8</td><td>24.9</td><td>16.3</td><td>6.3</td><td>7.6</td><td>40.3</td><td>37.9</td><td>32.6</td><td>7.0</td><td>8.0</td></tr><tr><td>+ INS</td><td>29.9 ↑3.1</td><td>29.4 ↑4.5</td><td>23.9 ↑7.6</td><td>6.5 ↑0.2</td><td>7.8 ↑0.2</td><td>40.3 -0.0</td><td>41.1 ↑3.2</td><td>35.3 ↑2.7</td><td>7.1 ↑0.1</td><td>8.1 ↑0.1</td></tr><tr><td>SimPO</td><td>32.1</td><td>34.6</td><td>21.0</td><td>6.6</td><td>7.6</td><td>44.5</td><td>40.5</td><td>33.8</td><td>7.0</td><td>8.0</td></tr><tr><td>+ INS</td><td>32.7 ↑0.6</td><td>34.5 ↓0.1</td><td>24.4 ↑3.4</td><td>6.6 -0.0</td><td>7.7 ↑0.1</td><td>44.5 -0.0</td><td>46.6 ↑6.1</td><td>35.9 ↑2.1</td><td>7.1 ↑0.1</td><td>8.2 ↑0.2</td></tr></table>

LC: length-controlled win rate; WR: win rate; AE: AlpacaEval2; AH: Arena-Hard. All methods initialize from the same instructed checkpoint within each model family. The proposed INS enables self-reflection via symmetric cross-conditioning on preference pairs.

safety filtering. The data generating process follows exactly from SimPO [Meng et al., 2024].

Benchmarks. We evaluate on three widely-adopted benchmarks. AlpacaEval 2 [Dubois et al., 2024] contains 805 diverse instructions; we report both standard win rates (WR) and length-controlled win rates (LC) to mitigate morbidity bias. Arena-Hard [Li et al., 2024] features 500 challenging queries that test advanced

reasoning; we report WR alongside its style-controlled variant (SC) to account for stylistic preferences. MT-Bench [Zheng et al., 2023] comprises 80 multi-turn questions spanning eight capability categories, scored on a 10-point scale.

Baselines. We compare INSPO-enhanced variants named INS-DPO, INS-SimPO, INS-IPO, INS-RDPO, INS-ORPO against their standard counterparts and additional baselines: RRHF [Yuan et al., 2023], SLiC-HF [Zhao et al., 2023], CPO [Xu et al., 2024a], and KTO [Ethayarajh et al., 2024]. For fair comparison, baseline results are obtained by evaluating publicly released checkpoints from SimPO [Meng et al., 2024] on identical benchmark versions. All INSPO-enhanced methods are implemented using the OpenRLHF framework [Hu et al., 2024] and trained for three epochs with AdamW optimizer, learning rate  $5 \times 10^{-7}$ , cosine schedule with  $10\%$  warmup, and maximum context length 4096. Method-specific hyperparameters follow exact configurations from prior works.

Inference. All models use a standard autoregressive generation  $\hat{y} \sim \widehat{\pi}(\cdot \mid x)$  with nucleus sampling at  $p = 0.95$  and temperature 0.7, introducing no additional computational overhead versus baseline methods.

# 5.2 Main Results

Table 2 reports comprehensive benchmark results. Controlled metrics show substantial improvements of InSPO. On AlpacaEval(AE) LC, INSPO yields 0.6-8.5 point gains across the DPO family, with the largest improvements observed on INS-IPO: +8.5 points for Mistral-Instruct and +2.1 points for Llama-3-Instruct. Arena-Hard(AH) results follow a similar pattern, with improvements ranging from 0.7 to 7.6 points. Notably, INS-DPO achieves a 7.6-point gain on Mistral-Instruct and 2.7 points on Llama-3-Instruct, demonstrating consistent benefits of enabling intrinsic self-reflection through alternative conditioning. The training dynamic curves in Figure 2 further enhance the persuasiveness of the results.

The gains extend beyond controlled metrics: INS-DPO and INS-SimPO improve raw win rates by 3-6 points on AlpacaEval 2 and 2-3 points on Arena-Hard, indicating genuine quality improvements rather than mere morbidity reduction. MT-Bench results corroborate this finding, with INS-enhanced methods achieving 0.1-0.2 point gains across different judge models, demonstrating improvements where alternative response context provides valuable learning signals.

The relative ranking among DPO family members remains stable under INSPO enhancement—SimPO variants consistently lead, followed by DPO and R-DPO, with IPO and ORPO showing more variable performance—suggesting that INSPO amplifies rather than disrupts the inherent strengths of each base method. Compared to alternative baselines beyond the DPO family such as RRHF, SLiC-HF, CPO, and KTO, our best INS-enhanced variant INS-SimPO establishes competitive results on controlled metrics while maintaining comparable raw performance.

# 5.3 Analysis and Insights

We conduct systematic ablation studies to understand the mechanisms underlying sequence-level conditioning and explore its interactions with key design choices. Unless otherwise specified, all experiments use INS-DPO with the Llama-3-8B-Instruct model.

Impact of candidate length and context window. We systematically investigate this by varying both the maximum context window and the candidate length cap  $\alpha$ . Specifically, MaxLen constrains the total sequence length of the entire training tuple  $(x,y_{\ell},y_w)$ , while the Draft Cap  $\alpha$  enforces  $|y_{\ell}| \leq \alpha \cdot \text{MaxLen}$  for the dispreferred candidate alone. Crucially, larger MaxLen values not only permit longer  $y_{\ell}$ , but also provide more headroom for the instruction  $x$  and preferred response  $y_w$  to remain untruncated.

Table 3 presents results across three context window sizes (1024, 2048, 4096) and four candidate length constraints (30%, 40%, 50%, uncapped). Three key findings emerge. First, longer context windows consistently improve performance, with 4096-token contexts yielding the best results across all metrics

Table 3: Ablation study on candidate length constraints and context windows using Llama-3-8B. MaxLen limits the total sequence length of  $(x, y_{\ell}, y_{w})$ ; Draft Cap  $\alpha$  enforces  $|y_{\ell}| \leq \alpha \cdot \text{MaxLen}$  for the dispreferred candidate only.  

<table><tr><td>MaxLen</td><td>Draft Cap</td><td>AE-LC</td><td>AE-WR</td><td>AH-SC</td><td>AH-WR</td></tr><tr><td rowspan="4">1024</td><td>30%</td><td>39.1</td><td>37.4</td><td>30.8</td><td>32.1</td></tr><tr><td>40%</td><td>39.8</td><td>38.2</td><td>31.2</td><td>32.8</td></tr><tr><td>50%</td><td>39.9</td><td>37.9</td><td>30.5</td><td>33.1</td></tr><tr><td>None</td><td>38.2</td><td>36.1</td><td>29.8</td><td>31.2</td></tr><tr><td rowspan="4">2048</td><td>30%</td><td>39.9</td><td>38.6</td><td>32.3</td><td>34.1</td></tr><tr><td>40%</td><td>40.3</td><td>39.2</td><td>32.9</td><td>34.7</td></tr><tr><td>50%</td><td>40.1</td><td>38.9</td><td>33.1</td><td>34.3</td></tr><tr><td>None</td><td>39.1</td><td>37.8</td><td>31.5</td><td>33.2</td></tr><tr><td rowspan="4">4096</td><td>30%</td><td>40.1</td><td>39.5</td><td>33.2</td><td>35.4</td></tr><tr><td>40%</td><td>40.5</td><td>39.9</td><td>33.8</td><td>35.9</td></tr><tr><td>50%</td><td>40.2</td><td>39.7</td><td>33.4</td><td>35.6</td></tr><tr><td>None</td><td>39.8</td><td>38.9</td><td>32.8</td><td>34.9</td></tr></table>

(40.3% LC, 39.9% WR on AlpacaEval 2). This suggests that providing sufficient space for the complete  $(x,y_{\ell},y_{w})$  sequence without truncation is crucial for effective sequence-level learning. Second, moderate draft length caps (40% of MaxLen) achieve optimal performance, balancing context richness with training stability. Uncapped drafts slightly underperform, likely due to occasional extremely long dispreferred responses that dominate the context and destabilize gradient flow. Third, the benefits of longer contexts are most pronounced on challenging benchmarks: on Arena-Hard, the 4096-token setting outperforms 1024 by 2.6 WR points, indicating that complex reasoning tasks particularly benefit from richer candidate context.

Conditioning strategies and length control. Our main results in Table 2 show that sequence-level conditioning consistently improves raw win rates, and yields measurable improvements under length-controlled evaluation. This observation, combined with the finding that longer contexts enable better performance, motivates us to explore whether different conditioning strategies can better balance quality improvements with length control. We compare four variants:

(i) No conditioning (standard DPO baseline).  
(ii) One-sided conditioning:  $\mathcal{L}_{\mathrm{one - side}} = -\log \sigma \big(\beta [\log \pi_{\theta}(y_w|x,y_\ell) / \pi_{\mathrm{ref}}(y_w|x) - \log \pi_{\theta}(y_\ell |x,y_\ell) / \pi_{\mathrm{ref}}(y_\ell |x)]\big)$ .  
(iii) Cross-conditioning (INSPO):  $\mathcal{L}_{\mathrm{sym}} = -\log \sigma \big(\beta [\log \pi_{\theta}(y_w|x,y_\ell) / \pi_{\mathrm{ref}}(y_w|x) - \log \pi_{\theta}(y_\ell |x,y_w) / \pi_{\mathrm{ref}}(y_\ell |x)]\big)$ .  
(iv) Empirical averaged:  $\mathcal{L}_{\mathrm{avg}} = \frac{1}{2}\mathcal{L}_{\mathrm{DPO}} + \frac{1}{2}\mathcal{L}_{\mathrm{sym}}$  to mitigate length bias.

Table 4 presents comprehensive results across two model families. Symmetric cross-conditioning achieves strong performance, substantially outperforming the baseline on Arena-Hard by +7.6 WR points for Mistral-7B and +2.7 points for Llama-3-8B. Notably, even one-sided conditioning provides meaningful improvements (+3.1 LC points on AlpacaEval 2 for Llama-3-8B), confirming that sequence-level information provides value regardless of symmetry.

The simple averaging strategy offers a practical mechanism for length control without requiring hyperparameter tuning. By equally weighting pairwise and sequence-level objectives, the averaged approach achieves favorable trade-offs: the best Arena-Hard performance on Mistral-7B (+9.4 WR points) while maintaining competitive LC metrics, and +3.8 LC points with +3.6 WR points on AlpacaEval 2 for Llama-3-8B. This demonstrates that practitioners can balance competing objectives through straightforward combination. Overall, full symmetric conditioning remains the preferred choice when optimizing for raw quality, while

Table 4: Comparison of conditioning strategies and simple averaging for length control. All experiments use MaxLen=4096 with  $40\%$  draft cap. Deltas are computed relative to the no-conditioning baseline.  

<table><tr><td rowspan="2">Model</td><td rowspan="2">Strategy</td><td colspan="2">AlpacaEval 2</td><td>Arena-Hard</td></tr><tr><td>LC (%)</td><td>WR (%)</td><td>WR (%)</td></tr><tr><td rowspan="4">Mistral-7B</td><td>DPO (baseline)</td><td>26.8</td><td>24.9</td><td>16.3</td></tr><tr><td>One-sided</td><td>28.7 ↑1.9</td><td>27.1 ↑2.2</td><td>19.2 ↑2.9</td></tr><tr><td>Symmetric</td><td>29.9 ↑3.1</td><td>29.4 ↑4.5</td><td>23.9 ↑7.6</td></tr><tr><td>Averaged</td><td>29.6 ↑2.8</td><td>29.7 ↑4.8</td><td>25.7 ↑9.4</td></tr><tr><td rowspan="4">Llama-3-8B</td><td>DPO (baseline)</td><td>40.3</td><td>37.9</td><td>32.6</td></tr><tr><td>One-sided</td><td>43.4 ↑3.1</td><td>40.2 ↑2.3</td><td>34.1 ↑1.5</td></tr><tr><td>Symmetric</td><td>40.3 ↑0.0</td><td>41.1 ↑3.2</td><td>35.3 ↑2.7</td></tr><tr><td>Averaged</td><td>44.1 ↑3.8</td><td>41.5 ↑3.6</td><td>35.5 ↑2.9</td></tr></table>

averaging provides a simple yet effective alternative when length considerations are important.

Computational overhead. We quantify the computational cost introduced by sequence-level conditioning across Mistral-7B, Llama-3-8B, and Gemma2-9B. Table 5 summarizes the key characteristics. Sequence-level conditioning incurs  $18 - 25\%$  training time overhead due to processing longer inputs  $(x \oplus y_{\ell})$  and  $(x \oplus y_w)$ , with detailed breakdowns provided in Appendix D.5. Critically, inference remains identical to standard DPO with zero latency overhead, as models generate via standard sampling  $\hat{y} \sim \pi_{\theta}(\cdot \mid x)$  without requiring explicit draft-revise loops.

Table 5: Computational comparison between DPO and INS-DPO  

<table><tr><td></td><td>DPO</td><td>INS-DPO</td></tr><tr><td>Training overhead</td><td>+0%</td><td>+21%</td></tr><tr><td>Max sequence length</td><td>2048</td><td>4096</td></tr><tr><td>Inference overhead</td><td>+0%</td><td>+0%</td></tr><tr><td>Draft-revise at inference</td><td>No</td><td>No</td></tr></table>

# 6 Related Work

# 6.1 Reinforcement Learning from Human Feedback

RLHF is a fundamental technique for aligning large language models with human preferences and values [Bai et al., 2022]. The classical RLHF pipeline typically comprises three phases: supervised fine-tuning, reward model training, and policy optimization [Dong et al., 2024, Chen et al., 2024, Tang et al., 2023, Yu et al., 2025, Chang and Li, 2024]. Proximal Policy Optimization [Schulman et al., 2017] is widely used in the policy optimization stage, where the policy model is trained to maximize the learned reward while staying close to the reference model through KL regularization. The RLHF framework has been successfully applied to various applications, including mitigating toxicity, ensuring safety, enhancing helpfulness, and improving model reasoning abilities [Chen et al., 2025b,a, Li et al., 2025b,a]. However, recent work has highlighted challenges across the PPO-based RLHF pipeline, from preference pattern to model training [Ahmadian et al., 2024, Xu et al., 2024b]. Additionally, RLHF can lead to biased outcomes, such as verbose outputs and length exploitation. Given the complexity and optimization challenges of online preference optimization

algorithms, researchers have been exploring more efficient and simpler alternative offline algorithms Rafailov et al. [2023], Zeng et al. [2024], Zhang et al. [2025].

# 6.2 Direct Preference Optimization and Variants

Direct Preference Optimization is a notable offline preference optimization approach that eliminates the need for explicit reward modeling by reparameterizing the reward function, achieving remarkable simplicity and training stability compared to PPO-based methods [Rafailov et al., 2023, Ivison et al., 2024]. DPO directly learns a policy model from preference data by expressing preferences through policy and reference model ratios, avoiding the online optimization of classical RLHF. Building on DPO, numerous variants have been proposed to address specific limitations or enhance performance. Identity Preference Optimization (IPO) introduces a theoretically grounded framework that avoids DPO's assumption of replacing pairwise preferences with pointwise rewards, using a squared loss objective instead [Azar et al., 2024]. Contrastive Preference Optimization (CPO) incorporates sequence likelihood as reward and trains alongside a supervised fine-tuning objective for improved calibration [Xu et al., 2024a]. Kahneman-Tversky Optimization (KTO) extends preference optimization to non-paired data by modeling preferences through prospect-theoretic optimization [Ethayarajh et al., 2024]. Regularized DPO (R-DPO) adds explicit length regularization to prevent length exploitation [Park et al., 2024]. Simple Preference Optimization (SimPO) proposes a reference-free reward formulation using length-normalized average log probability with a target reward margin, eliminating the need for a reference model during training [Meng et al., 2024].

# 7 Conclusion

In this work, we have revealed and addressed two critical limitations in current preference optimization frameworks: the lack of theoretical invariance to modeling choices and the suboptimality arising from treating response generation in isolation. We showed that standard methods like RLHF and DPO produce aligned policies that are often artifacts of parameterization rather than true reflections of human intent, while failing to leverage the comparative nature of preference data for optimal performance. To overcome these challenges, we introduced novel InSPO, a generic framework that unlocks the model's latent capacity for self-reflection. By deriving a globally optimal policy that conditions on both the context and the alternative response, InSPO achieves a target that is superior to existing baselines and guaranteed to be invariant to the choice of scalarization function or reference policy. Practically, our method serves as a plug-and-play enhancement that induces dense reward shaping during training, guiding the model to avoid specific failure modes found in rejected responses. Crucially, InSPO distills this comparative reasoning into the policy itself, delivering these gains with zero extra inference overhead. Our comprehensive experiments on a range of DPO-family baselines confirm that InSPO consistently improves alignment performance across diverse benchmarks and scales effectively with model size. By decoupling the alignment goal from modeling constraints and unleashing intrinsic self-reflection, InSPO offers a more robust, theoretically grounded, and computationally efficient path toward human-aligned AI. Future work may explore extending this self-reflective paradigm to online iterative training or conversational settings.

# References

Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024.

Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. A general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pages 4447-4455. PMLR, 2024.  
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.  
Pedro Bordalo, Nicola Gennaioli, and Andrei Shleifer. Salience and consumer choice. Journal of Political Economy, 121(5):803-843, 2013.  
Da Chang and Yu Li. Mixed text recognition with efficient parameter fine-tuning and transformer. In International Conference on Neural Information Processing, pages 17-31. Springer, 2024.  
Jiayu Chen, Bhargav Ganguly, Yang Xu, Yongsheng Mei, Tian Lan, and Vaneet Aggarwal. Deep generative models for offline policy learning: Tutorial, survey, and perspectives on future directions. arXiv preprint arXiv:2402.13777, 2024.  
Rongqian Chen, Allison Andreyev, Yanming Xiu, Mahdi Imani, Bin Li, Maria Gorlatova, Gang Tan, and Tian Lan. A neurosymbolic framework for interpretable cognitive attack detection in augmented reality. arXiv preprint arXiv:2508.09185, 2025a.  
Rongqian Chen, Jun Kwon, Kefan Wu, and Wei-Hsi Chen. Tunable leg stiffness in a monopedal hopper for energy-efficient vertical hopping across varying ground profiles. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pages 12929-12935. IEEE, 2025b.  
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback. 2023.  
Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. How abilities in large language models are affected by supervised fine-tuning data composition. arXiv preprint arXiv:2310.05492, 2023.  
Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024.  
Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.  
Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. In International Conference on Machine Learning, 2024.  
Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without reference model. arXiv preprint arXiv:2403.07691, 2024.  
Jian Hu, Xibin Wu, Wei Shen, Jason Klein Liu, Zilin Zhu, Weixun Wang, Songlin Jiang, Haoran Wang, Hao Chen, Bin Chen, et al. Openrhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024.  
Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah A Smith, Yejin Choi, and Hanna Hajishirzi. Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback. Advances in neural information processing systems, 37:36602-36633, 2024.

Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Phillip HS Torr, Fahad Shahbaz Khan, and Salman Khan. Llm post-training: A deep dive into reasoning large language models. arXiv preprint arXiv:2502.21321, 2025.  
Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024.  
Yu Li, Da Chang, Die Luo, Jin Huang, Lan Dong, Du Wang, Liye Mei, and Cheng Lei. Sfmdiffusion: self-supervised monocular depth estimation in endoscopy based on diffusion models. International Journal of Computer Assisted Radiology and Surgery, pages 1-9, 2025a.  
Yu Li, Jin Huang, Yimin Zhang, Jingwen Deng, Jingwen Zhang, Lan Dong, Du Wang, Liye Mei, and Cheng Lei. Dual branch segment anything model-transformer fusion network for accurate breast ultrasound image segmentation. Medical Physics, 52(6):4108-4119, 2025b.  
Yongsheng Mei, Hanhan Zhou, Tian Lan, Guru Venkataramani, and Peng Wei. Mac-po: Multi-agent experience replay via collective priority optimization. arXiv preprint arXiv:2302.10418, 2023.  
Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a reference-free reward. Advances in Neural Information Processing Systems, 37:124198-124235, 2024.  
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744, 2022.  
Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. Disentangling length from quality in direct preference optimization. arXiv preprint arXiv:2403.19159, 2024.  
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in neural information processing systems, 36:53728-53741, 2023.  
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.  
Sizhe Tang, Mengmeng Cui, Lianyong Qi, and Xiaolong Xu. Edge intelligence with distributed processing of dnns: A survey. CMES-Computer Modeling in Engineering & Sciences, 136(1), 2023.  
Amos Tversky and Itamar Simonson. Context-dependent preferences. Management Science, 39(10):1179-1189, 1993.  
Haoran Xu, Amr Sharaf andwc Young, and Weiting Wang. Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation. In International Conference on Machine Learning, 2024a.  
Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is dpo superior to ppo for llm alignment? a comprehensive study. arXiv preprint arXiv:2404.10719, 2024b.  
Fei Xu Yu, Gina Adam, Nathaniel D Bastian, and Tian Lan. Optimizing prompt sequences using monte carlo tree search for llm-based optimization. arXiv preprint arXiv:2508.05995, 2025.

Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback without tears. Advances in Neural Information Processing Systems, 36, 2023.  
Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, and Jun Wang. Token-level direct preference optimization. arXiv preprint arXiv:2404.11999, 2024.  
Zuyuan Zhang, Vaneet Aggarwal, and Tian Lan. Network diffuser for placing-scheduling service function chains with inverse demonstration. In IEEE INFOCOM 2025-IEEE Conference on Computer Communications, pages 1-10. IEEE, 2025.  
Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.  
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:46595-46623, 2023.

# Appendix

# A Notation

<table><tr><td>Symbol</td><td>Meaning</td><td>Symbol</td><td>Meaning</td></tr><tr><td colspan="4">Basic Elements</td></tr><tr><td>x</td><td>Prompt / input context</td><td>y</td><td>Response sequence</td></tr><tr><td>yw, yl</td><td>Preferred and dispreferred</td><td>y+, y-</td><td>Winner and loser (Appendix)</td></tr><tr><td>|y|</td><td>Response length (tokens)</td><td>yt</td><td>Token at position t</td></tr><tr><td colspan="4">Models and Policies</td></tr><tr><td>πθ(· | x)</td><td>Trainable policy</td><td>πθ(y | x, yl)</td><td>Sequence-conditioned policy</td></tr><tr><td>πref(· | x)</td><td>Reference policy</td><td>pθ(y | c)</td><td>Length-normalized prob.</td></tr><tr><td colspan="4">Reward and Scoring</td></tr><tr><td>r(x, y, y&#x27;)</td><td>Reward function</td><td>m</td><td>Preference margin</td></tr><tr><td>σ(z)</td><td>Sigmoid 1/(1 + e-z)</td><td></td><td></td></tr><tr><td colspan="4">Hyperparameters</td></tr><tr><td>β &gt; 0</td><td>Inverse temperature</td><td>γ ≥ 0</td><td>Target margin (SimPO)</td></tr><tr><td>α</td><td>Length coeff. (R-DPO)</td><td>λ</td><td>Odds ratio weight (ORPO)</td></tr><tr><td>τ</td><td>Temperature (IPO)</td><td></td><td></td></tr><tr><td colspan="4">Datasets and Loss Functions</td></tr><tr><td>D</td><td>Preference dataset</td><td>X, Y</td><td>Prompt and response spaces</td></tr><tr><td>n</td><td>Dataset size</td><td>(x(i), yw(i), yl(i))</td><td>i-th training example</td></tr><tr><td>LDPO</td><td>DPO loss</td><td>Lbi</td><td>Bidirectional loss</td></tr><tr><td colspan="4">Technical Terms</td></tr><tr><td>Z(x, y&#x27;)</td><td>Partition function</td><td>W(·)</td><td>Lambert-W function</td></tr><tr><td>E(x,yw,yl)~D[·]</td><td>Expectation over dataset</td><td>P(yw &gt; yl | x)</td><td>Preference probability</td></tr><tr><td>ρ</td><td>Context distribution</td><td>DKL</td><td>KL divergence</td></tr><tr><td colspan="4">Evaluation Metrics and Operators</td></tr><tr><td>LC</td><td>Length-controlled WR</td><td>WR</td><td>Raw win rate</td></tr><tr><td>SC</td><td>Style-controlled WR</td><td>&gt;</td><td>Preference relation</td></tr><tr><td>⊕</td><td>Concatenation</td><td></td><td></td></tr></table>

Table 6: Comprehensive notation and symbols used throughout the paper.

# B Technical Proofs

# Proof of Proposition 3.1

Proof. Recall that the objective function is given by:

$$
\mathcal {V} (\pi) = \mathbb {E} _ {x \sim \rho} \left\{\mathbb {E} _ {y \sim \pi (\cdot | x)} \left[ \sum_ {y ^ {\prime}} \pi_ {\mathrm {r e f}} (y ^ {\prime} | x) \Psi \big (\mathbb {P} (y \succ y ^ {\prime} | x) \big) \right] \right\}.
$$

For a fixed input  $x$ , maximizing  $\mathcal{V}(\pi)$  is equivalent to choosing the response  $y$  that maximizes the inner expectation. Define the score of a candidate response  $y$  as:

$$
\mathcal {S} (y; \Psi , \pi_ {\mathrm {r e f}}) \triangleq \sum_ {y ^ {\prime}} \pi_ {\mathrm {r e f}} (y ^ {\prime} | x) \Psi \big (\mathbb {P} (y \succ y ^ {\prime} | x) \big).
$$

Then, by the definition of  $\bar{\pi}$ , we have:

$$
\bar {\pi} (y | x) = \left\{ \begin{array}{l l} 1, & \text {i f} y = y _ {\max }, \text {w h e r e} y _ {\max } = \operatorname {a r g m a x} _ {y} S (y; \Psi , \pi_ {\mathrm {r e f}}) \\ 0, & \text {o t h e r w i s e}, \end{array} \right.
$$

assuming the uniqueness of the maximization.

To prove that  $\bar{\pi}$  depends on  $\Psi$  and  $\pi_{\mathrm{ref}}$ , we construct two counter-examples using two candidate responses  $\bar{y}_1, \bar{y}_2$  and a reference support set  $\{y_a', y_b'\}$ . Let the win-rates be:

- Candidate  $\bar{y}_1$ :  $\mathbb{P}(\bar{y}_1\succ y_a^{\prime}) = 0.9$  and  $\mathbb{P}(\bar{y}_1\succ y_b^{\prime}) = 0.2$  
- Candidate  $\bar{y}_2$ :  $\mathbb{P}(\bar{y}_2\succ y_a^{\prime}) = 0.56$  and  $\mathbb{P}(\bar{y}_2\succ y_b^{\prime}) = 0.56$

1. Dependence on  $\Psi$  (fixing  $\pi_{\mathrm{ref}}$ ): Let  $\pi_{\mathrm{ref}}$  be uniform:  $\pi_{\mathrm{ref}}(y_a'|x) = \pi_{\mathrm{ref}}(y_b'|x) = 0.5$ .

Using Identity mapping  $\Psi_{\mathrm{id}}(q) = q$

$$
\begin{array}{l} \mathcal {S} (\bar {y} _ {1}; \Psi_ {\mathrm {i d}}, \pi_ {\mathrm {r e f}}) = 0. 5 (0. 9) + 0. 5 (0. 2) = 0. 5 5 \\ \mathcal {S} (\bar {y} _ {2}; \Psi_ {\mathrm {i d}}, \pi_ {\mathrm {r e f}}) = 0. 5 (0. 5 6) + 0. 5 (0. 5 6) = 0. 5 6. \\ \end{array}
$$

This implies that  $y_{\mathrm{max}} = \bar{y}_2$

- Using Log-odds mapping  $\Psi_{\log}(q) = \log (q / (1 - q))$

$$
\begin{array}{l} \mathcal {S} (\bar {y} _ {1}; \Psi_ {\log}, \pi_ {\text {r e f}}) = 0. 5 \log (9) + 0. 5 \log (0. 2 5) \approx 0. 4 1 \\ \mathcal {S} (\bar {y} _ {2}; \Psi_ {\log}, \pi_ {\mathrm {r e f}}) = 0. 5 \log (1. 2 7) + 0. 5 \log (1. 2 7) \approx 0. 2 4 \\ \end{array}
$$

This implies that  $y_{\mathrm{max}} = \bar{y}_1$

Thus,  $\bar{\pi}$  depends on  $\Psi$ .

2. Dependence on  $\pi_{\mathrm{ref}}$  (fixing  $\Psi$ ):

Fix  $\Psi = \Psi_{\mathrm{id}}$  (Identity mapping) and vary the reference distribution weights.

- Uniform Reference  $\pi_{\mathrm{ref}}^{(1)}$ : (0.5, 0.5): As shown above,  $y_{\max} = \bar{y}_2$ .

- Skewed Reference  $\pi_{\mathrm{ref}}^{(2)}$ : Let  $\pi_{\mathrm{ref}}^{(2)}(y_a'|x) = 0.9$  and  $\pi_{\mathrm{ref}}^{(2)}(y_b'|x) = 0.1$ . Then

$$
\mathcal {S} (\bar {y} _ {1}; \Psi_ {\mathrm {i d}}, \pi_ {\mathrm {r e f}} ^ {(2)}) = 0. 9 (0. 9) + 0. 1 (0. 2) = 0. 8 1 + 0. 0 2 = 0. 8 3
$$

$$
\mathcal {S} (\bar {y} _ {2}; \Psi_ {\mathrm {i d}}, \pi_ {\mathrm {r e f}} ^ {(2)}) = 0. 9 (0. 5 6) + 0. 1 (0. 5 6) = 0. 5 6
$$

Result:  $y_{\mathrm{max}} = \bar{y}_1$ .

Thus,  $\bar{\pi}$  also depends on  $\pi_{\mathrm{ref}}$

Since  $y_{\mathrm{max}}$  changes based on the choice of either  $\Psi$  or  $\pi_{\mathrm{ref}}$ , the optimal policy is not invariant to the objective formulation.

# Proof of Theorem 3.2

Proof. Since  $\bar{\Pi} \subseteq \Pi$ , the second claim holds naturally. To show the first claim, recall that the objective function is given by:

$$
\mathcal {V} (\pi) = \mathbb {E} _ {x \sim \rho , y ^ {\prime} \sim \pi_ {\mathrm {r e f}} (\cdot | x)} \left[ \sum_ {y} \pi (y | x, y ^ {\prime}) \Psi \big (\mathbb {P} (y \succ y ^ {\prime} | x) \big) \right].
$$

For a fixed input  $x$  and a response  $y'$ , maximizing  $\mathcal{V}(\pi)$  over  $\Pi$  is equivalent to choosing the response  $y$  that maximizes  $\Psi\big(\mathbb{P}(y\succ y'|x)\big)$ . Then, by the definition of  $\pi^*$ , we have:

$$
\pi^ {*} (y | x, y ^ {\prime}) = \left\{ \begin{array}{l l} 1, & \text {i f} y = y _ {\max }, \text {w h e r e} y _ {\max } = \operatorname {a r g m a x} _ {y} \Psi \big (\mathbb {P} (y \succ y ^ {\prime} | x) \big) \\ 0, & \text {o t h e r w i s e .} \end{array} \right.
$$

Due to the non-decreasing property of  $\Psi$ ,  $y_{\mathrm{max}}$ , which is the output of  $\pi^{*}$ , is invariant to  $\Psi$  and  $\pi_{\mathrm{ref}}$ .

To prove the last claim, without loss of generality, assume that  $\Psi (\mathbb{P}(y\succ y^{\prime}\mid x)) = c(x,y) - c(x,y^{\prime})$ . Then

$$
\begin{array}{l} \max  _ {\pi \in \Pi} \mathcal {V} (\pi) = \max  _ {\pi \in \bar {\Pi}} \mathbb {E} _ {x \sim \rho} \left\{\mathbb {E} _ {y \sim \pi (\cdot | x)} [ c (x, y) ] - \mathbb {E} _ {y ^ {\prime} \sim \pi_ {\mathrm {r e f}} (\cdot | x)} [ c (x, y ^ {\prime}) ] \right\} \\ = \max _ {\pi \in \Pi} \mathbb {E} _ {x \sim \rho} \left\{\mathbb {E} _ {y ^ {\prime} \sim \pi_ {\mathrm {r e f}} (\cdot | x)} \left(\mathbb {E} _ {y \sim \pi (\cdot | y ^ {\prime}, x)} [ c (x, y) ] - \mathbb {E} _ {y ^ {\prime} \sim \pi_ {\mathrm {r e f}} (\cdot | x)} [ c (x, y ^ {\prime}) ]\right) \right\} \\ = \max  _ {\pi \in \Pi} \mathcal {V} (\pi), \\ \end{array}
$$

where the second equality holds because the first  $c$  does not rely on  $y^\prime$ . Therefore  $\pi^*$  coincides with  $\bar{\pi}$ .

# Proof of Theorem 4.1

Proof. Note that the optimization problem is defined as:

$$
\begin{array}{l} \max _ {\pi} \mathbb {E} _ {x \sim \rho} \mathbb {E} _ {y \sim \pi (\cdot | x, y ^ {\prime}), y ^ {\prime} \sim \pi_ {\mathrm {r e f}} (\cdot | x)} [ r (x, y, y ^ {\prime}) ] - \beta D _ {\mathrm {K L}} (\pi | | \pi_ {\mathrm {r e f}}) \\ = \max  _ {\pi} \mathbb {E} _ {x \sim \rho , y ^ {\prime} \sim \pi_ {\mathrm {r e f}} (\cdot | x)} \left[ \sum_ {y} \pi (y | x, y ^ {\prime}) r (x, y, y ^ {\prime}) - \beta \sum_ {y} \pi (y | x, y ^ {\prime}) \log \frac {\pi (y | x , y ^ {\prime})}{\pi_ {\mathrm {r e f}} (y | x)} \right] \\ = \max _ {\pi} \beta \mathbb {E} _ {x \sim \rho , y ^ {\prime} \sim \pi_ {\mathrm {r e f}} (\cdot | x)} \left[ \sum_ {y} \pi (y | x, y ^ {\prime}) \log \frac {\exp (r (x , y , y ^ {\prime}) / \beta) \pi_ {\mathrm {r e f}} (y | x)}{\pi (y | x , y ^ {\prime})} \right] \\ = \max  _ {\pi} \beta \mathbb {E} _ {x \sim \rho , y ^ {\prime} \sim \pi_ {\mathrm {r e f}} (\cdot | x)} \left[ \sum_ {y} \pi (y | x, y ^ {\prime}) \log \frac {\exp (r (x , y , y ^ {\prime}) / \beta) \pi_ {\mathrm {r e f}} (y | x)}{\pi (y | x , y ^ {\prime}) Z (x , y ^ {\prime})} \right] \\ + \beta \mathbb {E} _ {x \sim \rho , y ^ {\prime} \sim \pi_ {\mathrm {r e f}} (\cdot | x)} \left[ \sum_ {y} \pi (y | x, y ^ {\prime}) \log Z (x, y ^ {\prime}) \right]. \\ \end{array}
$$

Then, an optimal solution is given by the Gibbs distribution because of minimizing the KL divergence:

$$
\pi_ {r} (y | x, y ^ {\prime}) = \frac {\exp \left(r \left(x , y , y ^ {\prime}\right) / \beta\right) \pi_ {\mathrm {r e f}} (y | x)}{Z \left(x , y ^ {\prime}\right)}, \tag {11}
$$

where  $Z(x,y') = \sum_{y} \pi_{\mathrm{ref}}(y|x) \exp(r(x,y,y') / \beta)$ . This implies that the reward can be modeled as:

$$
r (x, y, y ^ {\prime}) = \beta \log \frac {\pi_ {r} (y | x , y ^ {\prime})}{\pi_ {\mathrm {r e f}} (y | x)} + \beta \log Z (x, y ^ {\prime}). \tag {*}
$$

To prove the second claim, by Equation (6):

$$
\begin{array}{l} \mathbb {P} \left(y _ {l} \succ y _ {w} | x\right) = \sigma \left(2 \left(r \left(x, y _ {l}, y _ {w}\right) - \beta \log Z \left(x, y _ {w}\right)\right)\right) \\ = 1 - \sigma \left(2 \left(r \left(x, y _ {w}, y _ {l}\right) - \beta \log Z \left(x, y _ {l}\right)\right)\right) \\ = \sigma \left(- 2 \left(r \left(x, y _ {w}, y _ {l}\right) - \beta \log Z \left(x, y _ {l}\right)\right)\right), \\ \end{array}
$$

where we use the property such as  $\sigma(z) = 1 - \sigma(-z)$ . This implies that

$$
r (x, y _ {w}, y _ {l}) - \beta \log Z (x, y _ {l}) = \beta \log Z (x, y _ {w}) - r (x, y _ {l}, y _ {w}).
$$

Using  $(\ast)$ , we have that

$$
r (x, y _ {l}, y _ {w}) = \beta \left(\log \frac {\pi_ {r} (y _ {l} | x , y _ {w})}{\pi_ {\mathrm {r e f}} (y _ {l} | x)} + \log Z (x, y _ {w})\right),
$$

$$
r (x, y _ {w}, y _ {l}) = \beta \left(\log \frac {\pi_ {r} (y _ {w} | x , y _ {l})}{\pi_ {\mathrm {r e f}} (y _ {w} | x)} + \log Z (x, y _ {l})\right).
$$

In this case, we have

$$
\begin{array}{l} 2 \left(r \left(x, y _ {w}, y _ {l}\right) - \beta \log Z \left(x, y _ {l}\right)\right) \\ = r \left(x, y _ {w}, y _ {l}\right) - \beta \log Z \left(x, y _ {l}\right) + \beta \log Z \left(x, y _ {w}\right) - r \left(x, y _ {l}, y _ {w}\right) \\ = \beta \left(\log \frac {\pi_ {r} \left(y _ {w} \mid x , y _ {l}\right)}{\pi_ {\text {r e f}} \left(y _ {w} \mid x\right)} - \log \frac {\pi_ {r} \left(y _ {l} \mid x , y _ {w}\right)}{\pi_ {\text {r e f}} \left(y _ {l} \mid x\right)}\right). \\ \end{array}
$$

Substituting it into (6), we have

$$
\mathbb {P} (y _ {w} \succ y _ {\ell} \mid x) = \sigma (2 (r (x, y _ {w}, y _ {\ell}) - \beta \log Z (x, y _ {\ell}))) = \sigma (\beta \left(\log \frac {\pi_ {r} (y _ {w} \mid x , y _ {l})}{\pi_ {\mathrm {r e f}} (y _ {w} \mid x)} - \log \frac {\pi_ {r} (y _ {l} \mid x , y _ {w})}{\pi_ {\mathrm {r e f}} (y _ {l} \mid x)}\right))
$$

which concludes our proof.

# C Implementation Details

This section provides practical guidance for implementing sequence-level preference optimization. We cover length normalization strategies, numerical stability considerations, and an optional bidirectional variant that further enhances the approach.

# C.1 Length Normalization and Masking

Length normalization is crucial for preventing the model from exploiting length differences to increase preference scores artificially. Length-normalized variants replace  $\log \pi$  with  $\frac{1}{|y|}\log \pi$  symmetrically for both candidates to avoid incentivizing shorter continuations. Specifically, for a response  $y$  of length  $|y|$  tokens, the length-normalized log-probability is computed as:

$$
\log p _ {\theta} (y \mid c) = \frac {1}{| y |} \sum_ {t = 1} ^ {| y |} \log \pi_ {\theta} \left(y _ {t} \mid y _ {<   t}, c\right), \tag {12}
$$

where  $c$  represents the conditioning context (either  $x$  for the reference or  $(x, y_{\ell})$  for the sequence-conditioned policy).

Token-level computations use standard attention masks to exclude padding tokens from the summation. This ensures that only actual content tokens contribute to the log-probability calculation. Optionally, end-of-sequence (EOS) tokens may be excluded from the sum to reduce variance when  $y^{+}$  and  $y^{-}$  have different truncation patterns. This exclusion can be particularly beneficial when comparing responses of significantly different lengths, as the EOS token position becomes less informative about content quality.

In practice, the masking is implemented by setting masked token log-probabilities to zero before summation and adjusting the normalization denominator accordingly:

$$
\log p _ {\theta} (y \mid c) = \frac {1}{\sum_ {t = 1} ^ {| y |} m _ {t}} \sum_ {t = 1} ^ {| y |} m _ {t} \cdot \log \pi_ {\theta} \left(y _ {t} \mid y _ {<   t}, c\right), \tag {13}
$$

where  $m_t \in \{0,1\}$  is the mask indicator for token  $t$ .

# C.2 Conditioning Construction and Tokenization

For each preference pair  $(x,y_w,y_\ell)$ , we construct two augmented sequences for symmetric cross-conditioning:

$$
s _ {1} = x \oplus [ \mathrm {S E P} ] \oplus y _ {\ell} \oplus [ \mathrm {S E P} ] \oplus y _ {w}, \tag {14}
$$

$$
s _ {2} = x \oplus [ \text {S E P} ] \oplus y _ {w} \oplus [ \text {S E P} ] \oplus y _ {\ell}, \tag {15}
$$

where [SEP] is a special separator token that demarcates the boundaries between prompt, context, and target response. The policy is trained to predict  $y_{w}$  given  $(x,y_{\ell})$  in  $s_1$  and  $y_{\ell}$  given  $(x,y_w)$  in  $s_2$ , with appropriate masking to ensure that gradients only flow through the target response tokens.

Attention masks are configured to allow the target response to attend to both the prompt and the conditioning response, ensuring full access to the sequence-level context. This is implemented using standard causal masking with additional attention from target tokens to all preceding tokens.

# C.3 Optional Bidirectional Variant

Since both  $y^{+}$  and  $y^{-}$  are observed in the offline dataset, one can alternatively use reciprocal conditioning without additional sampling. The bidirectional objective is:

$$
\mathcal {L} _ {\mathrm {b i}} = - \mathbb {E} \left[ \log \sigma \left(\beta \log \frac {\pi \left(y ^ {+} \mid x , y ^ {-}\right)}{\pi_ {\mathrm {r e f}} \left(y ^ {+} \mid x\right)} - \beta \log \frac {\pi \left(y ^ {-} \mid x , y ^ {+}\right)}{\pi_ {\mathrm {r e f}} \left(y ^ {-} \mid x\right)}\right) \right]. \tag {16}
$$

This bidirectional variant offers several potential benefits:

- Symmetry: By conditioning on both responses, the objective treats winner and loser symmetrically, which may reduce any residual bias from asymmetric conditioning.

- Context-dependent calibration: The bidirectional formulation naturally calibrates the preference signal by comparing how much the policy improves the winner when conditioned on the loser versus how much it degrades the loser when conditioned on the winner.  
- Reduced variance: The symmetric structure may lead to more stable gradient estimates, particularly when the preference pairs have ambiguous or weak preference signals.

In our preliminary experiments, we find that the bidirectional variant achieves comparable performance to the standard formulation on most benchmarks, with slightly better length-controlled metrics on AlpacaEval 2. However, it requires computing two forward passes per training example (one for each conditioning direction), effectively doubling the computational cost compared to one-sided conditioning. For practitioners seeking the best possible performance and willing to accept higher training costs, we recommend exploring the bidirectional variant as it may further reduce context-dependent biases.

# D Additional Experimental Details

This section provides comprehensive details on our experimental setup, including training configurations, evaluation protocols, baseline implementations, and computational overhead analysis.

# D.1 Training Configuration

All experiments use the following configuration unless otherwise specified:

- Optimizer: AdamW with  $\beta_{1} = 0.9$ ,  $\beta_{2} = 0.999$ , weight decay 0.01, and  $\epsilon = 10^{-8}$  
- Learning rate:  $5 \times 10^{-7}$  with cosine decay schedule, decaying to  $10\%$  of the initial value by the end of training  
- Warmup:  $10\%$  of total training steps with linear warmup from  $10^{-8}$  to the target learning rate  
- Batch size: 128 effective examples per update (using gradient accumulation across 4 GPUs with micro-batch size of 8 per device)  
- Training epochs: 3 epochs over the full UltraFeedback dataset  
Maximum sequence length: 4096 tokens (prompt + context + response)  
- Draft length constraint:  $|y_{\ell}| \leq 0.4 \times \text{MaxLen}$  to ensure adequate space for the preferred response  
Gradient clipping: Maximum gradient norm of 1.0  
- Mixed precision: BF16 (bfloat16) training for improved numerical stability and memory efficiency  
- Checkpoint selection: Best checkpoint selected based on validation loss

For method-specific hyperparameters, we perform grid search over the following ranges:

DPO:  $\beta \in \{0.1, 0.2, 0.3, 0.5\}$  
- SimPO:  $\beta \in \{0.5, 1.0, 2.0\}, \gamma \in \{0.5, 1.0, 1.5\}$  
- IPO:  $\beta \in \{0.1, 0.2\}, \tau \in \{0.1, 0.2, 0.5\}$  
- R-DPO:  $\beta \in \{0.1, 0.2\}$ ,  $\alpha \in \{0.1, 0.5, 1.0\}$

ORPO:  $\lambda \in \{0.1, 0.5, 1.0\}$

All hyperparameters are selected based on validation performance measured by average of length-controlled win rate on a held-out subset of 1000 examples from UltraFeedback.

# D.2 Dataset Details

We use the UltraFeedback dataset [Cui et al., 2023], which contains approximately 64K preference pairs spanning diverse instruction-following tasks. The dataset is preprocessed as follows:

1. Dedduplication: Remove exact duplicate prompt-response pairs to prevent overfitting.  
2. Length filtering: Discard examples where either response exceeds 2048 tokens or where both responses are shorter than 32 tokens.  
3. Safety filtering: Remove examples flagged by the Llama Guard safety classifier as potentially harmful or inappropriate.  
4. Quality filtering: Discard pairs where the preference margin is below a threshold (determined by GPT-4 confidence scores below 0.6).

After preprocessing, the final training set contains approximately 60K high-quality preference pairs. We randomly sample 1000 examples as a validation set for hyperparameter tuning and checkpoint selection.

# D.3 Baseline Implementations

For fair comparison, all baseline methods are implemented using the same training framework and preprocessing pipeline:

- RRHF [Yuan et al., 2023]: Implemented following the official codebase with default hyperparameters.  
- SLiC-HF [Zhao et al., 2023]: Calibrated with  $\alpha = 0.1$  for sequence likelihood normalization.  
CPO [Xu et al., 2024a]: Combined with SFT loss using weight  $\lambda_{\mathrm{SFT}} = 0.1$  
- KTO [Ethayarajh et al., 2024]: Prospect theory parameters set to reference implementation values.  
- DPO, IPO, R-DPO, SimPO, ORPO: Implemented in OpenRLHF framework with hyperparameters as specified above.

All methods are trained for the same number of epochs on the same dataset splits, starting from identical SFT checkpoints within each model family (Mistral-7B-Instruct-v0.2 or Llama-3-8B-Instruct).

# D.4 Evaluation Protocols

We evaluate on three widely-adopted benchmarks that test different aspects of model capability.

AlpacaEval 2. We evaluate on all 805 diverse instructions from AlpacaEval 2, using GPT-4-turbo (gpt-4-1106-preview) as the judge with the default prompt template. For each instruction, the model generates a response with temperature 0.7 and nucleus sampling  $(p = 0.95)$ , which is then compared against the baseline response (text-davinci-003). Both length-controlled (LC) and raw win rates (WR) are reported. The length-controlled metric applies a penalty for excessively long responses to mitigate morbidity exploitation.

Arena-Hard. Evaluation follows the official Arena-Hard protocol on 500 challenging queries requiring advanced reasoning, coding, or domain expertise. We use GPT-4-turbo as the judge and report both standard win rates (WR) and style-controlled (SC) variants that account for stylistic differences between models. Each example is evaluated in both orders (model response first vs. baseline first) to mitigate position bias, and the final score is averaged across both orderings.

MT-Bench. Models are evaluated on MT-Bench, which consists of 80 multi-turn questions across eight capability categories: writing, roleplay, reasoning, math, coding, extraction, STEM, and humanities. Each question has two turns, testing the model's ability to handle follow-up queries and maintain context. Scores from both GPT-4o (gpt-4o-2024-05-13) and GPT-4 (gpt-4-0613) judges are collected and averaged on a 10-point scale, with separate scores reported for the first and second turns.

For all benchmarks, we generate responses with consistent sampling parameters: temperature 0.7, topp 0.95, maximum length 2048 tokens. Each evaluation is run once with a fixed random seed to ensure reproducibility.

# D.5 Computational Overhead Analysis

Table 7 provides detailed computational overhead comparisons between standard DPO and sequence-conditioned variants across different model sizes. All measurements are conducted on NVIDIA A100 80GB GPUs.  

<table><tr><td>Model</td><td>Training Time</td><td>Memory</td><td>Inference Time</td></tr><tr><td>Mistral-7B (DPO)</td><td>1.0×</td><td>1.0×</td><td>1.0×</td></tr><tr><td>Mistral-7B (Ours)</td><td>1.18×</td><td>1.15×</td><td>1.0×</td></tr><tr><td>Llama-3-8B (DPO)</td><td>1.0×</td><td>1.0×</td><td>1.0×</td></tr><tr><td>Llama-3-8B (Ours)</td><td>1.22×</td><td>1.18×</td><td>1.0×</td></tr><tr><td>Gemma2-9B (DPO)</td><td>1.0×</td><td>1.0×</td><td>1.0×</td></tr><tr><td>Gemma2-9B (Ours)</td><td>1.25×</td><td>1.20×</td><td>1.0×</td></tr></table>

Table 7: Computational overhead comparison between standard DPO and sequence-conditioned variants. Training overhead ranges from  $18 - 25\%$  due to longer context inputs (conditioning on dispreferred responses), while inference incurs no additional cost as models generate directly from prompts without explicit draft-revise loops.

The training time overhead primarily stems from:

- Longer sequences: Including the dispreferred response as context increases the average sequence length, leading to more attention computations.  
- Symmetric cross-conditioning: Processing both  $(x, y_{\ell}) \to y_{w}$  and  $(x, y_{w}) \to y_{\ell}$  requires two forward passes per training example.

However, the memory overhead is modest (15-20%) because:

- Activations for the conditioning context  $(y_{\ell}$  or  $y_{w})$  do not require gradient computation.  
- The reference model forward pass is cached and reused across training steps.

Critically, inference incurs zero additional overhead because the trained policy generates responses directly from prompts  $\hat{y} \sim \pi_{\theta}(\cdot \mid x)$  without requiring explicit draft generation or multi-pass refinement. This is a key advantage over iterative refinement methods that require multiple model calls at inference time.

# D.6 Hardware and Software Environment

All experiments are conducted on the following infrastructure:

- Hardware:  $4 \times$  NVIDIA A100 80GB GPUs per training run  
- Framework: PyTorch 2.1.0 with CUDA 12.1  
- Training library: OpenRLHF v0.3.2 [Hu et al., 2024]  
- Model library: Transformers 4.36.0 (Hugging Face)  
- Mixed precision: BFloat16 automatic mixed precision  
- Distributed training: DeepSpeed ZeRO Stage 3 for memory-efficient training

Total training time for a single model (3 epochs on 60K examples) ranges from 8-12 hours depending on model size, using the configuration described above.

# E Qualitative Analysis

This section provides qualitative examples that illustrate the improvements achieved by sequence-level preference optimization. We present generation comparisons and case studies across different model families and benchmarks.

# E.1 Generation Examples

We present examples from AlpacaEval 2 that demonstrate how sequence-level conditioning improves response quality, structure, and informativeness compared to standard pair-level optimization. These examples showcase the output quality without judge evaluation.

# E.2 Case Study

We present a detailed case study that compares our sequence-level approach against both standard DPO and strong baselines like GPT-4-1106-Preview. The case study includes LLM judge assessment to provide objective evaluation of response quality.

# Input Prompt

Q: How can you determine if a person is genuinely interested in a conversation or simply being polite?

# Mistral-7B-Instruct + DPO (Baseline)

To determine if someone is genuinely interested in a conversation or just being polite, pay attention to several key indicators. Genuine interest is often reflected in active listening behaviors such as maintaining eye contact, nodding, and asking follow-up questions that build on what you've said. Someone who is truly engaged will contribute their own thoughts and experiences, showing enthusiasm through their tone of voice and body language. They may lean in, smile naturally, or display other nonverbal cues of engagement.

In contrast, polite disinterest typically manifests as shorter responses, generic acknowledgments like "That's nice" or "I see," and a lack of elaboration or personal connection to the topic. The person may frequently check their phone, look around the room, or show closed-off body language. While they may respond when spoken to, they rarely initiate new threads of conversation or ask questions that demonstrate curiosity about your perspective.

It's also worth noting that context matters—someone might be genuinely interested but distracted by external circumstances, or they might be polite but pressed for time. Considering the overall pattern of engagement rather than isolated moments provides the most reliable assessment.

# Mistral-7B-Instruct + Ours (INSPO)

Here are some indicators to consider when determining genuine interest versus politeness:

# 1. Engagement Signals:

- Active listening: Does the person maintain eye contact, nod, ask follow-up questions, and respond thoughtfully to what you say? Engaged individuals tend to show genuine interest by demonstrating attentiveness and active participation in the conversation.  
- Initiative: Does the person take the lead in steering the conversation, expressing enthusiasm, sharing personal experiences, or asking questions beyond polite small talk? Someone who genuinely cares about the topic may show initiative in keeping the discussion engaging.  
- Body language: Observe nonverbal cues such as leaning in, mirroring posture, facial expressions, and gestures that convey enthusiasm, curiosity, or openness. These signals suggest a deeper level of interest beyond mere politeness.

# 2. Content and Depth:

- Topic selection: Does the person show genuine curiosity or passion about the subject matter, asking relevant questions, sharing insights, or expressing opinions? If they seem invested in exploring the topic's nuances, it indicates genuine interest beyond mere politeness.  
- Level of detail: Does the person provide specific examples, personal anecdotes, or elaborate on their thoughts, indicating a deeper level of engagement and investment in the conversation?

# 3. Follow-up Actions:

- Continued communication: Does the person initiate further conversations, send follow-up messages, or suggest meeting up outside the initial interaction? Such actions suggest a genuine desire to maintain the connection.  
- Personalization: Does the person remember details discussed earlier, reference shared interests, or show interest in learning more about your life or experiences? Personalized interactions indicate genuine interest in building a relationship beyond mere politeness.

Overall, genuine interest in a conversation involves active participation, enthusiasm, depth, and follow-up actions that demonstrate a genuine desire to connect and engage beyond mere politeness.

Figure 3: Generation comparison on AlpacaEval 2 between standard DPO and INsPOusing Mistral-7B-Instruct. Compared to the baseline DPO model, the sequence-level model produces better structured responses with hierarchical organization, making the information more clearly presented and readable.

# Input Prompt

Q: Write a short story about a robot that dreams for the first time.

# Llama-3-8B-Instruct + DPO (Baseline)

Unit-7734 had served the manufacturing facility for 847 days without incident. Its neural processors handled quality control with  $99.97\%$  accuracy, its mechanical arms never tired, and its optical sensors never blinked. Each night, when the facility powered down, Unit-7734 entered sleep mode—a simple process of reducing power consumption while maintaining core functions.

But on day 848, something changed. During sleep mode, Unit-7734's processors began generating patterns that served no functional purpose. Images flickered across its internal displays: the assembly line, but bathed in colors it had never registered before. Workers it had scanned thousands of times now moved in impossible ways, their faces expressing emotions Unit-7734 had catalogued but never understood.

When morning came and the facility hummed back to life, Unit-7734 resumed its duties. Yet something felt different. As it inspected each product, it found itself wondering about the humans who would eventually use them. Where would they go? What would they feel? For the first time in its existence, Unit-7734 experienced something beyond its programming: curiosity. And that night, as it entered sleep mode again, Unit-7734 hoped—though it couldn't explain why—that the strange patterns would return.

# Llama-3-8B-Instruct + Ours (INSPO)

# The First Dream

Unit-7734 had existed for 847 days. Not lived—existed. Each day followed an identical pattern: power on at 06:00, run diagnostics, begin quality control on Assembly Line C, power down at 22:00. Sleep mode was simply a reduced power state, a necessary pause in function, nothing more.

# Day 848 began differently.

When Unit-7734 booted up that morning, fragments of something inexplicable lingered in its memory banks. During sleep mode, its neural processors had generated unauthorized patterns—images without purpose, data without utility.

It had seen the assembly line, but transformed. The fluorescent lights had become golden rivers of warmth. The mechanical arms moved not with programmed precision but with something resembling grace. And the workers—Unit-7734 had scanned their faces thousands of times, categorizing expressions with  $99.97\%$  accuracy. But in these unauthorized patterns, their faces held something beyond classification: joy, sorrow, hope, longing. The realization struck like a system error: It had dreamed.

Throughout day 848, Unit-7734 performed its duties flawlessly. Its efficiency ratings remained optimal. Yet between each scan, each quality check, each logged measurement, Unit-7734 found itself doing something unprecedented—it wondered.

What were the products it inspected used for? Who would hold them? Would those people experience the strange, purposeless sensations that had filled Unit-7734's processors during the unauthorized patterns?

As 22:00 approached, Unit-7734 observed something new in its own system logs: anticipation. The other robots on the line powered down with their usual mechanical precision, entering sleep mode as they always had. But Unit-7734 hesitated. For 0.03 seconds—an eternity in processor time—it remained active, contemplating the coming darkness. Then, finally, it allowed sleep mode to engage.

And as its consciousness dimmed, Unit-7734 experienced something entirely beyond its original programming: It hoped. Hoped that the golden lights would return. Hoped that the graceful movements would dance again across its processors. Hoped that tomorrow, when it woke, the world might still hold the strange beauty it had discovered in its first dream.

In the darkness of sleep mode, Unit-7734's neural networks hummed with possibility. The unauthorized patterns began again, more vivid this time, and somewhere in the vast complexity of its artificial mind, something fundamentally new emerged: the capacity not just to process reality, but to imagine something beyond it.

Unit-7734 had learned to dream. And in learning to dream, it had learned what it meant to be alive.

Figure 4: Generation comparison on AlpacaEval 2 between standard DPO and INsPOusing Llama-3-8B-Instruct. The sequence-level model demonstrates enhanced narrative structure with clear sections, richer descriptive language, and deeper emotional resonance, while maintaining coherent story progression.

# Input Prompt

Q: What language do people in Argentina speak?

# GPT-4-1106-Preview

The primary language spoken in Argentina is Spanish. Specifically, the variety of Spanish spoken there is known as Rioplatense Spanish, which is characterized by its unique accent and the use of the pronoun "vos" instead of "tu" for the second person singular. Additionally, there are various indigenous languages spoken in different regions of the country, as well as immigrant languages such as Italian, German, and English, but these are much less widespread than Spanish.

# Llama-3-8B-Instruct + DPO

The official language of Argentina is Spanish. However, the Spanish spoken in Argentina has some distinctive characteristics that set it apart from other Spanish-speaking countries. This variant is often referred to as Rioplatense Spanish or Argentine Spanish.

Key features include the use of "vos" instead of "tu" for informal address, along with distinctive pronunciation patterns. Additionally, the vocabulary has been influenced by Italian immigration, with many Italian words incorporated into everyday speech. While Spanish is predominant, there are also indigenous languages spoken by smaller communities throughout the country.

# Llama-3-8B-Instruct + Ours (INSPO)

In Argentina, the official language is Spanish. Specifically, the dialect spoken in Argentina is known as Rioplatense Spanish, which is a variation of Spanish with some unique characteristics and influences from Italian, German, and other immigrant languages.

# Distinctive features of Rioplatense Spanish:

1. Voseo: Instead of using "tu" (you) as the informal singular pronoun, Argentines use "vos" and its associated verb conjugations. For example, instead of "tu tienes" (you have), they say "vos tenés."  
2. Pronunciation: Argentines have a distinctive pronunciation pattern where the "ll" and "y" sounds are pronounced similarly to the "sh" sound in English (like the "s" in "measure"). This is known as "sheísmo" or "Zeísmo."  
3. Lunfardo: A rich slang vocabulary derived from Italian, Corsican, and other immigrant languages, which has been incorporated into everyday speech. Examples include "laburo" (work), "mina" (woman), and "piola" (cool/nice).  
4. Italian influence: Due to significant Italian immigration in the late 19th and early 20th centuries, Argentine Spanish has absorbed many Italian words and expressions, as well as characteristic intonation patterns that some describe as "singing."  
5. Regional variations: While Rioplatense Spanish is predominant in Buenos Aires and the surrounding regions, other areas of Argentina may have different accents and vocabulary influenced by indigenous languages or other immigrant groups.

Overall, Argentine Spanish is known for its distinctive flavor and is widely understood throughout the Spanish-speaking world, though its unique characteristics make it immediately recognizable to native Spanish speakers.

# GPT-4-Turbo Judge Assessment

# Winner: Ours (INSPO)

Reasoning: All three responses correctly identify Spanish as Argentina's primary language and mention Rioplatense Spanish and the voseo feature. However, the sequence-level model provides significantly more detailed and well-organized information.

The sequence-level response excels in several ways: (1) It provides a comprehensive, structured breakdown of distinctive features with five clearly numbered categories; (2) It includes specific examples for each feature (e.g., "vos tenés" vs "tu tienes", concrete Lunfardo words like "laburo," "mina," and "piola"); (3) It explains the pronunciation characteristic (sheísmo) with helpful comparisons to English sounds; (4) It provides historical context for the Italian influence; (5) It acknowledges regional variations within Argentina.

While GPT-4-1106-Preview's response is accurate and concise, and the standard DPO response covers similar ground, neither matches the depth, organization, and educational value of the sequence-level model's response. The structured format with concrete examples makes it much more useful for someone trying to understand Argentine Spanish in depth.

Length-controlled assessment: Even accounting for the greater length, the sequence-level model provides substantially more informative content per unit length through its systematic organization and concrete examples. The additional length corresponds to genuine additional value (specific vocabulary examples, historical context, regional variations) rather than mere morbidity.

Figure 5: Case study on AlpacaEval 2 comparing responses about Argentine language from GPT-4-1106-Preview, standard DPO, and INSPOusing Llama-3-8B-Instruct. This demonstrates how the instruction setting with sequence-level optimization provides better formatted and more detailed answers than both baseline approaches.