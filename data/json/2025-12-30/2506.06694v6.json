[
  {
    "type": "text",
    "text": "Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning",
    "text_level": 1,
    "bbox": [
      130,
      99,
      869,
      151
    ],
    "page_idx": 0
  },
  {
    "type": "text",
    "text": "Yuan Yuan*, Yukun Liu*, Chonghua Han, Jie Feng, Yong Li  \nCenter for Urban Science and Computation, Tsinghua University",
    "bbox": [
      264,
      161,
      733,
      193
    ],
    "page_idx": 0
  },
  {
    "type": "text",
    "text": "Abstract",
    "text_level": 1,
    "bbox": [
      83,
      203,
      156,
      217
    ],
    "page_idx": 0
  },
  {
    "type": "text",
    "text": "Human mobility is a fundamental pillar of urban science and sustainability, providing critical insights into energy consumption, carbon emissions, and public health. However, the discovery of universal mobility laws is currently hindered by the \"data silo\" problem, where institutional boundaries and privacy regulations fragment the necessary large-scale datasets. In this paper, we propose MoveGCL, a transformative framework that facilitates collaborative and decentralized mobility science via generative continual learning. MoveGCL enables a distributed ecosystem of data holders to jointly evolve a foundation model without compromising individual privacy. The core of MoveGCL lies in its ability to replay synthetic trajectories derived from a generative teacher and utilize a mobility-pattern-aware Mixture-of-Experts (MoE) architecture. This allows the model to encapsulate the unique characteristics of diverse urban structures while mitigating the risk of knowledge erosion (catastrophic forgetting). With a specialized layer-wise progressive adaptation strategy, MoveGCL ensures stable convergence during the continuous integration of new urban domains. Our experiments on six global urban datasets demonstrate that MoveGCL achieves performance parity with joint training, a previously unattainable feat under siloed conditions. This work provides a scalable, privacy-preserving pathway toward Open Mobility Science, empowering researchers to address global sustainability challenges through cross-institutional AI collaboration. To facilitate reproducibility and future research, we have released the code and models at https://github.com/tsinghua-fib-lab/MoveGCL.",
    "bbox": [
      81,
      222,
      482,
      583
    ],
    "page_idx": 0
  },
  {
    "type": "text",
    "text": "1 Introduction",
    "text_level": 1,
    "bbox": [
      83,
      594,
      218,
      607
    ],
    "page_idx": 0
  },
  {
    "type": "text",
    "text": "The study of human mobility is more than a technical task; it is a cornerstone of urban systems theory and sustainability science [16, 46]. Understanding how billions of people move through urban environments is essential for optimizing transportation efficiency, reducing the global carbon footprint, and managing epidemiological risks [35, 57, 62]. In the era of Foundation Models, fields such as natural language processing (NLP) [4, 5, 20] and computer vision (CV) [9, 30] have undergone a paradigm shift. This evolution presents an unprecedented opportunity to move beyond insular empirical studies, which often focus on isolated datasets and specific cities, toward a unified, generalizable science of mobility.",
    "bbox": [
      81,
      611,
      482,
      763
    ],
    "page_idx": 0
  },
  {
    "type": "text",
    "text": "However, a critical barrier remains: the fragmentation of mobility data across organizational and national boundaries [56, 64]. Unlike the public-domain text used in NLP and CV, mobility data is inherently sensitive, held by disparate telecommunications operators or transit agencies in isolated \"data silo\" due to privacy concerns and stringent regulatory frameworks (e.g., GDPR) [22, 53]. This isolation prevents the scientific community from training large-scale models that can generalize across different urban morphologies and",
    "bbox": [
      81,
      763,
      482,
      876
    ],
    "page_idx": 0
  },
  {
    "type": "text",
    "text": "socio-economic contexts. Consequently, the field is stuck in a paradigm where models are trained on small, localized datasets, failing to capture the complex, universal patterns of human movement.",
    "bbox": [
      511,
      205,
      913,
      244
    ],
    "page_idx": 0
  },
  {
    "type": "text",
    "text": "To address these data barriers, recent studies have proposed different training strategies to leverage multiple datasets, and Figure 1 compares different approaches. The most common strategy is to train models separately, as shown in Figure 1(a). Recent efforts such as UniTraj [65], TrajBert [40], and TrajFM [27] have explored joint training for unified representation and cross-city generalization (Figure 1(b)), but these models remain tightly coupled with proprietary or limited-quality datasets. TrajFM and TrajBert are typically pre-trained on restricted or private data. While UniTraj uses public data, it suffers from low semantic richness and high sparsity. Federated learning [12, 31] offers a potential solution for distributed mobility model training (Figure 1(c)), but its reliance on frequent synchronization and communication poses challenges for scalability and practical deployment. Consequently, these approaches fall short of meeting the diverse, dynamic, and multi-source demands of real-world mobility modeling, and cannot support an open ecosystem of shared models as seen in NLP and CV domains.",
    "bbox": [
      511,
      246,
      913,
      481
    ],
    "page_idx": 0
  },
  {
    "type": "text",
    "text": "To truly usher in an era of shareable and sustainable human mobility science, we argue for a new collaborative paradigm. This paradigm enables multiple data holders to jointly evolve and continually build a foundation model without sharing raw data, while preserving both privacy and generalization capability. However, this vision poses several critical challenges: Privacy preservation: How to achieve cross-institutional collaboration without any raw data exchange. (2) Cumulative knowledge retention: How to prevent \"catastrophic forgetting\" so that the model can accumulate knowledge from diverse urban systems over time. (3) Scientific heterogeneity: How to design an architecture that captures the unique characteristics of different urban structures (e.g., the grid layouts of New York vs. the radial patterns of Paris).",
    "bbox": [
      511,
      482,
      913,
      661
    ],
    "page_idx": 0
  },
  {
    "type": "text",
    "text": "We propose MoveGCL, a scalable training framework for mobility foundation models based on generative continual learning. MoveGCL allows each data holder to evolve a shared model locally without exposing raw data, thereby ensuring full privacy preservation. Specifically, MoveGCL starts from a pre-trained base model and employs a synthetic trajectory replay mechanism: instead of accessing historical data, each participant generates synthetic trajectories that approximate previously seen mobility patterns. This replay process preserves prior knowledge and mitigates catastrophic forgetting. Furthermore, knowledge distillation is applied during replay to reinforce the model's ability to retain past capabilities while adapting to new data. To handle the diversity of mobility data, MoveGCL adopts a Mixture-of-Experts (MoE) architecture equipped with a mobility pattern-aware expert routing mechanism. This design enables the model to dynamically select expert modules",
    "bbox": [
      511,
      661,
      913,
      869
    ],
    "page_idx": 0
  },
  {
    "type": "aside_text",
    "text": "arXiv:2506.06694v6 [cs.LG] 29 Dec 2025",
    "bbox": [
      22,
      276,
      57,
      724
    ],
    "page_idx": 0
  },
  {
    "type": "page_footnote",
    "text": "*Equal contribution.",
    "bbox": [
      84,
      883,
      181,
      895
    ],
    "page_idx": 0
  },
  {
    "type": "image",
    "img_path": "images/7717c77417ffeab720897be522b1c161c4a104dd75a460721856630d2116aba4.jpg",
    "image_caption": [
      "(a) Training separately"
    ],
    "image_footnote": [],
    "bbox": [
      101,
      140,
      151,
      209
    ],
    "page_idx": 1
  },
  {
    "type": "image",
    "img_path": "images/1b99c1273b43f2a4c74621d2a3e8fd68a3fd3b256a9ff72899689de6fddfe1ca.jpg",
    "image_caption": [],
    "image_footnote": [],
    "bbox": [
      161,
      140,
      200,
      209
    ],
    "page_idx": 1
  },
  {
    "type": "image",
    "img_path": "images/7fec2769333e73c1684a9868bd8b4e8c899e79e9b4c77c2bf392c466e70cd831.jpg",
    "image_caption": [
      "(b) Joint training with all data"
    ],
    "image_footnote": [],
    "bbox": [
      215,
      140,
      264,
      209
    ],
    "page_idx": 1
  },
  {
    "type": "image",
    "img_path": "images/653f8aa6ecf9eae58cc2f71d8f2d3cb46290bc782dca0fa8b9d585f48153b5a6.jpg",
    "image_caption": [
      "Figure 1: Method comparison with multiple mobility datasets."
    ],
    "image_footnote": [],
    "bbox": [
      315,
      135,
      433,
      210
    ],
    "page_idx": 1
  },
  {
    "type": "image",
    "img_path": "images/97438945c5da12917aa7779955835790fc5c9d15d3ec20a4f10054b1f8784895.jpg",
    "image_caption": [
      "(c) Federated learning"
    ],
    "image_footnote": [],
    "bbox": [
      478,
      130,
      635,
      220
    ],
    "page_idx": 1
  },
  {
    "type": "image",
    "img_path": "images/7c5f4ea60841d6a85f30ea5fc3b2d45b56ef763d8e9fba4ca1ca770e530216c2.jpg",
    "image_caption": [
      "(d) Continual learning"
    ],
    "image_footnote": [],
    "bbox": [
      684,
      128,
      888,
      220
    ],
    "page_idx": 1
  },
  {
    "type": "text",
    "text": "tailored to local mobility characteristics. Together, these innovations make MoveGCL a practical and privacy-preserving solution for collaboratively building generalizable mobility foundation models across distributed, heterogeneous, and privacy-sensitive data sources. In summary, our key contributions are as follows:",
    "bbox": [
      81,
      276,
      482,
      347
    ],
    "page_idx": 1
  },
  {
    "type": "list",
    "sub_type": "text",
    "list_items": [
      "- We formalize the first privacy-preserving collaborative training paradigm for mobility foundation models, enabling the scientific community to build unified models across data silos.",
      "- We propose MoveGCL, a novel framework based on generative continual learning. Its core components—knowledge distillation, mobility-aware expert routing, and layer-wise progressive adaptation address catastrophic forgetting and urban heterogeneity, facilitating the long-term accumulation of mobility knowledge.",
      "- We demonstrate on six diverse global urban datasets that MoveGCL achieves performance comparable to centralized joint training, providing a practical pathway toward Open Mobility Science."
    ],
    "bbox": [
      83,
      349,
      488,
      502
    ],
    "page_idx": 1
  },
  {
    "type": "text",
    "text": "2 Related Work",
    "text_level": 1,
    "bbox": [
      83,
      513,
      225,
      527
    ],
    "page_idx": 1
  },
  {
    "type": "text",
    "text": "Due to space limitations, we provide a concise review here and defer the extended discussion to Appendix B.",
    "bbox": [
      81,
      532,
      482,
      561
    ],
    "page_idx": 1
  },
  {
    "type": "text",
    "text": "Mobility Data. Mobility data is commonly represented as aggregated flows or individual trajectories [3, 53, 60]. While flow data is relatively accessible and widely used in urban analytics [36, 58], trajectory data is often fragmented due to privacy constraints and institutional silos [22, 53]. Existing real-world and open trajectory datasets are typically limited in city coverage, duration, or sampling density, and even recent global-scale datasets may suffer from inconsistent resolution and quality. Generative AI has enabled synthetic mobility datasets [47, 56, 64], but matching real trajectories remains challenging, especially in behavioral diversity and temporal continuity. As a result, many studies rely on restricted-access data and cannot release the trajectories they use [37].",
    "bbox": [
      81,
      569,
      482,
      734
    ],
    "page_idx": 1
  },
  {
    "type": "text",
    "text": "Mobility Foundation Models. Recent work has begun to build foundation models for urban and mobility applications [6, 7, 15, 55, 59, 63]. Most efforts focus on aggregated mobility signals and demonstrate strong cross-city transfer via unified spatio-temporal pre-training [24, 25, 51, 52]. In contrast, foundation models for individual mobility remain less mature due to limited representative trajectory corpora and the complexity of human behavior [32, 53, 60]. LLM-based trajectory generation has also been explored [10, 14, 17, 18, 38], but the modality gap suggests the need for native mobility foundation models that can later be aligned with language-based reasoning.",
    "bbox": [
      81,
      742,
      482,
      896
    ],
    "page_idx": 1
  },
  {
    "type": "text",
    "text": "Continual Learning. Continual (lifelong) learning aims to update models on sequential data without degrading previously learned capabilities [19, 42, 49]. A key challenge is catastrophic forgetting [23, 44]. Existing methods typically fall into regularization, replay, and parameter-isolation families, which trade off memory, computation, and stability when adapting to new data streams.",
    "bbox": [
      511,
      277,
      916,
      361
    ],
    "page_idx": 1
  },
  {
    "type": "text",
    "text": "3 Preliminaries",
    "text_level": 1,
    "bbox": [
      514,
      372,
      656,
      386
    ],
    "page_idx": 1
  },
  {
    "type": "text",
    "text": "Data Format. In our setting, human mobility data is represented as sequences of spatiotemporal tokens, where each token corresponds to a visited location at a specific time. The spatial domain is typically discretized into a uniform grid  $(500\\mathrm{m} \\times 500\\mathrm{m}$  resolution), and the temporal domain is segmented into fixed-length intervals (30 minutes). Each individual trajectory can be formulated as a sequence  $z_{1}, z_{2}, \\ldots, z_{T}$ , where  $z_{t} = (l_{t}, t_{t})$  denotes the location and timestamp of a mobility event at time step  $t$ . These sequences capture rich behavioral patterns across time and space and form the foundation for model training.",
    "bbox": [
      511,
      390,
      915,
      529
    ],
    "page_idx": 1
  },
  {
    "type": "text",
    "text": "Model Training via Next-Token Prediction. Following the standard practice in language modeling, the training objective for mobility foundation models is formulated as a next-location prediction task. Given a partial trajectory  $\\{z_{1},\\ldots ,z_{t - 1}\\}$ , the model aims to predict the next location  $l_{t}$ , where  $l_{t}$  represents the spatial component of the upcoming step in the trajectory. Formally, the training objective is defined as maximizing the log-likelihood of the observed sequence:",
    "bbox": [
      511,
      537,
      915,
      648
    ],
    "page_idx": 1
  },
  {
    "type": "equation",
    "text": "\n$$\n\\mathcal {L} = \\sum_ {t = 1} ^ {T} \\log P \\left(l _ {t} \\mid z _ {1}, \\dots , z _ {t - 1}; \\theta\\right), \\tag {1}\n$$\n",
    "text_format": "latex",
    "bbox": [
      612,
      652,
      911,
      689
    ],
    "page_idx": 1
  },
  {
    "type": "text",
    "text": "where  $\\theta$  denotes the model parameters. This objective allows the model to learn rich dependencies across spatial locations, temporal patterns, and contextual semantics, and serves as the core pretraining strategy for mobility foundation models.",
    "bbox": [
      511,
      694,
      915,
      750
    ],
    "page_idx": 1
  },
  {
    "type": "text",
    "text": "Training Pipeline for Mobility Foundation Model Development. We adopt a continual learning paradigm to train the mobility foundation model. To simulate learning on highly heterogeneous data, each round of continual learning introduces the dataset of a new city to the model. Initially, the base model  $f_{\\mathrm{base}}$  is trained using the base dataset  $\\mathcal{D}_{\\mathrm{base}}$ . During continual learning, at the beginning of the  $n$ -th round, the model has already been trained on the dataset  $\\mathcal{D}_{\\mathrm{all},n-1} = \\mathcal{D}_{\\mathrm{base}} \\cup \\mathcal{D}_{\\mathrm{continual},n-1}$ , where  $\\mathcal{D}_{\\mathrm{continual},n-1} = \\bigcup_{i=1}^{n-1} d_i$  and each  $d_i$  represents the dataset introduced in the  $i$ -th round. However, the historical dataset  $\\mathcal{D}_{\\mathrm{all},n-1}$  is no longer accessible. The",
    "bbox": [
      511,
      757,
      915,
      896
    ],
    "page_idx": 1
  },
  {
    "type": "header",
    "text": "Conference'17, July 2017, Washington, DC, USA",
    "bbox": [
      83,
      75,
      313,
      85
    ],
    "page_idx": 1
  },
  {
    "type": "header",
    "text": "Yuan et al.",
    "bbox": [
      859,
      75,
      911,
      85
    ],
    "page_idx": 1
  },
  {
    "type": "text",
    "text": "model update at round  $n$  is performed solely based on the current data  $d_{n}$  and a copy of the model from the previous round, denoted as  $f_{\\mathrm{old},n}$ . The continual learning process can be formalized as the following optimization objective:",
    "bbox": [
      81,
      106,
      480,
      162
    ],
    "page_idx": 2
  },
  {
    "type": "equation",
    "text": "\n$$\n\\theta_ {\\text {n e w}, n} = \\arg \\min  _ {\\theta} \\mathcal {L} (\\theta ; d _ {n}; f _ {\\text {o l d}, n}), \\quad \\text {s . t .} \\quad \\theta \\leftarrow \\theta_ {\\text {o l d}, n}, \\tag {2}\n$$\n",
    "text_format": "latex",
    "bbox": [
      125,
      167,
      482,
      186
    ],
    "page_idx": 2
  },
  {
    "type": "text",
    "text": "where  $\\theta$  denotes the model parameters, and  $\\theta_{\\mathrm{old},n}$  and  $\\theta_{\\mathrm{new},n}$  correspond to the parameters of  $f_{\\mathrm{old},n}$  and  $f_{\\mathrm{new},n}$ , respectively. The loss function  $\\mathcal{L}(\\theta; d_n; f_{\\mathrm{old},n})$  incorporates constraint terms derived from the previous model  $f_{\\mathrm{old},n}$  to mitigate catastrophic forgetting. Rather than reinitializing from scratch, we optimize  $\\theta_{\\mathrm{new},n}$  starting from  $\\theta_{\\mathrm{old},n}$ . Specifically, when  $n = 0$ , we set  $f_{\\mathrm{old},n} = f_{\\mathrm{base}}$ ; for  $n > 0$ , we have  $f_{\\mathrm{old},n} = f_{\\mathrm{new},n - 1}$ .",
    "bbox": [
      81,
      191,
      482,
      289
    ],
    "page_idx": 2
  },
  {
    "type": "text",
    "text": "4 MoveGCL",
    "text_level": 1,
    "bbox": [
      83,
      300,
      194,
      313
    ],
    "page_idx": 2
  },
  {
    "type": "text",
    "text": "In this section, we introduce the overall framework of MoveGCL, which is shown in Figure 2.",
    "bbox": [
      81,
      318,
      482,
      347
    ],
    "page_idx": 2
  },
  {
    "type": "text",
    "text": "4.1 Generative Continual Learning",
    "text_level": 1,
    "bbox": [
      83,
      358,
      382,
      375
    ],
    "page_idx": 2
  },
  {
    "type": "text",
    "text": "Generative Replay with Teacher Model. To retain knowledge from previously visited cities without storing real-world mobility trajectory data, we design a generative replay strategy, as illustrated in Figure 2(a). At each stage, we keep a copy of the previously trained model  $f_{\\mathrm{old}}$ , referred to as the teacher model. This teacher model represents the model trained on earlier mobility datasets. It remains frozen during subsequent learning and serves as a knowledge source to guide the student model  $f_{\\mathrm{new}}$  when learning new cities.",
    "bbox": [
      81,
      377,
      482,
      500
    ],
    "page_idx": 2
  },
  {
    "type": "text",
    "text": "To simulate past mobility behaviors, we employ  $f_{\\mathrm{old}}$  to generate synthetic trajectory data  $\\tilde{x}_{\\mathrm{old}}^{ci}$ . First, we extract a trajectory",
    "bbox": [
      83,
      501,
      480,
      531
    ],
    "page_idx": 2
  },
  {
    "type": "equation",
    "text": "\n$$\nx _ {\\mathrm {n e w}} = \\left[ \\left(l _ {0} ^ {\\prime}, t _ {0} ^ {\\prime}\\right), \\left(l _ {1} ^ {\\prime}, t _ {1} ^ {\\prime}\\right), \\dots , \\left(l _ {L} ^ {\\prime}, t _ {L} ^ {\\prime}\\right) \\right], \\tag {3}\n$$\n",
    "text_format": "latex",
    "bbox": [
      166,
      534,
      482,
      551
    ],
    "page_idx": 2
  },
  {
    "type": "text",
    "text": "from the new dataset, where  $L$  denotes its length and  $\\{(l_i', t_i')\\}_{i=0}^L$  are the location-time pairs. Next, for a specific previously observed city  $c_i$ , we sample an initial location from the empirical distribution of actual locations in city  $c_i$  conditioned on length  $L$ :  $l_0 \\sim \\rho_{\\mathrm{loc}|L}^{c_i}$  where  $\\rho_{\\mathrm{loc}|L}^{c_i}$  denotes the empirical distribution of initial locations in previously observed city  $c_i$  given a trajectory length  $L$ . We then replace  $l_0'$  with the sampled  $l_0$  and generate the pseudo old-city trajectory",
    "bbox": [
      81,
      555,
      482,
      671
    ],
    "page_idx": 2
  },
  {
    "type": "equation",
    "text": "\n$$\n\\tilde {x} _ {\\mathrm {o l d}} ^ {c _ {i}} = \\left[ (l _ {0}, t _ {0} ^ {\\prime}), (l _ {1}, t _ {1} ^ {\\prime}), \\dots , (l _ {L}, t _ {L} ^ {\\prime}) \\right],\n$$\n",
    "text_format": "latex",
    "bbox": [
      166,
      674,
      398,
      691
    ],
    "page_idx": 2
  },
  {
    "type": "text",
    "text": "by drawing subsequent locations autoregressively:",
    "bbox": [
      81,
      695,
      390,
      709
    ],
    "page_idx": 2
  },
  {
    "type": "equation",
    "text": "\n$$\n\\left(l _ {1}, l _ {2}, \\dots , l _ {L}\\right) \\sim f _ {\\text {o l d}} \\left(\\cdot \\mid l _ {0}, \\left\\{t _ {i} ^ {\\prime} \\right\\} _ {i = 0} ^ {L}\\right), \\tag {4}\n$$\n",
    "text_format": "latex",
    "bbox": [
      166,
      714,
      482,
      736
    ],
    "page_idx": 2
  },
  {
    "type": "text",
    "text": "where each  $t_i'$  is taken from the time distribution of the extracted new-data trajectory.",
    "bbox": [
      81,
      739,
      480,
      767
    ],
    "page_idx": 2
  },
  {
    "type": "text",
    "text": "These pseudo-trajectories reflect the mobility patterns learned in earlier stages. We combine all pseudo-trajectories with the real data from the current city, to construct the full training set:",
    "bbox": [
      81,
      767,
      480,
      809
    ],
    "page_idx": 2
  },
  {
    "type": "equation",
    "text": "\n$$\n\\mathcal {D} _ {\\text {t r a i n}} = \\bigcup_ {i = 1} ^ {N} \\alpha \\tilde {\\chi} _ {\\text {o l d}} ^ {c _ {i}} \\cup \\chi_ {\\text {n e w}}, \\tag {5}\n$$\n",
    "text_format": "latex",
    "bbox": [
      191,
      813,
      482,
      849
    ],
    "page_idx": 2
  },
  {
    "type": "text",
    "text": "where  $\\{c_1, c_2, \\ldots, c_N\\}$  denotes the  $N$  previously observed cities,  $\\tilde{\\mathcal{X}}_{\\mathrm{old}}^{c_i}$  denotes the set of pseudo-trajectories generated for city  $c_i$ , and  $\\mathcal{X}_{\\mathrm{new}}$  is the set of real trajectories from the current city. Coefficient",
    "bbox": [
      81,
      853,
      482,
      896
    ],
    "page_idx": 2
  },
  {
    "type": "text",
    "text": "$\\alpha > 0$  specifies ratio between the number of pseudo-trajectories in each previous city and the number of real trajectories in  $X_{\\mathrm{new}}$ .",
    "bbox": [
      513,
      107,
      911,
      133
    ],
    "page_idx": 2
  },
  {
    "type": "text",
    "text": "This allows the student model to learn current-city behaviors while retaining knowledge of previously learned cities.",
    "bbox": [
      513,
      133,
      911,
      162
    ],
    "page_idx": 2
  },
  {
    "type": "text",
    "text": "Distilling Knowledge to Preserve Mobility Patterns. To further strengthen the model's ability to preserve prior knowledge, we introduce a knowledge distillation loss that transfers behavioral patterns from the teacher model to the student model, as depicted in Figure 2(b). For each generated pseudo-trajectory  $\\tilde{x}_{\\mathrm{old}}$ , we extract the predicted mobility distributions from both models:",
    "bbox": [
      513,
      169,
      913,
      252
    ],
    "page_idx": 2
  },
  {
    "type": "equation",
    "text": "\n$$\nP _ {\\text {o l d}} (\\cdot \\mid \\tilde {x} _ {\\text {o l d}}) = f _ {\\text {o l d}} (\\tilde {x} _ {\\text {o l d}}), \\quad P _ {\\text {n e w}} (\\cdot \\mid \\tilde {x} _ {\\text {o l d}}) = f _ {\\text {n e w}} (\\tilde {x} _ {\\text {o l d}}). \\tag {6}\n$$\n",
    "text_format": "latex",
    "bbox": [
      550,
      258,
      913,
      273
    ],
    "page_idx": 2
  },
  {
    "type": "text",
    "text": "We minimize the Kullback-Leibler (KL) divergence between the teacher's and student's predicted distributions:",
    "bbox": [
      513,
      279,
      911,
      306
    ],
    "page_idx": 2
  },
  {
    "type": "equation",
    "text": "\n$$\n\\mathcal {L} _ {\\mathrm {K D}} = \\mathbb {E} _ {\\tilde {x} _ {\\mathrm {o l d}} \\sim f _ {\\mathrm {o l d}}} \\left[ \\operatorname {K L} \\left(P _ {\\mathrm {o l d}} (\\cdot \\mid \\tilde {x} _ {\\mathrm {o l d}}) \\| P _ {\\mathrm {n e w}} (\\cdot \\mid \\tilde {x} _ {\\mathrm {o l d}})\\right) \\right]. \\tag {7}\n$$\n",
    "text_format": "latex",
    "bbox": [
      557,
      311,
      913,
      329
    ],
    "page_idx": 2
  },
  {
    "type": "text",
    "text": "For the new data  $x_{\\mathrm{new}}$ , we compute the task loss as the cross-entropy between the model's predicted distribution and the true labels, referred to as  $\\mathcal{L}_{\\mathrm{cross - entropy}}$ . The total training objective of the student model is a weighted sum of the task loss on new-city data and the distillation loss:",
    "bbox": [
      513,
      333,
      913,
      401
    ],
    "page_idx": 2
  },
  {
    "type": "equation",
    "text": "\n$$\n\\mathcal {L} _ {\\text {t o t a l}} = \\mathcal {L} _ {\\text {c r o s s - e n t r o p y}} + \\lambda \\cdot \\mathcal {L} _ {\\mathrm {K D}}, \\tag {8}\n$$\n",
    "text_format": "latex",
    "bbox": [
      617,
      407,
      913,
      422
    ],
    "page_idx": 2
  },
  {
    "type": "text",
    "text": "where  $\\lambda$  is a hyperparameter that balances learning new knowledge and retaining previously acquired behaviors.",
    "bbox": [
      513,
      428,
      911,
      455
    ],
    "page_idx": 2
  },
  {
    "type": "text",
    "text": "4.2 Model Architecture",
    "text_level": 1,
    "bbox": [
      514,
      468,
      718,
      481
    ],
    "page_idx": 2
  },
  {
    "type": "text",
    "text": "To enable scalable and adaptive learning across heterogeneous urban environments, our model is designed with a modular architecture that integrates flexible location encoders/decoders, a Mixture-of-Experts (MoE) Transformer backbone, and a mobility-aware expert routing mechanism, as shown in Figure 2(c). This design ensures the model's scalability and adaptability in multi-city continual learning scenarios.",
    "bbox": [
      511,
      486,
      913,
      583
    ],
    "page_idx": 2
  },
  {
    "type": "text",
    "text": "Unified Location Encoder. Conventional location representations often rely on discrete location IDs, which are inherently city-specific and hinder cross-city generalization. To overcome this limitation, we design a continuous location representation that embeds each location into a shared latent space, capturing transferable semantic and spatial properties across cities. This unified representation facilitates knowledge sharing and supports incremental learning across heterogeneous urban environments. Concretely, each location  $l \\in \\mathcal{L}$  is represented by a feature vector  $\\mathbf{z}_l \\in \\mathbb{R}^d$  constructed from three key components:  $\\mathbf{z}_l = \\phi_{\\mathrm{POI}}(l) \\oplus \\phi_{\\mathrm{lat - lon}}(l) \\oplus \\phi_{\\mathrm{hot}}(l)$ , where  $\\phi_{\\mathrm{POI}}(l) \\in \\mathbb{R}^{d_1}$  denotes the Point-of-Interest (POI) embedding, capturing semantic land-use attributes such as residential, commercial, educational, or recreational functions, often indicative of mobility intent and purpose;  $\\phi_{\\mathrm{hot}}(l) \\in \\mathbb{R}^{d_2}$  is the mobility heat embedding, derived from public available OD flows at each location, which reflects the functional centrality of that location;  $\\phi_{\\mathrm{lat - lon}}(l) \\in \\mathbb{R}^{d_3}$  is the normalized latitude-longitude embedding, representing the relative spatial position of the location within the city boundary.",
    "bbox": [
      511,
      590,
      913,
      840
    ],
    "page_idx": 2
  },
  {
    "type": "text",
    "text": "The overall location representation  $\\mathbf{z}_l$  is obtained via concatenation  $(\\oplus)$  of these features, and is further transformed by a shared multi-layer perceptron (MLP). This design is sufficient and generalizable because it captures three complementary views of spatial",
    "bbox": [
      511,
      840,
      913,
      896
    ],
    "page_idx": 2
  },
  {
    "type": "header",
    "text": "MoveGCL",
    "bbox": [
      84,
      75,
      135,
      85
    ],
    "page_idx": 2
  },
  {
    "type": "header",
    "text": "Conference'17, July 2017, Washington, DC, USA",
    "bbox": [
      684,
      75,
      913,
      87
    ],
    "page_idx": 2
  },
  {
    "type": "image",
    "img_path": "images/8c1a6f490b2342c4b929f61b26d85756ba31533e0aaf72a773ffc84ea754549e.jpg",
    "image_caption": [
      "Figure 2: Overview of the MoveGCL framework: (a) the overall workflow; (b) the implementation of generative continual learning; (c) the model architecture."
    ],
    "image_footnote": [],
    "bbox": [
      89,
      104,
      906,
      438
    ],
    "page_idx": 3
  },
  {
    "type": "text",
    "text": "semantics: (1) semantic functions via POI types, (2) actual mobility signals via visitation popularity, and (3) captures where the location sits in the urban layout. Together, they provide a compact yet expressive embedding that generalizes well across different cities with varied spatial structures.",
    "bbox": [
      81,
      502,
      483,
      571
    ],
    "page_idx": 3
  },
  {
    "type": "text",
    "text": "Mixture-of-Experts Transformer. The Mixture-of-Experts (MoE) architecture comprises a router network and multiple expert networks, serving as a replacement for the Feed-Forward Network (FFN) within the Transformer [33]. The output of the MoE layer,  $F_{\\mathrm{MoE}}(x)$ , is the weighted sum of the selected expert outputs, where the weights are given by the router network's output:",
    "bbox": [
      81,
      583,
      493,
      667
    ],
    "page_idx": 3
  },
  {
    "type": "equation",
    "text": "\n$$\nF _ {\\mathrm {M o E}} (x) = \\sum_ {i = 1} ^ {k} R _ {i} (x) \\cdot E _ {i} (x). \\tag {9}\n$$\n",
    "text_format": "latex",
    "bbox": [
      189,
      680,
      482,
      718
    ],
    "page_idx": 3
  },
  {
    "type": "text",
    "text": "Here,  $x$  denotes the input to the MoE module,  $k$  is the number of selected experts,  $R_{i}(x)$  is the output of the router network for expert  $i$  (detailed in Section 4.2),  $E_{i}(x)$  is the output of expert  $i$ .",
    "bbox": [
      81,
      729,
      482,
      771
    ],
    "page_idx": 3
  },
  {
    "type": "text",
    "text": "MoveGCL is built upon Mixture-of-Experts (MoE) Transformer blocks, in which each expert module is responsible for capturing specific mobility patterns. During continual learning, we introduce new experts to accommodate knowledge from new cities, and design layer-wise progressive adaptation training strategy (detail in Section 4.3). This partial parameter update strategy injects new knowledge without overwriting existing capabilities, thus alleviating catastrophic forgetting. The modularity of the MoE block also supports elastic model expansion as more cities are introduced.",
    "bbox": [
      81,
      771,
      482,
      896
    ],
    "page_idx": 3
  },
  {
    "type": "text",
    "text": "Mobility-Aware Expert Routing. For each input trajectory  $x$ , we extract a set of mobility behavior descriptive features and encode them into a mobility feature descriptor vector  $\\mathbf{z}_m \\in \\mathbb{R}^d$ . This feature set comprises: the jump distance  $d_{\\mathrm{jump}}$  (distance between the current point and the previous point in the trajectory); the waiting time  $t_{\\mathrm{wait}}$  (time difference between arrivals at the current and previous points in the trajectory); the quantized radius of gyration  $r_{\\mathrm{gyr}}$ ; the quantized location entropy  $H_{\\mathrm{loc}}$ ; and the city identifier  $\\mathrm{ID}_{\\mathrm{city}}$ . These features are embedded via their respective encoders, where  $d_{\\mathrm{jump}}$  and  $t_{\\mathrm{wait}}$  are processed by a Transformer-based continuous feature encoder, and  $r_{\\mathrm{gyr}}, H_{\\mathrm{loc}}$ , and  $\\mathrm{ID}_{\\mathrm{city}}$  are handled by discrete embedding modules. Finally, the five feature embeddings are concatenated to form the mobility behavior vector:",
    "bbox": [
      511,
      502,
      913,
      681
    ],
    "page_idx": 3
  },
  {
    "type": "equation",
    "text": "\n$$\n\\mathbf {z} _ {m} = \\left[ \\begin{array}{c} \\phi_ {d _ {\\text {j u m p}}} \\left(d _ {\\text {j u m p}} (x)\\right), \\phi_ {t _ {\\text {w a i t}}} \\left(t _ {\\text {w a i t}} (x)\\right), \\phi_ {r _ {\\text {g y r}}} \\left(r _ {\\text {g y r}} (x)\\right), \\\\ \\phi_ {H _ {\\text {l o c}}} \\left(H _ {\\text {l o c}} (x)\\right), \\phi_ {\\mathrm {I D} _ {\\text {c i t y}}} \\left(\\mathrm {I D} _ {\\text {c i t y}}\\right) \\end{array} \\right]. \\tag {10}\n$$\n",
    "text_format": "latex",
    "bbox": [
      529,
      685,
      911,
      724
    ],
    "page_idx": 3
  },
  {
    "type": "text",
    "text": "Here,  $\\phi_{d_{\\mathrm{jump}}}$  and  $\\phi_{t_{\\mathrm{wait}}}$  denote the Transformer encoders for continuous mobility features, while  $\\phi_{r_{\\mathrm{gyr}}}, \\phi_{H_{\\mathrm{loc}}},$  and  $\\phi_{\\mathrm{ID}_{\\mathrm{city}}}$  represent the embedding encoders for discrete features.",
    "bbox": [
      513,
      727,
      913,
      768
    ],
    "page_idx": 3
  },
  {
    "type": "text",
    "text": "Within each layer of the MoE blocks, we introduce a routing network based on a linear transformation to compute the routing weights for each expert, leveraging both the mobility descriptor feature vector  $\\mathbf{z}_m$  and the output of the self-attention submodule at that layer. The routing weights for layer  $i$  are computed as:",
    "bbox": [
      513,
      768,
      911,
      839
    ],
    "page_idx": 3
  },
  {
    "type": "equation",
    "text": "\n$$\nR _ {i} (x) = \\operatorname {s o f t m a x} \\left(\\operatorname {T o p K} \\left(W _ {i, r} \\left(\\mathbf {z} _ {m} \\oplus X _ {i} (x)\\right) + b _ {i}\\right)\\right), \\tag {11}\n$$\n",
    "text_format": "latex",
    "bbox": [
      562,
      843,
      911,
      864
    ],
    "page_idx": 3
  },
  {
    "type": "text",
    "text": "where  $\\mathbf{z}_m$  is the mobility feature descriptor vector defined above;  $X_{i}(x)$  denotes the output of the self-attention submodule;  $W_{i,r}$  is",
    "bbox": [
      513,
      867,
      913,
      896
    ],
    "page_idx": 3
  },
  {
    "type": "header",
    "text": "Conference'17, July 2017, Washington, DC, USA",
    "bbox": [
      84,
      75,
      313,
      85
    ],
    "page_idx": 3
  },
  {
    "type": "header",
    "text": "Yuan et al.",
    "bbox": [
      859,
      75,
      911,
      85
    ],
    "page_idx": 3
  },
  {
    "type": "image",
    "img_path": "images/c98980fae8bfd70c2936501179fc745ee1fd57690d3ba6dc0d49a0d02b9ac59b.jpg",
    "image_caption": [
      "Figure 3: Illustration of the layer-wise progressive adaptation"
    ],
    "image_footnote": [],
    "bbox": [
      135,
      118,
      857,
      406
    ],
    "page_idx": 4
  },
  {
    "type": "text",
    "text": "a learnable projection matrice;  $b_{i}$  is the bias term; and  $\\mathrm{TopK}(\\cdot)$  retains only the top  $K$  values (corresponding to the  $K$  highest-scoring experts), setting the remaining expert scores to  $-\\infty$  so that their weights after the softmax operation are effectively zero.",
    "bbox": [
      81,
      455,
      482,
      513
    ],
    "page_idx": 4
  },
  {
    "type": "text",
    "text": "This routing strategy achieves two goals: (1) it promotes functional specialization by directing similar motion patterns to consistent expert subsets; (2) it enables the model to discover and transfer shared mobility structures across cities, thus enhancing generalization in multi-city scenarios. Additionally, this mobility-aware routing provides a structured inductive bias that accelerates model adaptation during incremental learning, allowing new experts to specialize rapidly with minimal interference to retained knowledge.",
    "bbox": [
      81,
      513,
      482,
      625
    ],
    "page_idx": 4
  },
  {
    "type": "text",
    "text": "Similarity-Based Decoder. The next-location prediction is performed by computing the similarity between the final output of the Mixture-of-Experts (MoE) Transformer blocks and the representation vectors of all candidate locations in the city. These location representations are generated using a Deep & Cross Network (DCN) [43], which consists of both a Cross layer and a Deep layer to capture feature interactions and nonlinear transformations.",
    "bbox": [
      81,
      633,
      482,
      729
    ],
    "page_idx": 4
  },
  {
    "type": "text",
    "text": "The Cross network captures inter-location correlations within a city by applying element-wise interactions over location embeddings  $E_{l}$ , producing:",
    "bbox": [
      81,
      729,
      482,
      772
    ],
    "page_idx": 4
  },
  {
    "type": "equation",
    "text": "\n$$\nE _ {\\text {c r o s s}} = \\sum_ {i = 1} ^ {d} E _ {i} \\odot W _ {i} E _ {l} + b _ {i}, \\tag {12}\n$$\n",
    "text_format": "latex",
    "bbox": [
      200,
      782,
      482,
      818
    ],
    "page_idx": 4
  },
  {
    "type": "text",
    "text": "where  $\\odot$  denotes element-wise multiplication, and  $W_{i}, b_{i}$  are learnable parameters. The Deep layer refines the same  $E_{l}$  using a two-layer MLP:",
    "bbox": [
      81,
      828,
      483,
      869
    ],
    "page_idx": 4
  },
  {
    "type": "equation",
    "text": "\n$$\nE _ {\\text {d e e p}} = \\operatorname {G E L U} \\left(\\left(W _ {1} E _ {l} + b _ {1}\\right) W _ {2} + b _ {2}\\right), \\tag {13}\n$$\n",
    "text_format": "latex",
    "bbox": [
      169,
      882,
      482,
      897
    ],
    "page_idx": 4
  },
  {
    "type": "text",
    "text": "with weights  $W_{1}, W_{2}$ , biases  $b_{1}, b_{2}$ , and GELU activation. The DCN output is the concatenation of both branches:",
    "bbox": [
      513,
      455,
      913,
      484
    ],
    "page_idx": 4
  },
  {
    "type": "equation",
    "text": "\n$$\nE _ {\\mathrm {D C N}} = E _ {\\text {c r o s s}} \\oplus E _ {\\text {d e e p}}. \\tag {14}\n$$\n",
    "text_format": "latex",
    "bbox": [
      645,
      491,
      913,
      505
    ],
    "page_idx": 4
  },
  {
    "type": "text",
    "text": "For next-location prediction, the user's historical trajectory is encoded by MoE transformer blocks to produce a prediction vector  $P$ ; separately, the Deep & Cross Network (DCN) encodes every candidate location in the city to produce location representations  $E_{\\mathrm{DCN}}$ . The similarity score is computed as:",
    "bbox": [
      513,
      510,
      913,
      580
    ],
    "page_idx": 4
  },
  {
    "type": "equation",
    "text": "\n$$\n\\text {S c o r e} _ {\\text {s i m i l a r i t y}} = \\sum_ {i = 1} ^ {d} P \\cdot E _ {\\mathrm {D C N}, i}, \\tag {15}\n$$\n",
    "text_format": "latex",
    "bbox": [
      622,
      590,
      913,
      627
    ],
    "page_idx": 4
  },
  {
    "type": "text",
    "text": "where  $d$  is the number of locations. Higher similarity scores indicate higher probabilities of being the next location. This similarity-based decoding strategy ensures scalability, as it decouples the prediction process from a fixed output space. Instead of classifying over a static set of locations, the model performs representation-level matching, allowing it to generalize across cities with different spatial layouts and dynamically varying numbers of candidate locations.",
    "bbox": [
      511,
      628,
      913,
      726
    ],
    "page_idx": 4
  },
  {
    "type": "text",
    "text": "4.3 Layer-Wise Progressive Adaptation",
    "text_level": 1,
    "bbox": [
      513,
      737,
      844,
      753
    ],
    "page_idx": 4
  },
  {
    "type": "text",
    "text": "To ensure a balance between previously and newly learned knowledge, MoveGCL employs a layer-wise progressive adaptation strategy, where model parameters are updated in stages, as illustrated in Figure 3. For a model composed of  $N$  layers of MoE transformer blocks, the total number of training epochs  $E$  is evenly divided into  $N/2$  stages, with each stage lasting  $\\frac{E}{N/2}$  epochs. At each stage, a pair of symmetrically positioned MoE transformer blocks—one near the input side and the other near the output side—are unfrozen for fine-tuning, while the remaining layers remain frozen. The specific process is as follows:",
    "bbox": [
      511,
      756,
      913,
      896
    ],
    "page_idx": 4
  },
  {
    "type": "header",
    "text": "MoveGCL",
    "bbox": [
      84,
      75,
      135,
      85
    ],
    "page_idx": 4
  },
  {
    "type": "header",
    "text": "Conference'17, July 2017, Washington, DC, USA",
    "bbox": [
      684,
      75,
      913,
      87
    ],
    "page_idx": 4
  },
  {
    "type": "table",
    "img_path": "images/18f443fb3a33d1eb07dea6393463260e5cb41dfd485eb45d1a31ec0136835a94.jpg",
    "table_caption": [
      "Table 1: Performance comparison between MoveGCL and baseline methods across different cities and Acc@k metrics."
    ],
    "table_footnote": [],
    "table_body": "<table><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">Atlanta</td><td colspan=\"2\">Chicago</td><td colspan=\"2\">Los Angeles</td><td colspan=\"2\">New York</td><td colspan=\"2\">Seattle</td><td colspan=\"2\">Washington D.C</td></tr><tr><td>Acc@1</td><td>Acc@3</td><td>Acc@1</td><td>Acc@3</td><td>Acc@1</td><td>Acc@3</td><td>Acc@1</td><td>Acc@3</td><td>Acc@1</td><td>Acc@3</td><td>Acc@1</td><td>Acc@3</td></tr><tr><td>Markov</td><td>0.183</td><td>0.325</td><td>0.146</td><td>0.260</td><td>0.103</td><td>0.201</td><td>0.115</td><td>0.275</td><td>0.202</td><td>0.318</td><td>0.162</td><td>0.347</td></tr><tr><td>LSTM</td><td>0.231</td><td>0.373</td><td>0.194</td><td>0.334</td><td>0.131</td><td>0.275</td><td>0.169</td><td>0.312</td><td>0.259</td><td>0.420</td><td>0224</td><td>0.383</td></tr><tr><td>Transformer</td><td>0.210</td><td>0.353</td><td>0.175</td><td>0.300</td><td>0.124</td><td>0.268</td><td>0.156</td><td>0.318</td><td>0.235</td><td>0.397</td><td>0.192</td><td>0.356</td></tr><tr><td>DeepMove</td><td>0.242</td><td>0.393</td><td>0.203</td><td>0.344</td><td>0.147</td><td>0.274</td><td>0.177</td><td>0.329</td><td>0.278</td><td>0.444</td><td>0.247</td><td>0.408</td></tr><tr><td>TrajBert</td><td>0.214</td><td>0.370</td><td>0.183</td><td>0.316</td><td>0.146</td><td>0.277</td><td>0.159</td><td>0.310</td><td>0.234</td><td>0.402</td><td>0.207</td><td>0.367</td></tr><tr><td>CLET</td><td>0.263</td><td>0.422</td><td>0.200</td><td>0.341</td><td>0.138</td><td>0.275</td><td>0.156</td><td>0.313</td><td>0.289</td><td>0.454</td><td>0.232</td><td>0.390</td></tr><tr><td>PMF</td><td>0.248</td><td>0.381</td><td>0.151</td><td>0.249</td><td>0.112</td><td>0.190</td><td>0.150</td><td>0.257</td><td>0.269</td><td>0.418</td><td>0.217</td><td>0.349</td></tr><tr><td>LightTR</td><td>0.269</td><td>0.402</td><td>0.168</td><td>0.269</td><td>0.130</td><td>0.216</td><td>0.168</td><td>0.281</td><td>0.296</td><td>0.444</td><td>0.242</td><td>0.376</td></tr><tr><td>MoveGCL (FullTune)</td><td>0.188</td><td>0.304</td><td>0.125</td><td>0.206</td><td>0.064</td><td>0.114</td><td>0.208</td><td>0.329</td><td>0.199</td><td>0.318</td><td>0.147</td><td>0.257</td></tr><tr><td>MoveGCL (ExpertTune)</td><td>0.192</td><td>0.310</td><td>0.132</td><td>0.215</td><td>0.062</td><td>0.108</td><td>0.207</td><td>0.327</td><td>0.195</td><td>0.322</td><td>0.147</td><td>0.259</td></tr><tr><td>MoveGCL (WSC→A→L→N)</td><td>0.282</td><td>0.421</td><td>0.197</td><td>0.306</td><td>0.157</td><td>0.254</td><td>0.206</td><td>0.328</td><td>0.324</td><td>0.478</td><td>0.273</td><td>0.413</td></tr></table>",
    "bbox": [
      88,
      119,
      910,
      361
    ],
    "page_idx": 5
  },
  {
    "type": "list",
    "sub_type": "text",
    "list_items": [
      "- In Stage 1, the outermost layers (closest to the input and output) are unfrozen.",
      "- In Stage 2, the second closest layers to the input and output are unfrozen.",
      ".",
      "- In Stage N/2, the two central layers of the model are unfrozen."
    ],
    "bbox": [
      83,
      382,
      478,
      464
    ],
    "page_idx": 5
  },
  {
    "type": "text",
    "text": "Within each stage, only a subset of parameters in the unfrozen layers is updated—the routing modules, newly added experts and previously trained experts that were not frequently activated in the prior generative continual learning phase. To facilitate adaptation to the mobility features across different datasets, parameters of the mobility feature encoder are updated continuously during all stages. Furthermore, to prevent large parameter shifts during the initial stage of parameter updating, all previously trained experts in the input-side MoE transformer layer are kept frozen during Stage 1.",
    "bbox": [
      81,
      472,
      482,
      598
    ],
    "page_idx": 5
  },
  {
    "type": "text",
    "text": "5 Results",
    "text_level": 1,
    "bbox": [
      83,
      614,
      174,
      628
    ],
    "page_idx": 5
  },
  {
    "type": "text",
    "text": "5.1 Experimental Settings",
    "text_level": 1,
    "bbox": [
      83,
      633,
      307,
      648
    ],
    "page_idx": 5
  },
  {
    "type": "text",
    "text": "Datasets. We utilize human mobility datasets from multiple cities to evaluate the performance of MoveGCL. Specifically, the datasets cover over eight hundred thousand users and feature a relatively high sampling rate compared to currently available public datasets. Detailed statistics and descriptions are provided in Appendix Table 3. For each city, we randomly sample 120,000 trajectories for training, 40,000 for validation, and 40,000 for testing.",
    "bbox": [
      81,
      652,
      482,
      748
    ],
    "page_idx": 5
  },
  {
    "type": "text",
    "text": "Baselines and Parameter Settings. We compare our method with a diverse set of baselines, including traditional mobility models, federated learning-based approaches, and joint learning models with privacy-preserving mechanisms. Appendix A.2 provides detailed descriptions of the baselines, and Appendix A.3 summarizes the parameter settings.",
    "bbox": [
      81,
      750,
      482,
      833
    ],
    "page_idx": 5
  },
  {
    "type": "text",
    "text": "5.2 Overall Performance",
    "text_level": 1,
    "bbox": [
      83,
      849,
      297,
      863
    ],
    "page_idx": 5
  },
  {
    "type": "text",
    "text": "Table 1 presents the performance of MoveGCL compared to state-of-the-art baseline methods.",
    "bbox": [
      81,
      867,
      482,
      895
    ],
    "page_idx": 5
  },
  {
    "type": "list",
    "sub_type": "text",
    "list_items": [
      "- MoveGCL consistently outperforms traditional deep learning models trained independently on each dataset, demonstrating strong cross-city scalability. On average, it achieves an  $8\\%$  improvement in Acc@1, highlighting its ability to generalize across diverse urban environments. This result validates the promise of mobility foundation models, which unify knowledge across cities and reduce redundancy. In contrast, training separate models for each city not only increases computational and deployment costs, but also fails to leverage shared mobility patterns across domains.",
      "- MoveGCL surpasses privacy-preserving federated learning approaches in both accuracy and stability. Compared to domain-specific baselines such as PVM [12] and LightTR [31], MoveGCL achieves significantly higher performance. This advantage stems from its unified generative continual learning framework, which maintains global generalization without suffering from the synchronization overhead and convergence instability inherent in federated setups. This further supports its practicality in real-world multi-party mobility modeling scenarios.",
      "- MoveGCL effectively balances adaptation to new data while retaining knowledge from previously seen data. We benchmark against two continual learning strategies—FullTune and ExpertTune—which either fine-tune or expand the model on new datasets. While these methods partially adapt to new cities, they suffer from severe performance degradation on previously seen data, indicating catastrophic forgetting. In contrast, MoveGCL preserves prior knowledge and achieves higher performance on both old and new datasets, demonstrating its ability to support continuous model evolution without sacrificing stability."
    ],
    "bbox": [
      514,
      382,
      915,
      784
    ],
    "page_idx": 5
  },
  {
    "type": "text",
    "text": "5.3 Order Invariance in Continual Learning",
    "text_level": 1,
    "bbox": [
      514,
      808,
      883,
      824
    ],
    "page_idx": 5
  },
  {
    "type": "text",
    "text": "To assess the robustness of MoveGCL in real-world deployment scenarios, we investigate its sensitivity to the order in which data from different cities is introduced during continual learning. As shown in Appendix Table 4, the performance of MoveGCL remains remarkably consistent, with the vast majority of metrics showing",
    "bbox": [
      511,
      825,
      913,
      897
    ],
    "page_idx": 5
  },
  {
    "type": "header",
    "text": "Conference'17, July 2017, Washington, DC, USA",
    "bbox": [
      84,
      75,
      313,
      87
    ],
    "page_idx": 5
  },
  {
    "type": "header",
    "text": "Yuan et al.",
    "bbox": [
      859,
      75,
      911,
      85
    ],
    "page_idx": 5
  },
  {
    "type": "image",
    "img_path": "images/20edb1d1738504d0d69b9447bc1728f1d647688203f2a3ce189d58367e5532dc.jpg",
    "image_caption": [
      "Figure 4: Similarity score distribution in uniqueness testing."
    ],
    "image_footnote": [],
    "bbox": [
      84,
      95,
      478,
      223
    ],
    "page_idx": 6
  },
  {
    "type": "image",
    "img_path": "images/90f7b3c93b1fab7a3c2949f0117456de3461da4234c2cbaf1768c91a80e71ded.jpg",
    "image_caption": [
      "Figure 5: Success rate in membership inference attack"
    ],
    "image_footnote": [],
    "bbox": [
      107,
      266,
      455,
      424
    ],
    "page_idx": 6
  },
  {
    "type": "text",
    "text": "deviations of less than  $5\\%$  across original and reversed sequences. This empirical finding confirms the order-invariant learning behavior of our model, demonstrating that MoveGCL can integrate new city data without disrupting previously acquired knowledge, even when the order of exposure varies significantly. We refer the reader to the detailed analysis in Appendix C.1.",
    "bbox": [
      81,
      465,
      482,
      550
    ],
    "page_idx": 6
  },
  {
    "type": "text",
    "text": "5.4 Privacy Evaluation",
    "text_level": 1,
    "bbox": [
      83,
      563,
      282,
      579
    ],
    "page_idx": 6
  },
  {
    "type": "text",
    "text": "As MoveGCL is built on a generative continual learning framework that does not retain raw data from previous cities, a key question is whether the synthetic data used for replay may inadvertently leak private information from the original training data. To rigorously evaluate the privacy-preserving properties of our approach, following the methodology in [53, 54], we conduct a comprehensive analysis from three complementary perspectives:",
    "bbox": [
      81,
      580,
      482,
      679
    ],
    "page_idx": 6
  },
  {
    "type": "list",
    "sub_type": "text",
    "list_items": [
      "- Uniqueness Testing [8, 45]: To evaluate the degree of similarity between the generated data and the real data.",
      "- Membership Inference Attack [28, 39]: Given a trained model and a set of samples, it assesses whether an classifier can accurately determine which samples were included in the model's training set based on the model's outputs.",
      "- Differential Privacy [1, 2]: To ensure that the model does not depend on a small subset of training examples, we remove a minimal set of training samples and evaluate whether the distribution of model outputs undergoes an obvious change."
    ],
    "bbox": [
      83,
      683,
      482,
      821
    ],
    "page_idx": 6
  },
  {
    "type": "text",
    "text": "Due to space limitations, we summarize the key conclusions here, and provide detailed experimental settings for the privacy evaluation in Appendix C.2.",
    "bbox": [
      81,
      825,
      482,
      867
    ],
    "page_idx": 6
  },
  {
    "type": "text",
    "text": "Uniqueness Testing. Figure 4 presents the cumulative distribution of similarity scores. As shown in the figure, over  $95\\%$  of the",
    "bbox": [
      83,
      867,
      482,
      896
    ],
    "page_idx": 6
  },
  {
    "type": "table",
    "img_path": "images/0c833c800aff4c8bccf2898694ff1d51976dedbb2d62483de915d44d799eb938.jpg",
    "table_caption": [
      "Table 2: Differential Privacy statistics by city."
    ],
    "table_footnote": [],
    "table_body": "<table><tr><td>€</td><td>Mean</td><td>Median</td><td>75th Percentile</td></tr><tr><td>Atlanta</td><td>2.671</td><td>0.706</td><td>2.212</td></tr><tr><td>Chicago</td><td>2.919</td><td>0.752</td><td>2.504</td></tr><tr><td>Los Angeles</td><td>2.988</td><td>0.593</td><td>2.572</td></tr><tr><td>New York</td><td>3.394</td><td>0.713</td><td>2.001</td></tr><tr><td>Seattle</td><td>2.934</td><td>0.655</td><td>1.870</td></tr><tr><td>Washington D.C</td><td>3.037</td><td>0.600</td><td>1.787</td></tr></table>",
    "bbox": [
      542,
      121,
      880,
      222
    ],
    "page_idx": 6
  },
  {
    "type": "text",
    "text": "generated trajectories do not have any corresponding real trajectory with a similarity score higher than  $50\\%$ . This indicates that the model's outputs are based on the knowledge it has acquired rather than directly copying trajectories from the training set.",
    "bbox": [
      511,
      239,
      913,
      295
    ],
    "page_idx": 6
  },
  {
    "type": "text",
    "text": "Membership Inference Attack. Figure 5 shows the attack results. As observed, the success rates across all datasets are approximately  $50\\%$ , indicating that the classifier can hardly determine whether a trajectory was part of the training data or not based on the generated sample. These results indicate that our model is not easily susceptible to membership inference attacks.",
    "bbox": [
      513,
      295,
      913,
      378
    ],
    "page_idx": 6
  },
  {
    "type": "text",
    "text": "Differential Privacy. As shown in Table 2, without applying any additional privacy-preserving mechanisms, MoveGCL naturally achieves a privacy budget of  $\\varepsilon \\approx 1 - 2$  for  $75\\%$  of randomly sampled trajectories. This level is generally considered an acceptable operating point for generative models [28]; for example, Apple adopts a privacy budget of  $\\varepsilon = 4.0^{*}$ .",
    "bbox": [
      513,
      378,
      913,
      460
    ],
    "page_idx": 6
  },
  {
    "type": "text",
    "text": "5.5 Ablation Studies",
    "text_level": 1,
    "bbox": [
      514,
      474,
      692,
      488
    ],
    "page_idx": 6
  },
  {
    "type": "text",
    "text": "In this section, we conduct two sets of incremental ablation studies based on MoveGCL (WSC  $\\rightarrow$  A  $\\rightarrow$  L  $\\rightarrow$  N). The first set focuses on the input features of the Mobility-Aware Expert Routing module. We selectively remove or adjust different dimensions of the location feature to evaluate the contribution and necessity of each type of input in guiding expert routing. The second set targets the incremental learning mechanism itself. We remove the knowledge distillation strategy designed to mitigate catastrophic forgetting in GCL, and instead train the model using only the conventional cross-entropy loss. This setup allows us to assess the effectiveness of knowledge distillation in preserving previously learned knowledge.",
    "bbox": [
      511,
      492,
      913,
      645
    ],
    "page_idx": 6
  },
  {
    "type": "text",
    "text": "As shown in Appendix Figure 7, removing any input feature from the Mobility-Aware Expert Routing module leads to a noticeable performance drop. Similarly, disabling the knowledge distillation strategy in GCL also results in a significant decline in model performance. These findings highlight the critical role of each input feature in expert selection, as well as the importance of knowledge distillation in ensuring model stability during continual learning.",
    "bbox": [
      511,
      645,
      913,
      742
    ],
    "page_idx": 6
  },
  {
    "type": "text",
    "text": "5.6 In-Depth Analysis",
    "text_level": 1,
    "bbox": [
      514,
      753,
      709,
      770
    ],
    "page_idx": 6
  },
  {
    "type": "text",
    "text": "To better understand why MoveGCL is capable of unifying diverse mobility datasets and effectively handling substantial inter-city heterogeneity, we conduct an in-depth analysis of the location embedding layer to examine whether MoveGCL can learn shared spatial representations across cities. To this end, we extract the location embeddings for each city at two stages: (1) after the initial encoder, and (2) after the Deep & Cross Network (DCN). By comparing these",
    "bbox": [
      511,
      773,
      913,
      869
    ],
    "page_idx": 6
  },
  {
    "type": "header",
    "text": "MoveGCL",
    "bbox": [
      84,
      75,
      135,
      85
    ],
    "page_idx": 6
  },
  {
    "type": "header",
    "text": "Conference'17, July 2017, Washington, DC, USA",
    "bbox": [
      684,
      75,
      911,
      87
    ],
    "page_idx": 6
  },
  {
    "type": "page_footnote",
    "text": "*https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf",
    "bbox": [
      514,
      883,
      862,
      896
    ],
    "page_idx": 6
  },
  {
    "type": "image",
    "img_path": "images/80b90c7255deecda670ba21420cae098d5ae081bcb63bf2a096d62ac24b7fc90.jpg",
    "image_caption": [
      "ATL original embeddings"
    ],
    "image_footnote": [],
    "bbox": [
      127,
      107,
      236,
      189
    ],
    "page_idx": 7
  },
  {
    "type": "image",
    "img_path": "images/4a7021603530e1d4cdee01a4bf6158488c0671c6a0af595718c6098cde8f2063.jpg",
    "image_caption": [
      "CHI original embeddings"
    ],
    "image_footnote": [],
    "bbox": [
      250,
      104,
      359,
      193
    ],
    "page_idx": 7
  },
  {
    "type": "image",
    "img_path": "images/eb7e290ac1a67d9ea02bcceebe28b09dc5a2fde083c7c4d20704c957272c53d8.jpg",
    "image_caption": [
      "LA original embeddings"
    ],
    "image_footnote": [],
    "bbox": [
      372,
      104,
      485,
      193
    ],
    "page_idx": 7
  },
  {
    "type": "image",
    "img_path": "images/1c38ecf8f2be6b5cb3c519835e6fd8216694f0e020ee27a2f75bd256b7e73998.jpg",
    "image_caption": [
      "NYC original embeddings"
    ],
    "image_footnote": [],
    "bbox": [
      500,
      104,
      611,
      193
    ],
    "page_idx": 7
  },
  {
    "type": "image",
    "img_path": "images/54a8899a290ac449d0ccf10b1894c16f161eb69704a49cecd28a835edcff3c46.jpg",
    "image_caption": [
      "SEA original embeddings"
    ],
    "image_footnote": [],
    "bbox": [
      633,
      109,
      741,
      191
    ],
    "page_idx": 7
  },
  {
    "type": "image",
    "img_path": "images/a7c8d5e0d8b80a54f4217d7313b9750aee750dcbe437a80bce34b6b777558c1b.jpg",
    "image_caption": [
      "DC original embeddings"
    ],
    "image_footnote": [],
    "bbox": [
      756,
      109,
      870,
      193
    ],
    "page_idx": 7
  },
  {
    "type": "image",
    "img_path": "images/0d68eff83fea15c606e5f7515206cd4fcfac9b5f3aeaaf06265ea0db1a5d3e06.jpg",
    "image_caption": [
      "ATL final embeddings"
    ],
    "image_footnote": [],
    "bbox": [
      125,
      220,
      235,
      301
    ],
    "page_idx": 7
  },
  {
    "type": "image",
    "img_path": "images/a2d141dd8ec9e5405c212153508aaffe431d260ddbf1d70754adc18c67ecf19e.jpg",
    "image_caption": [
      "CHI final embeddings"
    ],
    "image_footnote": [],
    "bbox": [
      250,
      228,
      357,
      303
    ],
    "page_idx": 7
  },
  {
    "type": "image",
    "img_path": "images/36b57f00d0c5517f21e1ca5d5cf85268a9fc8c44f1b6f2ca42ace599418290dd.jpg",
    "image_caption": [
      "LA final embeddings"
    ],
    "image_footnote": [],
    "bbox": [
      372,
      215,
      483,
      301
    ],
    "page_idx": 7
  },
  {
    "type": "image",
    "img_path": "images/9848dca59e787f6190fc6cc0e4447e3dce0d1d768fda9e2432257f01116176d5.jpg",
    "image_caption": [
      "NYC final embeddings"
    ],
    "image_footnote": [],
    "bbox": [
      506,
      222,
      609,
      304
    ],
    "page_idx": 7
  },
  {
    "type": "image",
    "img_path": "images/df8bc56d7cf6120da74299cce9f84c8de758a02827aef53943134ce500f7eecb.jpg",
    "image_caption": [
      "SEA final embeddings"
    ],
    "image_footnote": [],
    "bbox": [
      625,
      224,
      736,
      292
    ],
    "page_idx": 7
  },
  {
    "type": "image",
    "img_path": "images/16d15bfc21de886c7252628df3de2d0ef81e652fc91cb07b5fb42a9a87221eef.jpg",
    "image_caption": [
      "DC final embeddings",
      "Figure 6: City location embeddings before (original) and after (final) DCN."
    ],
    "image_footnote": [],
    "bbox": [
      756,
      220,
      869,
      303
    ],
    "page_idx": 7
  },
  {
    "type": "text",
    "text": "two sets of embeddings, we evaluate the role of DCN in aligning spatial semantics across heterogeneous urban environments.",
    "bbox": [
      81,
      377,
      480,
      402
    ],
    "page_idx": 7
  },
  {
    "type": "text",
    "text": "As shown in Figure 6, DCN aligns location embedding distributions in different cities much more closely than the original encoder output. This indicates that DCN successfully captures shared location-feature patterns across urban areas, thereby boosting the model's ability to generalize in cross-city settings. Moreover, within each city, the DCN-processed embeddings become less densely clustered than their original counterparts, indicating a marked increase in separability among individual locations and further enhancing the model's capacity to encode location semantics.",
    "bbox": [
      81,
      404,
      482,
      527
    ],
    "page_idx": 7
  },
  {
    "type": "text",
    "text": "5.7 Impact of Replay Volume",
    "text_level": 1,
    "bbox": [
      83,
      573,
      334,
      588
    ],
    "page_idx": 7
  },
  {
    "type": "text",
    "text": "In generative continual learning, synthetic data replay serves as a key mechanism for preserving previously acquired knowledge without accessing raw data. A critical hyperparameter in this process is the volume of generated data used during training on new cities. While too little replay data may result in catastrophic forgetting, excessive generation increases computational costs and may introduce noise or redundancy. Understanding this trade-off is essential for building scalable and efficient mobility foundation models. To explore this, we vary the amount of generated data and evaluate its impact on both knowledge retention (for previously seen cities) and adaptation to new cities.",
    "bbox": [
      81,
      590,
      482,
      743
    ],
    "page_idx": 7
  },
  {
    "type": "text",
    "text": "As shown in Appendix Figure 8, the performance on the base cities (WSC) consistently improves with more generated data, indicating that replay volume directly affects the ability to retain past knowledge. In contrast, performance on the newly introduced cities (A, L, N) remains largely stable regardless of replay volume, with no consistent trend of improvement or degradation. These results suggest that while synthetic replay is crucial for mitigating forgetting, it has limited effect on new knowledge acquisition. Thus, allocating a moderate amount of generated data offers a practical balance—sufficient to preserve prior knowledge without incurring unnecessary overhead—supporting the long-term scalability.",
    "bbox": [
      81,
      743,
      482,
      896
    ],
    "page_idx": 7
  },
  {
    "type": "text",
    "text": "6 Limitations and Ethical Considerations",
    "text_level": 1,
    "bbox": [
      513,
      375,
      864,
      388
    ],
    "page_idx": 7
  },
  {
    "type": "text",
    "text": "While MoveGCL provides a robust framework for bridging data silos in mobility science, it faces several limitations and ethical challenges. First, the scientific generalizability of the model is inherently tied to the diversity of participating institutions; if the collaborative training sequence is skewed toward megacities in developed regions, it may fail to accurately represent the unique mobility dynamics of the Global South or rural areas, potentially leading to inequitable urban planning recommendations. From an ethical standpoint, although generative replay provides a significant privacy buffer, it still necessitates future research into integrating formal Differential Privacy (DP) guarantees. Furthermore, high-fidelity mobility foundation models present dual-use concerns; while they are intended to advance sustainability goals such as carbon footprint reduction, strict governance is required to prevent their misuse for unauthorized surveillance or restrictive movement policies.",
    "bbox": [
      511,
      393,
      915,
      616
    ],
    "page_idx": 7
  },
  {
    "type": "text",
    "text": "7 Conclusion",
    "text_level": 1,
    "bbox": [
      514,
      628,
      638,
      641
    ],
    "page_idx": 7
  },
  {
    "type": "text",
    "text": "In this work, we present a scalable and privacy-preserving framework for training mobility foundation models via generative continual learning. By enabling decentralized model evolution without sharing raw data, it addresses key challenges in real-world human mobility modeling, including data silos, privacy constraints, and heterogeneous mobility distributions. MoveGCL represents a significant step toward realizing mobility foundation models by offering a practical and generalizable framework that facilitates collaborative learning across cities and institutions. It paves the way for long-term, privacy-safe, and adaptive modeling of human movement, with broad implications for urban planning, transportation optimization, and evidence-based policy making. The development of more large-scale, semantically rich, and geographically diverse mobility datasets will be crucial for further improving the model's generalization and robustness. We encourage the broader research community and data-holding institutions to join this collaborative effort, contributing to the creation of open, inclusive, and powerful spatiotemporal foundation models for the mobility domain.",
    "bbox": [
      511,
      646,
      915,
      896
    ],
    "page_idx": 7
  },
  {
    "type": "header",
    "text": "Conference'17, July 2017, Washington, DC, USA",
    "bbox": [
      84,
      75,
      313,
      85
    ],
    "page_idx": 7
  },
  {
    "type": "header",
    "text": "Yuan et al.",
    "bbox": [
      859,
      75,
      911,
      85
    ],
    "page_idx": 7
  },
  {
    "type": "text",
    "text": "References",
    "text_level": 1,
    "bbox": [
      84,
      104,
      176,
      119
    ],
    "page_idx": 8
  },
  {
    "type": "list",
    "sub_type": "ref_text",
    "list_items": [
      "[1] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM, 308-318.",
      "[2] Galen Andrew, Steve Chien, and Nicolas Papernot. 2019. TensorFlow Privacy: Learning with Differential Privacy for Training Machine Learning Models. https:// blog.tensorflow.org/2019/03/introducing-tensorflow-privacy-learning.html. Accessed: 2025-06-06.",
      "[3] Hugo Barbosa, Marc Barthelemy, Gourab Ghoshal, Charlotte R James, Maxime Lenormand, Thomas Louail, Ronaldo Menezes, José J Ramasco, Filippo Simini, and Marcello Tomasini. 2018. Human mobility: Models and applications. Physics Reports 734 (2018), 1-74.",
      "[4] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. 2024. DeepSeek LLM: Scaling Open-Source Language Models with Longtermism. CoRR (2024).",
      "[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901.",
      "[6] Haoye Chai, Yuan Yuan, and Yong Li. 2025. MobiWorld: World Models for Mobile Wireless Network. arXiv preprint arXiv:2507.09462 (2025).",
      "[7] Shushman Choudhury, Abdul Rahman Kreidieh, Ivan Kuznetsov, and Neha Arora. 2024. Towards a Trajectory-powered Foundation Model of Mobility. In Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Spatial Big Data and AI for Industrial Applications. 1-4.",
      "[8] Yves-Alexandre de Montjoye, Cesar A. Hidalgo, Michel Verleysen, and Vincent D. Blondel. 2013. Unique in the crowd: The privacy bounds of human mobility. Scientific Reports 3 (2013), 1376. doi:10.1038/srep01376",
      "[9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. 2024. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning.",
      "[10] Jie Feng, Yuwei Du, Jie Zhao, and Yong Li. 2024. Agentmove: Predicting human mobility anywhere using large language model based agentic framework. arXiv preprint arXiv:2408.13986 (2024).",
      "[11] Jie Feng, Yong Li, Zeyu Yang, Qiang Qiu, and Depeng Jin. 2022. Predicting Human Mobility With Semantic Motivation via Multi-Task Attentional Recurrent Networks. IEEE Transactions on Knowledge and Data Engineering 34, 5 (2022), 2360-2374. doi:10.1109/TKDE.2020.3006048",
      "[12] Jie Feng, Can Rong, Funing Sun, Diansheng Guo, and Yong Li. 2020. PMF: A privacy-preserving human mobility prediction framework via federated learning. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 4, 1 (2020), 10:1-10:21. doi:10.1145/3381006",
      "[13] Sébastien Gambs, Marc-Olivier Killijian, and Miguel Núñez del Prado Cortez. 2012. Next place prediction using mobility Markov chains. In Proceedings of the First Workshop on Measurement, Privacy, and Mobility. ACM, Bern, Switzerland, 3-8.",
      "[14] Letian Gong, Yan Lin, Yiwen Lu, Xuedi Han, Yichen Liu, Shengnan Guo, Youfang Lin, Huaiyu Wan, et al. 2024. Mobility-llm: Learning visiting intentions and travel preference from human mobility data with large language models. Advances in Neural Information Processing Systems 37 (2024), 36185–36217.",
      "[15] Chonghua Han, Yuan Yuan, Kaiyan Chen, Jingtao Ding, and Yong Li. 2025. TrajMoE: Spatially-Aware Mixture of Experts for Unified Human Mobility Modeling. arXiv preprint arXiv:2505.18670 (2025).",
      "[16] Lu Huang, Jianguo Wu, and Lijiao Yan. 2015. Defining and measuring urban sustainability: a review of indicators. *Landscape ecology* 30, 7 (2015), 1175-1193.",
      "[17] WANG JIAWEI, Renhe Jiang, Chuang Yang, Zengqing Wu, Ryosuke Shibasaki, Noboru Koshizuka, Chuan Xiao, et al. 2024. Large language models as urban residents: An llm agent framework for personal mobility generation. Advances in Neural Information Processing Systems 37 (2024), 124547-124574.",
      "[18] Chenlu Ju, Jiaxin Liu, Shobhit Sinha, Hao Xue, and Flora Salim. 2025. TrajLLM: A Modular LLM-Enhanced Agent-Based Framework for Realistic Human Trajectory Simulation. arXiv preprint arXiv:2502.18712 (2025).",
      "[19] Gyuhak Kim, Changnan Xiao, Tatsuya Konishi, Zixuan Ke, and Bing Liu. 2022. A theoretical study on solving continual learning. Advances in neural information processing systems 35 (2022), 5065-5079.",
      "[20] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems 35 (2022), 22199-22213.",
      "[21] Dejiang Kong and Fei Wu. 2018. HST-LSTM: A hierarchical spatial-temporal long-short term memory network for location prediction. In Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI). AAAI Press, Stockholm, Sweden, 2341-2347.",
      "[22] Xiangjie Kong, Qiao Chen, Mingliang Hou, Hui Wang, and Feng Xia. 2023. Mobility trajectory generation: a survey. Artificial Intelligence Review 56, Suppl 3 (2023), 3057-3098."
    ],
    "bbox": [
      86,
      122,
      483,
      888
    ],
    "page_idx": 8
  },
  {
    "type": "list",
    "sub_type": "ref_text",
    "list_items": [
      "[23] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. 2019. Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting. In International conference on machine learning. PMLR, 3925-3934.",
      "[24] Zhonghang Li, Long Xia, Lei Shi, Yong Xu, Dawei Yin, and Chao Huang. 2024. Opencity: Open spatio-temporal foundation models for traffic prediction. arXiv preprint arXiv:2408.10269 (2024).",
      "[25] Zhonghang Li, Lianghao Xia, Jiabin Tang, Yong Xu, Lei Shi, Long Xia, Dawei Yin, and Chao Huang. 2024. Urbangpt: Spatio-temporal large language models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 5351-5362.",
      "[26] Yuxuan Lin, Hongxu Wan, Shenglin Guo, and Yantao Lin. 2021. Pre-training context and time aware location embeddings from spatial-temporal trajectories for user next location prediction. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 4241-4248. doi:10.1609/aaai.v35i5.16548",
      "[27] Yan Lin, Tonglong Wei, Zeyu Zhou, Haomin Wen, Jilin Hu, Shengnan Guo, Youfang Lin, and Huaiyu Wan. 2024. TrajFM: A vehicle trajectory foundation model for region and task transferability. arXiv preprint arXiv:2408.15251 (2024).",
      "[28] Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar. 2020. Using gans for sharing networked time series data: Challenges, initial promise, and open questions. In Proceedings of the ACM internet measurement conference. 464-483.",
      "[29] Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar. 2020. Using GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions. In Proceedings of the ACM Internet Measurement Conference (IMC). 464-483.",
      "[30] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. 2024. Sora: A review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177 (2024).",
      "[31] Ziqiao Liu, Hao Miao, Yan Zhao, Chenxi Liu, Kai Zheng, and Huan Li. 2024. LightTR: A Lightweight Framework for Federated Trajectory Recovery. In 2024 IEEE 40th International Conference on Data Engineering (ICDE). 4422-4434. doi:10.1109/ICDE60146.2024.00337",
      "[32] Qingyue Long, Yuan Yuan, and Yong Li. 2024. A Universal Model for Human Mobility Prediction. arXiv preprint arXiv:2412.15294 (2024).",
      "[33] Saeed Masoudnia and Reza Ebrahimpour. 2014. Mixture of experts: a literature survey. Artificial Intelligence Review 42 (2014), 275-293.",
      "[34] New York City Taxi and Limousine Commission. 2023. TLC Trip Record Data. https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page. [Online; accessed 16-November-2023].",
      "[35] Pierre Nouvellet, Sangeeta Bhatia, Anne Cori, Kylie EC Ainslie, Marc Baguelin, Samir Bhatt, Adhiratha Boonyasiri, Nicholas F Brazeau, Lorenzo Cattarino, Laura V Cooper, et al. 2021. Reduction in mobility and COVID-19 transmission. Nature communications 12, 1 (2021), 1090.",
      "[36] Can Rong, Jingtao Ding, and Yong Li. 2024. An interdisciplinary survey on origin-destination flows modeling: Theory and techniques. Comput. Surveys 57, 1 (2024), 1-49.",
      "[37] Markus Schlapfer, Lei Dong, Kevin O'Keeffe, Paolo Santi, Michael Szell, Hadrien Salat, Samuel Ankesaria, Mohammad Vazifeh, Carlo Ratti, and Geoffrey B West. 2021. The universal visitation law of human mobility. Nature 593, 7860 (2021), 522-527.",
      "[38] Chenyang Shao, Fengli Xu, Bingbing Fan, Jingtao Ding, Yuan Yuan, Meng Wang, and Yong Li. 2024. Beyond imitation: Generating human mobility from context-aware reasoning with large language models. CoRR (2024).",
      "[39] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Membership inference attacks against machine learning models. In Proceedings of the 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 3-18.",
      "[40] Junjun Si, Jin Yang, Yang Xiang, Hanqiu Wang, Li Li, Rongqing Zhang, Bo Tu, and Xiangqun Chen. 2023. TrajBERT: BERT-based trajectory recovery with spatial-temporal refinement for implicit sparse trajectories. IEEE Transactions on Mobile Computing 23, 5 (2023), 4849-4860.",
      "[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (NeurIPS). Curran Associates Inc., Long Beach, California, USA, 6000-6010.",
      "[42] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. 2024. A comprehensive survey of continual learning: Theory, method and application. IEEE transactions on pattern analysis and machine intelligence 46, 8 (2024), 5362-5383.",
      "[43] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17. 1-7.",
      "[44] Buddhi Wickramasinghe, Gobinda Saha, and Kaushik Roy. 2023. Continual learning: A review of techniques, challenges, and future directions. IEEE Transactions on Artificial Intelligence 5, 6 (2023), 2526-2546.",
      "[45] Fengli Xu, Zhen Tu, Yong Li, Pengyu Zhang, Xiaoming Fu, and Depeng Jin. 2017. Trajectory recovery from ash: User privacy is not preserved in aggregated mobility data. In Proceedings of the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 1241-1250."
    ],
    "bbox": [
      516,
      108,
      913,
      904
    ],
    "page_idx": 8
  },
  {
    "type": "header",
    "text": "MoveGCL",
    "bbox": [
      84,
      75,
      135,
      85
    ],
    "page_idx": 8
  },
  {
    "type": "header",
    "text": "Conference'17, July 2017, Washington, DC, USA",
    "bbox": [
      684,
      75,
      911,
      87
    ],
    "page_idx": 8
  },
  {
    "type": "list",
    "sub_type": "ref_text",
    "list_items": [
      "[46] Fengli Xu, Qi Wang, Esteban Moro, Lin Chen, Arianna Salazar Miranda, Marta C González, Michele Tizzoni, Chaoming Song, Carlo Ratti, Luis Bettencourt, et al. 2025. Using human mobility data to quantify experienced urban inequalities. Nature Human Behaviour (2025), 1-11.",
      "[47] Takahiro Yabe, Kota Tsubouchi, Toru Shimizu, Yoshihide Sekimoto, Kaoru Sezaki, Esteban Moro, and Alex Pentland. 2024. YJMob100K: City-scale and longitudinal dataset of anonymized human mobility trajectories. Scientific Data 11, 1 (2024), 397.",
      "[48] Dingqi Yang, Bingqing Qu, Jie Yang, and Philippe Cudre-Mauroux. 2019. Revisiting user mobility and social relationships in lbsns: a hypergraph embedding approach. In The world wide web conference. 2147-2157.",
      "[49] Jaehong Yoon, Wonyong Jeong, Giwoong Lee, Eunho Yang, and Sung Ju Hwang. 2021. Federated continual learning with weighted inter-client transfer. In International conference on machine learning. PMLR, 12073-12086.",
      "[50] Jing Yuan, Yu Zheng, Chengyang Zhang, Wenlei Xie, Xing Xie, Guangzhong Sun, and Yan Huang. 2010. T-drive: driving directions based on taxi trajectories. In Proceedings of the 18th SIGSPATIAL International conference on advances in geographic information systems. 99-108.",
      "[51] Yuan Yuan, Jingtao Ding, Jie Feng, Depeng Jin, and Yong Li. 2024. Unist: A prompt-empowered universal model for urban spatio-temporal prediction. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 4095-4106.",
      "[52] Yuan Yuan, Jingtao Ding, Chonghua Han, Zhi Sheng, Depeng Jin, and Yong Li. 2024. UniFlow: A Foundation Model for Unified Urban Spatio-Temporal Flow Prediction. arXiv preprint arXiv:2411.12972 (2024).",
      "[53] Yuan Yuan, Jingtao Ding, Depeng Jin, and Yong Li. 2025. Learning the complexity of urban mobility with deep generative network. PNAS nexus 4, 5 (2025), pgaf081.",
      "[54] Yuan Yuan, Jingtao Ding, Huandong Wang, and Depeng Jin. 2024. Generating Daily Activities with Need Dynamics. ACM Transactions on Intelligent Systems and Technology 15, 2 (2024), 29:1-29:28. doi:10.1145/3637493",
      "[55] Yuan Yuan, Chonghua Han, Jingtao Ding, Depeng Jin, and Yong Li. 2024. Urbandit: A foundation model for open-world urban spatio-temporal learning. arXiv preprint arXiv:2411.12164 (2024)."
    ],
    "bbox": [
      84,
      108,
      483,
      443
    ],
    "page_idx": 9
  },
  {
    "type": "list",
    "sub_type": "ref_text",
    "list_items": [
      "[56] Yuan Yuan, Yuheng Zhang, Jingtao Ding, and Yong Li. 2025. WorldMove, a global open data for human mobility. arXiv preprint arXiv:2504.10506 (2025).",
      "[57] Jinwei Zeng, Yu Liu, Jingtao Ding, Jian Yuan, and Yong Li. 2024. Estimating on-road transportation carbon emissions from open data of road network and origin-destination flow data. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 22493–22501.",
      "[58] Junbo Zhang, Yu Zheng, and Dekang Qi. 2017. Deep spatio-temporal residual networks for citywide crowd flows prediction. In Proceedings of the AAAI conference on artificial intelligence, Vol. 31.",
      "[59] Weijia Zhang, Jindong Han, Zhao Xu, Hang Ni, Hao Liu, and Hui Xiong. 2024. Urban foundation models: A survey. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 6633-6643.",
      "[60] Yuheng Zhang, Yuan Yuan, Jingtao Ding, Jian Yuan, and Yong Li. 2025. Noise Matters: Diffusion Model-based Urban Mobility Generation with Collaborative Noise Priors. In Proceedings of the ACM on Web Conference 2025. 5352-5363.",
      "[61] Yu Zheng, Hao Fu, Xing Xie, Wei-Ying Ma, and Quannan Li. [n.d.]. Geolife GPS trajectory dataset-User Guide, geolifegps trajectories 1 July 2011, geolife GPS trajectories 1.1. July 2011 geolife GPS trajectories 1.1 ([n.d.]).",
      "[62] Yu Zheng, Yuming Lin, Liang Zhao, Tinghai Wu, Depeng Jin, and Yong Li. 2023. Spatial planning of urban communities via deep reinforcement learning. Nature Computational Science 3, 9 (2023), 748-762.",
      "[63] Zhen Zhou, Ziyuan Gu, Xiaobo Qu, Pan Liu, Zhiyuan Liu, and Wenwu Yu. 2024. Urban mobility foundation model: A literature review and hierarchical perspective. Transportation Research Part E: Logistics and Transportation Review 192 (2024), 103795.",
      "[64] Yuanshao Zhu, Yongchao Ye, Ying Wu, Xiangyu Zhao, and James Yu. 2023. Synmob: Creating high-fidelity synthetic gps trajectory dataset for urban mobility analysis. Advances in Neural Information Processing Systems 36 (2023), 22961-22977.",
      "[65] Yuanshao Zhu, James Jianqiao Yu, Xiangyu Zhao, Xuetao Wei, and Yuxuan Liang. 2024. UniTraj: Universal human trajectory modeling from billion-scale worldwide traces. arXiv preprint arXiv:2411.03859 (2024)."
    ],
    "bbox": [
      516,
      108,
      913,
      431
    ],
    "page_idx": 9
  },
  {
    "type": "header",
    "text": "Conference'17, July 2017, Washington, DC, USA",
    "bbox": [
      84,
      75,
      313,
      85
    ],
    "page_idx": 9
  },
  {
    "type": "header",
    "text": "Yuan et al.",
    "bbox": [
      859,
      75,
      911,
      85
    ],
    "page_idx": 9
  },
  {
    "type": "text",
    "text": "A Experimental Settings",
    "text_level": 1,
    "bbox": [
      83,
      104,
      297,
      122
    ],
    "page_idx": 10
  },
  {
    "type": "text",
    "text": "A.1 Dataset",
    "text_level": 1,
    "bbox": [
      83,
      125,
      194,
      138
    ],
    "page_idx": 10
  },
  {
    "type": "table",
    "img_path": "images/91ccd55c7bf1f79283b3a6fb549b17f279219d4343165a54e16076a6902bef65.jpg",
    "table_caption": [
      "Table 3: Basic statistics of mobility data."
    ],
    "table_footnote": [],
    "table_body": "<table><tr><td>City</td><td>User</td><td>Trajectory</td><td>Location</td></tr><tr><td>Atlanta</td><td>114941</td><td>2348218</td><td>1175</td></tr><tr><td>Chicago</td><td>148000</td><td>8051522</td><td>4166</td></tr><tr><td>Los Angeles</td><td>161544</td><td>16844127</td><td>6198</td></tr><tr><td>New York</td><td>170321</td><td>15766369</td><td>4988</td></tr><tr><td>Seattle</td><td>88569</td><td>3362353</td><td>1046</td></tr><tr><td>Washington D.C</td><td>134442</td><td>11024181</td><td>1361</td></tr></table>",
    "bbox": [
      120,
      179,
      444,
      281
    ],
    "page_idx": 10
  },
  {
    "type": "text",
    "text": "A.2 Baselines",
    "text_level": 1,
    "bbox": [
      83,
      295,
      209,
      309
    ],
    "page_idx": 10
  },
  {
    "type": "list",
    "sub_type": "text",
    "list_items": [
      "- Traditional approach: This includes Markov models [13] that fit separate transition matrices for different datasets.",
      "- Deep mobility models: We include LSTM [21], Transformer [41], DeepMove [11], TrajBert [40], and CLET [26] as representative baselines. For each dataset, we train a separate model to ensure fair comparison under the same training conditions.",
      "- Fedarated learning models: We evaluate against PMF [12] and LightTR [31], which leverage federated learning frameworks for human mobility prediction while maintaining data decentralization. We apply their federated learning methods to train our model.",
      "- Joint models with privacy protection: These methods enable continual learning without accessing previously seen data. Specifically, we consider two variants of our model: MoveGCL (FullTune) unfreezes all experts and routers in the MoE Transformer while keeping the rest of the model frozen, and fine-tunes using only the new city's data. MoveGCL (ExpertTune) incrementally adds one new expert per layer in the MoE Transformer, unfreezes all experts and routers, and fine-tunes on the new city's data while keeping all other parameters fixed."
    ],
    "bbox": [
      83,
      313,
      482,
      590
    ],
    "page_idx": 10
  },
  {
    "type": "text",
    "text": "A.3 Parameter Settings",
    "text_level": 1,
    "bbox": [
      83,
      602,
      285,
      618
    ],
    "page_idx": 10
  },
  {
    "type": "text",
    "text": "The key parameters of our framework fall into three main categories.",
    "bbox": [
      81,
      621,
      482,
      648
    ],
    "page_idx": 10
  },
  {
    "type": "list",
    "sub_type": "text",
    "list_items": [
      "- For the model architecture, we set the temporal embedding dimension to 48. In the mobility encoder, the embedding dimensions of  $d_{\\mathrm{jump}}$  and  $t_{\\mathrm{wait}}$  are both 128, the embedding dimensions of  $r_{\\mathrm{gyr}}$  and location entropy  $H_{\\mathrm{loc}}$  are 64, and the embedding dimension of  $\\mathrm{ID}_{\\mathrm{city}}$  is 32, with the self-attention modules for both  $d_{\\mathrm{jump}}$  and  $t_{\\mathrm{wait}}$  using a hidden dimension of 64. In the trajectory location encoder and the city location encoder, the embedding dimensions of  $\\phi_{\\mathrm{POI}}, \\phi_{\\mathrm{hot}},$  and  $\\phi_{\\mathrm{lat - lon}}$  are 256, 128, 128, respectively. The hidden dimension of each MoE transformer block is set to 512. The initial number of experts in each MoE transformer block is 4, and the model comprises 6 layers of MoE transformer blocks.",
      "- For the base model training phase, the initial model is obtained by training on three cities' datasets. During this phase, we use a batch size of 16 and train for 30 epochs. The initial learning rate is set to  $1.2 \\times 10^{-5}$ , and the learning rate decays in a stepwise fashion during training.",
      "- For generative continual learning, whenever we introduce a new dataset (i.e., a new city), we add one new expert to every MoE"
    ],
    "bbox": [
      83,
      650,
      482,
      897
    ],
    "page_idx": 10
  },
  {
    "type": "text",
    "text": "transformer block. The initial learning rate for this phase is  $1.2 \\times 10^{-4}$ , the batch size is 128, and training also runs for 30 epochs, with the learning rate decaying stepwise throughout. The generative coefficient  $\\alpha$  is set to  $20\\%$ . The balance coefficient  $\\lambda$  for  $\\mathcal{L}_{\\mathrm{total}}$  is set to 1.",
    "bbox": [
      527,
      106,
      913,
      176
    ],
    "page_idx": 10
  },
  {
    "type": "text",
    "text": "A.4 Evaluation Metrics",
    "text_level": 1,
    "bbox": [
      514,
      186,
      718,
      202
    ],
    "page_idx": 10
  },
  {
    "type": "text",
    "text": "We adopt top-  $k$  accuracy as our evaluation metric, defined as",
    "bbox": [
      513,
      205,
      883,
      220
    ],
    "page_idx": 10
  },
  {
    "type": "equation",
    "text": "\n$$\n\\operatorname {a c c} @ k = \\frac {1}{N} \\sum_ {i = 1} ^ {N} 1 \\left(x _ {i} \\in f _ {k} \\left(x _ {i}\\right)\\right), \\tag {16}\n$$\n",
    "text_format": "latex",
    "bbox": [
      612,
      224,
      911,
      260
    ],
    "page_idx": 10
  },
  {
    "type": "text",
    "text": "where  $N$  is the total number of samples,  $x_{i}$  is the ground-truth label for the  $i^{\\mathrm{th}}$  sample,  $f_{k}(x_{i})$  denotes the set of the model's top- $k$  predicted labels for sample  $i$ , and  $\\mathbf{1}(\\cdot)$  is the indicator function that equals 1 if its argument is true and 0 otherwise. We report on acc@1 and acc@3 to assess the performance of the model.",
    "bbox": [
      513,
      263,
      913,
      333
    ],
    "page_idx": 10
  },
  {
    "type": "text",
    "text": "B Related Work",
    "text_level": 1,
    "bbox": [
      514,
      344,
      661,
      358
    ],
    "page_idx": 10
  },
  {
    "type": "text",
    "text": "Mobility Data. Mobility data includes aggregated flows and individual trajectories [3, 53, 60]. Aggregated flows are relatively easier to obtain and have been widely used in urban analytics [36, 58]. However, individual-level mobility data remain fragmented due to privacy concerns and institutional data silos [22, 53]. Real-world trajectory datasets, such as GeoLife [61], T-Drive [50], NYC Taxi [34], and Foursquare [48], often have limited city coverage, short time spans, and sparse sampling. Some global-scale open datasets, such as the one used in UniTraj, have been introduced, but they suffer from low spatial-temporal resolution and inconsistent data quality. With the advancement of generative AI, synthetic mobility datasets have emerged, such as SynMob [64], YJMob100K [47] and WorldMove [56]. However, the quality of synthetic data still falls short compared to real-world trajectories, particularly in terms of behavioral diversity, temporal continuity, and semantic consistency. In practice, access to real trajectory datasets typically requires signing NDAs, and most published studies do not release the mobility datasets they use due to privacy and legal restrictions [37].",
    "bbox": [
      511,
      362,
      913,
      612
    ],
    "page_idx": 10
  },
  {
    "type": "text",
    "text": "Mobility Foundation Models. Inspired by the success of foundation models in NLP and CV, recent efforts have explored pretrained models for urban and mobility domains [6, 7, 15, 55, 59, 63]. Early studies primarily focused on aggregated mobility data, leveraging mobility flows across cities to build unified spatio-temporal representations, and have demonstrated strong zero-shot transfer capabilities [24, 25, 51, 52]. In contrast, individual-level mobility foundation models are less developed. Researchers have explored multi-scale mobility modeling [32, 53, 60], aiming to capture both micro-level behaviors and macro-level patterns essential for generalization. Attempts such as UniTraj [65], TrajBert [40], and TrajFM [27] have explored learning from open trajectory datasets, but these datasets often consist of short-term or non-representative mobility traces that do not reflect regular human movement patterns. As a result, current models struggle to capture the full complexity and diversity of real-world individual mobility. Recently, LLMs have also widely utilized in generating human mobility [10, 14, 17, 18, 38], but the gap between natural language and trajectory data suggests that mobility still requires native foundation models, which can",
    "bbox": [
      511,
      618,
      913,
      882
    ],
    "page_idx": 10
  },
  {
    "type": "header",
    "text": "MoveGCL",
    "bbox": [
      84,
      75,
      135,
      85
    ],
    "page_idx": 10
  },
  {
    "type": "header",
    "text": "Conference'17, July 2017, Washington, DC, USA",
    "bbox": [
      684,
      75,
      911,
      87
    ],
    "page_idx": 10
  },
  {
    "type": "text",
    "text": "later be aligned with LLMs to bridge symbolic reasoning and physical behavior modeling.",
    "bbox": [
      81,
      106,
      485,
      136
    ],
    "page_idx": 11
  },
  {
    "type": "text",
    "text": "Continual Learning. Continual learning [19, 42, 42], also known as lifelong learning, aims to enable models to learn from a sequence of tasks or data streams without forgetting previously acquired knowledge [49]. A central challenge in continual learning is catastrophic forgetting [23, 44], where the model's performance on earlier tasks degrades as it learns new ones. Continual learning methods are typically categorized into three main types. Regularization-based methods introduce constraints on parameter updates to preserve important knowledge from earlier tasks. Replay-based methods mitigate forgetting by either storing a subset of previous data (experience replay) or generating pseudo-data (generative replay) to simulate past learning. Parameter isolation methods allocate different parts of the model to different tasks, using techniques such as dynamic networks or task-specific masking to reduce interference between tasks.",
    "bbox": [
      81,
      141,
      486,
      349
    ],
    "page_idx": 11
  },
  {
    "type": "text",
    "text": "C Additional Results",
    "text_level": 1,
    "bbox": [
      83,
      359,
      269,
      375
    ],
    "page_idx": 11
  },
  {
    "type": "text",
    "text": "C.1 Order Invariance in Continual Learning",
    "text_level": 1,
    "bbox": [
      83,
      380,
      455,
      396
    ],
    "page_idx": 11
  },
  {
    "type": "text",
    "text": "To assess the robustness of MoveGCL in real-world deployment scenarios, we investigate its sensitivity to the order in which data from different cities is introduced during continual learning. In practice, the arrival of mobility data is often dictated by external factors such as data access regulations, infrastructure development cycles, or institutional collaborations. As a result, foundation models intended for long-term, large-scale deployment must remain robust to such variations in data sequencing.",
    "bbox": [
      81,
      398,
      480,
      508
    ],
    "page_idx": 11
  },
  {
    "type": "text",
    "text": "We simulate this scenario by reversing the order of datasets used in the continual learning phase. As shown in Table 4, the performance of MoveGCL remains remarkably consistent, with the vast majority of metrics showing deviations of less than  $5\\%$  across original and reversed sequences. This empirical finding confirms the order-invariant learning behavior of our model, demonstrating that MoveGCL can integrate new city data without disrupting previously acquired knowledge, even when the order of exposure varies significantly. This property is especially crucial for building scalable and unified mobility foundation models, which must support progressive, privacy-preserving knowledge accumulation in non-i.i.d. settings where data arrives incrementally and asynchronously. Order robustness is thus a key enabler for deploying foundation models that can continually evolve while ensuring stable performance and generalization across diverse urban contexts.",
    "bbox": [
      81,
      510,
      482,
      717
    ],
    "page_idx": 11
  },
  {
    "type": "text",
    "text": "C.2 Experimental Settings of Privacy Evaluation",
    "text_level": 1,
    "bbox": [
      83,
      728,
      398,
      760
    ],
    "page_idx": 11
  },
  {
    "type": "text",
    "text": "Uniqueness Testing. We randomly extract a subset of trajectories from the training set and use an autoregressive process to generate new trajectories conditioned on each sampled trajectory. We then compute the pairwise similarity between each original sample and its corresponding generated trajectory. If the lengths of two trajectories are different, the similarity score is defined as 0. If they are of equal length, the similarity score is calculated as the proportion of positions where both the timestamp and location ID exactly match. For each generated trajectory, we compute its similarity score with the top-1, top-3, and top-5 most similar real trajectories.",
    "bbox": [
      511,
      106,
      915,
      256
    ],
    "page_idx": 11
  },
  {
    "type": "text",
    "text": "Membership Inference Attack. Following the experimental setup in [29, 39], we use the similarity between generated trajectories and their corresponding real input trajectories as the classification feature. For each trajectory  $x$ , MoveGCL generates  $\\tilde{x}$  autoregressively conditioned on  $x$ , and we compute a similarity score  $s(x, \\tilde{x})$  to form the input to the classifier. The classifier is then tasked with determining whether  $x$  was included in the model's training set. Positive samples consist of real-world trajectories that were used during training, while negative samples are real trajectories from the same city that were held out. We evaluate the attack success rate, defined as the proportion of samples for which the classifier correctly infers membership status. We employ three widely used classification algorithms: Logistic Regression (LR), Support Vector Machine (SVM) and Random Forest (RF).",
    "bbox": [
      511,
      263,
      915,
      458
    ],
    "page_idx": 11
  },
  {
    "type": "text",
    "text": "Differential Privacy. For any pair of datasets  $D$  and  $D'$  that differ by only a small number of training trajectories, a model  $M$  is said to satisfy  $(\\varepsilon, \\delta)$ -differential privacy if the following condition holds:",
    "bbox": [
      511,
      463,
      913,
      518
    ],
    "page_idx": 11
  },
  {
    "type": "equation",
    "text": "\n$$\n\\mathbb {P} [ M (z; D) = z ] \\leq e ^ {\\varepsilon} \\mathbb {P} [ M (z; D ^ {\\prime}) = z ] + \\delta , \\tag {17}\n$$\n",
    "text_format": "latex",
    "bbox": [
      575,
      522,
      911,
      540
    ],
    "page_idx": 11
  },
  {
    "type": "text",
    "text": "where  $\\mathbb{P}\\big[M(z;D) = z\\big]$  denotes the probability of observing output  $z$  when the model is trained on dataset  $D$ , and  $\\mathbb{P}\\big[M(z;D') = z\\big]$  is defined analogously for dataset  $D'$ . Smaller values of  $\\varepsilon$  and  $\\delta$  imply stronger privacy guarantees, since the model's output distribution becomes less dependent on any single trajectory.",
    "bbox": [
      511,
      542,
      913,
      614
    ],
    "page_idx": 11
  },
  {
    "type": "text",
    "text": "In our experiment, we randomly select a subset of trajectories and consider two training scenarios: one in which this subset is included in the training data  $(D)$ , and one in which it is excluded  $(D^{\\prime})$ . For each scenario, we train  $M$  on the corresponding dataset and then use each selected trajectory as a conditioning input to generate multiple synthetic trajectories. We compute a similarity score between each generated trajectory and its original conditioning trajectory. These similarity scores are modeled as two Gaussian distributions corresponding to  $\\mathbb{P}\\big[M(z;D)\\big]$  and  $\\mathbb{P}\\big[M(z;D^{\\prime})\\big]$ , respectively. Finally, we estimate the privacy-budget parameters  $\\varepsilon$  from these distributions.",
    "bbox": [
      511,
      614,
      915,
      766
    ],
    "page_idx": 11
  },
  {
    "type": "text",
    "text": "C.3 Supplementary Figures",
    "text_level": 1,
    "bbox": [
      514,
      777,
      751,
      795
    ],
    "page_idx": 11
  },
  {
    "type": "header",
    "text": "Conference'17, July 2017, Washington, DC, USA",
    "bbox": [
      83,
      75,
      313,
      87
    ],
    "page_idx": 11
  },
  {
    "type": "header",
    "text": "Yuan et al.",
    "bbox": [
      859,
      75,
      911,
      85
    ],
    "page_idx": 11
  },
  {
    "type": "table",
    "img_path": "images/70960c22f617cdcc7a8309f5f7f14d8fdecd0fd17f13ba086efb236da6b14c99.jpg",
    "table_caption": [
      "Table 4: Performance comparison across cities and Acc@k metrics for evaluating order invariance."
    ],
    "table_footnote": [],
    "table_body": "<table><tr><td>Model</td><td colspan=\"2\">Atlanta</td><td colspan=\"2\">Chicago</td><td colspan=\"2\">Los Angeles</td><td colspan=\"2\">New York</td><td colspan=\"2\">Seattle</td><td colspan=\"2\">Washington D.C</td></tr><tr><td></td><td>Acc@1</td><td>Acc@3</td><td>Acc@1</td><td>Acc@3</td><td>Acc@1</td><td>Acc@3</td><td>Acc@1</td><td>Acc@3</td><td>Acc@1</td><td>Acc@3</td><td>Acc@1</td><td>Acc@3</td></tr><tr><td>WSC→A→L→N</td><td>0.282</td><td>0.421</td><td>0.197</td><td>0.306</td><td>0.157</td><td>0.254</td><td>0.206</td><td>0.328</td><td>0.324</td><td>0.478</td><td>0.273</td><td>0.413</td></tr><tr><td>AWN→L→S→C</td><td>0.284</td><td>0.423</td><td>0.197</td><td>0.308</td><td>0.150</td><td>0.246</td><td>0.200</td><td>0.326</td><td>0.317</td><td>0.469</td><td>0.265</td><td>0.407</td></tr><tr><td>WAL→N→S→C</td><td>0.285</td><td>0.426</td><td>0.194</td><td>0.304</td><td>0.151</td><td>0.245</td><td>0.188</td><td>0.306</td><td>0.321</td><td>0.472</td><td>0.267</td><td>0.407</td></tr><tr><td>Joint Training</td><td>0.288</td><td>0.428</td><td>0.192</td><td>0.302</td><td>0.156</td><td>0.250</td><td>0.198</td><td>0.320</td><td>0.322</td><td>0.475</td><td>0.270</td><td>0.410</td></tr></table>",
    "bbox": [
      86,
      119,
      910,
      213
    ],
    "page_idx": 12
  },
  {
    "type": "image",
    "img_path": "images/a2f608a4d796db0574158617d6a79a64f69cd61c7a44cb9c371c3628777e431b.jpg",
    "image_caption": [
      "Figure 7: Ablation study. \"w/o KD\" denotes removal of the knowledge distillation loss; \"w/o MAER\" denotes removal of mobility feature from MoE transformer's router inputs."
    ],
    "image_footnote": [],
    "bbox": [
      259,
      229,
      736,
      441
    ],
    "page_idx": 12
  },
  {
    "type": "image",
    "img_path": "images/8eeb9594b660523b902bb4dedca8dcbda5534cf6643ae6f45b21a363dce5f8ab.jpg",
    "image_caption": [
      "Figure 8: Acc@1 changes at different generated data ratios  $(\\alpha)$ , relative to  $\\alpha = 5\\%$ ."
    ],
    "image_footnote": [],
    "bbox": [
      166,
      510,
      493,
      674
    ],
    "page_idx": 12
  },
  {
    "type": "image",
    "img_path": "images/57086d1dbfd53d9cd3677228bf77caea2388f2d774654080c5c3fc0662d77701.jpg",
    "image_caption": [],
    "image_footnote": [],
    "bbox": [
      504,
      511,
      813,
      674
    ],
    "page_idx": 12
  },
  {
    "type": "header",
    "text": "MoveGCL",
    "bbox": [
      84,
      75,
      135,
      85
    ],
    "page_idx": 12
  },
  {
    "type": "header",
    "text": "Conference'17, July 2017, Washington, DC, USA",
    "bbox": [
      684,
      75,
      913,
      87
    ],
    "page_idx": 12
  }
]