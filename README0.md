# ğŸ“ ArXiv Daily Institutional Paper Collector

> Automated daily downloader for arXiv **Computer Science** papers.  
> æ¯æ—¥è‡ªåŠ¨ä¸‹è½½å¹¶åˆ†ç±» arXiv **è®¡ç®—æœºç§‘å­¦** è®ºæ–‡ã€‚

---

<details>
<summary>ğŸŒ English Version (click to expand)</summary>

## âœ¨ Whatâ€™s New (Optimized)

- **Sharded Baseline**: Baseline crawl now supports **per-subcategory shards** (e.g., `cs.CV`, `cs.CL`, `cs.LG`â€¦), avoiding `cat:cs.*` pagination limits on some mirrors.  
- **Endpoint Priority**: Prefer `https://arxiv.org/api/query` to fix the â€œ10 results onlyâ€ issue.  
- **Rich Diagnostics**: Page-level logs, per-org fallback stats, and time window coverage check.  
- **PDF-Based Classification**: Detect affiliations from PDF author blocks.  
- **Configurable Depth**: Adjustable per-org search and baseline crawl size.

---

## ğŸ“Œ Core Features

| Feature | Description |
|----------|--------------|
| ğŸ•’ Daily Schedule | Filters papers by **yesterday (Beijing time)** |
| ğŸ› Real Affiliation Detection | Extracts institution info from **PDF** (not title/abstract) |
| ğŸ¯ Organization Classification | Supports **Big Tech, Universities, Chinese AI Labs, Institutes** |
| ğŸ“‚ Auto Folder Structure | `output_org_pdfs/YYYY-MM-DD/<ORG>/paper.pdf` |
| ğŸ§  Smart Fallback | Org-specific API search when baseline misses |
| ğŸ’¾ Caching | Unified `cache_pdfs/` to prevent re-downloads |
| ğŸ§© Sharded Crawl | Multiple CS subcategories for better coverage |

---

## ğŸ—‚ Structure

```

DailyPaper/
â”œâ”€â”€ app.py
â”œâ”€â”€ config.py
â”œâ”€â”€ fetch_arxiv.py          # sharded baseline + robust session
â”œâ”€â”€ filters.py
â”œâ”€â”€ classify.py
â”œâ”€â”€ pdf_affil.py
â”œâ”€â”€ prefetch.py
â”œâ”€â”€ affil_classify.py
â”œâ”€â”€ utils.py
â””â”€â”€ output_org_pdfs/
â””â”€â”€ YYYY-MM-DD/
â”œâ”€â”€ Google/
â”œâ”€â”€ OpenAI/
â””â”€â”€ ...

````

---

## âš™ï¸ Installation

```bash
git clone https://github.com/<yourname>/arxiv-daily-paper.git
cd arxiv-daily-paper
python -m venv venv
# Windows
venv\Scripts\activate
# macOS/Linux
source venv/bin/activate
pip install -r requirements.txt
````

---

## ğŸ” Configuration (`config.py`)

### Time Window

```python
WINDOW_FIELD = "updated"    # "updated" | "published" | "both"
```

| Mode        | Meaning                          |
| ----------- | -------------------------------- |
| `updated`   | Papers updated yesterday         |
| `published` | Papers first submitted yesterday |
| `both`      | Must satisfy both conditions     |

### PDF Affiliation Extraction

```python
CLASSIFY_FROM_PDF = True
MAX_PDF_PAGES_TO_SCAN = 2
```

### Baseline Crawl

```python
USE_SHARDED_BASELINE = True
MAX_RESULTS_PER_PAGE = 200
MAX_PAGES = 10
```

### Per-Org Fallback

```python
PER_ORG_SEARCH_LIMIT_PAGES = 5
PER_ORG_SEARCH_PAGE_SIZE   = 200
```

---

## ğŸš€ Run

```bash
python app.py
```

### Dry Run (No File Output)

```python
# config.py
DRY_RUN = True
LIMIT_PER_ORG = 2
```

---

## ğŸ§ª Example Output

```
output_org_pdfs/
â””â”€â”€ 2025-10-20/
    â”œâ”€â”€ Google/
    â”‚   â””â”€â”€ 2510.13778v1.pdf
    â”œâ”€â”€ OpenAI/
    â”‚   â””â”€â”€ 2510.13724v1.pdf
    â””â”€â”€ ETH/
        â””â”€â”€ 2510.11448v2.pdf
```

---

## ğŸ§  Why PDF-Based Classification?

| Title/Abstract-Based     | PDF Author Block-Based       |
| ------------------------ | ---------------------------- |
| âŒ Often inaccurate       | âœ… Reliable                   |
| Misses real affiliations | Captures printed author info |
| Ignores lab names        | Detects institutes clearly   |

---

## âš™ï¸ GitHub Actions (Optional)

```yaml
schedule:
  - cron: "0 0 * * *"   # Run daily 08:00 BJT
```

---

## ğŸ“œ License

MIT License â€“ free to modify, fork, and automate.

ğŸŒŸ If this tool saves your time, please star the repo!

</details>

---

<details open>
<summary>ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç‰ˆæœ¬ (ç‚¹å‡»å±•å¼€)</summary>

## âœ¨ æ–°ç‰¹æ€§ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

* **åˆ†ç‰‡æŠ“å–**ï¼šæŒ‰ `cs.AI`, `cs.CL`, `cs.CV`, `cs.LG` ç­‰å­ç±»åˆ†é¡µï¼Œé¿å… `cat:cs.*` åˆ†é¡µå¤±æ•ˆã€‚
* **ä¸»ç«™ä¼˜å…ˆ**ï¼šé»˜è®¤ä½¿ç”¨ `https://arxiv.org/api/query`ï¼Œé¿å… `export.arxiv.org` é™åˆ¶è¿”å› 10 æ¡ã€‚
* **è°ƒè¯•æ—¥å¿—**ï¼šæä¾›é¡µçº§/åˆ†ç‰‡çº§æŠ“å–æ—¥å¿—ä¸æœºæ„å›é€€ç»Ÿè®¡ã€‚
* **PDF åˆ†ç±»**ï¼šåŸºäºè®ºæ–‡ PDF ä½œè€…å•ä½åŒºå—è¯†åˆ«çœŸå®æœºæ„ã€‚
* **å¯é…ç½®æ·±åº¦**ï¼šæ”¯æŒè°ƒæ•´ baseline ä¸æœºæ„å›é€€çš„é¡µæ•°ä¸å¤§å°ã€‚

---

## ğŸ“Œ æ ¸å¿ƒåŠŸèƒ½

| åŠŸèƒ½         | è¯´æ˜                                           |
| ---------- | -------------------------------------------- |
| ğŸ•’ æ¯æ—¥è¿‡æ»¤    | æŒ‰åŒ—äº¬æ—¶é—´ç­›é€‰â€œæ˜¨æ—¥è®ºæ–‡â€                                |
| ğŸ› å®ä½“è¯†åˆ«    | ä» PDF ä½œè€…æ æå–æœºæ„ä¿¡æ¯                              |
| ğŸ¯ è‡ªåŠ¨åˆ†ç±»    | æ”¯æŒç§‘æŠ€å…¬å¸ã€é«˜æ ¡ã€ç ”ç©¶é™¢ã€AI å®éªŒå®¤                         |
| ğŸ“‚ è‡ªåŠ¨æ–‡ä»¶å¤¹ç»“æ„ | `output_org_pdfs/YYYY-MM-DD/<ORG>/paper.pdf` |
| ğŸ§  æ™ºèƒ½è¡¥å…¨    | å½“ baseline æ¼æ£€æ—¶æŒ‰æœºæ„å…³é”®è¯ç›´æœè¡¥é½                     |
| ğŸ’¾ ç¼“å­˜æœºåˆ¶    | `cache_pdfs/` é¿å…é‡å¤ä¸‹è½½                         |
| ğŸ§© åˆ†ç‰‡çˆ¬å–    | éå†å¤šä¸ª CS å­ç±»ä»¥ç¡®ä¿è¦†ç›–ç‡                             |

---

## ğŸ—‚ é¡¹ç›®ç»“æ„

```
DailyPaper/
â”œâ”€â”€ app.py
â”œâ”€â”€ config.py
â”œâ”€â”€ fetch_arxiv.py          # åˆ†ç‰‡+ç¨³å¥ session
â”œâ”€â”€ filters.py
â”œâ”€â”€ classify.py
â”œâ”€â”€ pdf_affil.py
â”œâ”€â”€ prefetch.py
â”œâ”€â”€ affil_classify.py
â”œâ”€â”€ utils.py
â””â”€â”€ output_org_pdfs/
    â””â”€â”€ YYYY-MM-DD/
        â”œâ”€â”€ Google/
        â”œâ”€â”€ OpenAI/
        â””â”€â”€ ...
```

---

## âš™ï¸ å®‰è£…æ­¥éª¤

```bash
git clone https://github.com/<ä½ çš„ç”¨æˆ·å>/arxiv-daily-paper.git
cd arxiv-daily-paper
python -m venv venv
# Windows
venv\Scripts\activate
# macOS/Linux
source venv/bin/activate
pip install -r requirements.txt
```

---

## ğŸ” ä¸»è¦é…ç½®ï¼ˆconfig.pyï¼‰

### æ—¶é—´çª—å£

```python
WINDOW_FIELD = "updated"    # "updated" | "published" | "both"
```

| æ¨¡å¼          | å«ä¹‰             |
| ----------- | -------------- |
| `updated`   | æ˜¨æ—¥æœ‰æ›´æ–°ï¼ˆv2/v3 ç­‰ï¼‰ |
| `published` | æ˜¨æ—¥é¦–æ¬¡æäº¤         |
| `both`      | åŒæ—¶æ»¡è¶³ä¸¤è€…         |

### PDF ä½œè€…å•ä½è¯†åˆ«

```python
CLASSIFY_FROM_PDF = True
MAX_PDF_PAGES_TO_SCAN = 2
```

### åŸºç¡€æŠ“å–

```python
USE_SHARDED_BASELINE = True
MAX_RESULTS_PER_PAGE = 200
MAX_PAGES = 10
```

### æŒ‰æœºæ„è¡¥å…¨æœç´¢

```python
PER_ORG_SEARCH_LIMIT_PAGES = 5
PER_ORG_SEARCH_PAGE_SIZE   = 200
```

---

## ğŸš€ è¿è¡Œ

```bash
python app.py
```

### æµ‹è¯•æ¨¡å¼ï¼ˆä¸å†™æ–‡ä»¶ï¼‰

```python
# config.py
DRY_RUN = True
LIMIT_PER_ORG = 2
```

---

## ğŸ§ª è¾“å‡ºç¤ºä¾‹

```
output_org_pdfs/
â””â”€â”€ 2025-10-20/
    â”œâ”€â”€ Google/
    â”‚   â””â”€â”€ 2510.13778v1.pdf
    â”œâ”€â”€ OpenAI/
    â”‚   â””â”€â”€ 2510.13724v1.pdf
    â””â”€â”€ ETH/
        â””â”€â”€ 2510.11448v2.pdf
```

---

## ğŸ§  ä¸ºä»€ä¹ˆè¦ä» PDF è¯†åˆ«æœºæ„ï¼Ÿ

| æ ‡é¢˜/æ‘˜è¦åˆ†ç±»  | PDF ä½œè€…æ åˆ†ç±»      |
| -------- | -------------- |
| âŒ é”™è¯¯ç‡é«˜   | âœ… ç²¾ç¡®           |
| éš¾æ£€æµ‹å®éªŒå®¤åç§° | ç›´æ¥åŒ…å«çœŸå®å•ä½/å®éªŒå®¤åç§° |

---

## ğŸ”§ è‡ªåŠ¨åŒ–è¿è¡Œï¼ˆå¯é€‰ï¼‰

```yaml
schedule:
  - cron: "0 0 * * *"   # æ¯å¤© 08:00ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰
```

---

## ğŸ“œ è®¸å¯è¯

MIT License â€“ å¯è‡ªç”±ä¿®æ”¹ã€å¤ç”¨ä¸è‡ªåŠ¨åŒ–ã€‚

ğŸŒŸ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ä¸ºä»“åº“ç‚¹ä¸ª â­ï¼

</details> ```



