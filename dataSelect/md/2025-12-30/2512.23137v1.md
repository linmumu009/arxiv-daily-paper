# Graph Neural Networks with Transformer Fusion of Brain Connectivity Dynamics and Tabular Data for Forecasting Future Tobacco Use

Runzhi Zhou and Xi Luo

# Abstract

Integrating non-Euclidean brain imaging data with Euclidean tabular data, such as clinical and demographic information, poses a substantial challenge for medical imaging analysis, particularly in forecasting future outcomes. While machine learning and deep learning techniques have been applied successfully to cross-sectional classification and prediction tasks, effectively forecasting outcomes in longitudinal imaging studies remains challenging. To address this challenge, we introduce a time-aware graph neural network model with transformer fusion (GNN-TF). This model flexibly integrates both tabular data and dynamic brain connectivity data, leveraging the temporal order of these variables within a coherent framework. By incorporating non-Euclidean and Euclidean sources of information from a longitudinal resting-state fMRI dataset from the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA), the GNN-TF enables a comprehensive analysis that captures critical aspects of longitudinal imaging data. Comparative analyses against a variety of established machine learning and deep learning models demonstrate that GNN-TF outperforms these state-of-the-art methods, delivering superior predictive accuracy for predicting future tobacco usage. The end-to-end, time-aware transformer fusion structure of the proposed GNN-TF model successfully integrates multiple data modalities and leverages temporal dynamics, making it a valuable analytic tool for functional brain imaging studies focused on clinical outcome prediction.

Keywords: dynamic functional connectivity, graph neural networks, tobacco use, fusion, transformer, longitudinal fMRI, NCANDA.

# 1. Introduction

Globally, tobacco use is acknowledged as the foremost preventable cause of death and disease, with the number of deaths projected to rise from 5 million to 8 million annually by 2030 (Warren et al. 2009). The widespread prevalence of smoking among children and adolescents is a major public health concern. A seminal study conducted in 1995 in the United States revealed that  $20\%$  of premature deaths related to tobacco among individuals under 17 could have been prevented if they had not started smoking (Epstein et al. 2000). Typically beginning in adolescence, smoking behavior is prone to persist into adulthood, significantly contributing to the escalating rates of tobacco-related diseases. These deeply ingrained behaviors are challenging to alter, highlighting the critical need for early detection and targeted prevention programs aimed at high-risk adolescent smokers as vital public health initiatives.

In the present study, our primary focus lies in harnessing longitudinal data obtained from Functional Magnetic Resonance Imaging (fMRI) in conjunction with demographic data from the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA) study (Brown et al. 2015). Our attention is particularly directed towards resting-state fMRI, a pivotal technique in exploring brain functional connectivity (Biswal et al. 1995). This methodology has been crucial in uncovering links between brain activity and a range of cognitive and behavioral outcomes (Van Den Heuvel and Pol 2010; Zhang et al. 2021). Importantly, it is increasingly acknowledged that alterations in functional connectivity attributable to disorders and diseases broadly affect neural networks, extending beyond localized brain regions (Zhang et al. 2021). In a recent review (Boer et al. 2022), structural changes observed via MRI have been shown to predict future tobacco and other substance use in adolescents, providing evidence of specific brain structure vulnerabilities for future use. However, evidence from longitudinal functional MRI studies remains limited. In adolescents, resting-state functional connectivity is integral to neurodevelopment (Ernst et al. 2015) and has the potential to identify vulnerable connectivity patterns and populations.

To gain a more comprehensive understanding of functional connectivity (FC) networks and their associations with diverse outcomes (Lurie et al. 2020), researchers have increasingly shifted their focus from static FC to time-varying FC, leveraging resting-state fMRI data. This shift represents a significant evolution in the field, underscored by the rapid growth and burgeoning interest in time-varying FC (Lurie

et al. 2020). Predominantly, these studies utilize the sliding window technique to analyze pairwise correlations, thereby establishing it as the de facto standard method (Lurie et al. 2020). However, a considerable limitation of most time-varying FC methods is their inadequate focus on coherently summarizing temporally ordered FC, particularly in relation to predicting future disease outcomes. The critical role of incorporating the temporal sequence in FC matrices has been recognized and previously explored in research (Yaesoubi et al. 2015).

Recent advancements in machine learning and deep learning have profoundly influenced the development of innovative methods for analyzing functional connectivity from resting-state fMRI data (Khosla et al. 2019). These methods have proven instrumental in predicting social anxiety disorders (Månsson et al. 2015) and diagnosing neurological conditions (Devika and Oruganti 2021). Notably, the application of Convolutional Neural Networks (CNN) in Alzheimer's Disease analysis (Sun et al. 2022) and the utilization of Recurrent Neural Network (RNN) models, such as Long Short-Term Memory (LSTM) models (Hochreiter and Schmidhuber 1997), with the NCANDA dataset for studying adolescents with alcohol use disorders (Ouyang et al. 2020) highlight the versatility of these advancements. In network neuroscience, Graph Neural Networks (GNN) have emerged as a crucial tool for analyzing non-Euclidean graph data, such as functional connectivity (Bessadok et al. 2022). This contrasts with traditional tabular variables like sex and age, which are typically represented in Euclidean space. Applications of GNNs to functional connectivity data have included cross-sectional predictions of age, sex (Gadgil et al. 2020; Li et al. 2022), and various diseases (Ktena et al. 2018; Wang et al. 2021). Prior applications of GNN and LSTM have focused on examining dynamic brain networks for disease prediction without integrating tabular data (Cao et al. 2022; Liu et al. 2023). These efforts have either targeted outcomes concurrent with the fMRI data or overlooked the integration of tabular data in their models. However, recent advancements in sequence modeling, particularly with transformers (Vaswani et al. 2017), such as large language models (Chang et al. 2023; Thirunavukarasu et al. 2023), have opened new avenues for enhancing predictions based on sequences.

Simultaneously, a myriad of machine learning and deep learning models have been developed specifically for Euclidean tabular predictors. In the context of fMRI research, it is common practice to collect various subject characteristics, including demographic and clinical variables, in addition to outcome measures and fMRI data. These variables are important biological variables of interest and should be

integrated into analytical models. Intrinsically, brain networks are non-Euclidean in nature. To integrate both types of data, traditional methods attempt to simplify this complexity by undertaking feature engineering, such as the extraction of key network attributes via graph-theoretical analysis (Bullmore and Sporns 2009). These techniques transform non-Euclidean graph data into Euclidean space, facilitating the application of numerous Euclidean-based methods. However, the potential information loss resulting from feature engineering is a significant concern. Additionally, the complex, nonlinear relationships between graph data and tabular data might be inadequately addressed in these feature engineering attempts, prompting questions about the thoroughness of such methodologies.

Despite these advancements, a significant gap persists in effectively integrating time-varying functional connectivity with tabular data in a holistic, end-to-end approach, especially respecting the temporal order. To address this, our study introduces the Graph Neural Network Transformer Fusion (GNN-TF), a novel model specifically designed to concurrently analyze brain dynamic connectivity and tabular data within a unified neural network framework. This approach aims to harness the potential of existing methodologies while uniquely accounting for the temporal sequence of events, thereby offering a new perspective in understanding brain dynamics. In our study, there is a natural temporal order among the tabular data, dynamic connectivities, and future tobacco use outcomes, which our model inherently respects. This logical, time-aware integration enhances prediction performance.

We validated our GNN-TF approach using a longitudinal fMRI dataset from the NCANDA study, focusing on the challenging task of forecasting future tobacco use. Furthermore, we conducted extensive comparisons with other machine learning and deep learning models. The contributions of this paper are as follows:

a. Introduction of an end-to-end, temporally-aware Transformer Fusion structure for integrating multiple non-Euclidean and Euclidean data sources within a unified framework.  
b. Extensive comparison with various machine learning and deep learning models in evaluation studies.  
c. Integration of our model with state-of-the-art pretrained transformer models using transfer learning to enhance its predictive capabilities.  
d. Application and comparison of various GNN backbone structures (Kipf and Welling 2016; Velicković et al. 2017; Brody et al. 2021; Xu et al. 2018; Gravina et al. 2022; Gasteiger et al. 2018; Monti et al. 2017) within our framework, offering

a comprehensive assessment of their performance.

e. Evaluation of the model's efficacy in forecasting future tobacco use using a public, medium-sized longitudinal fMRI dataset from NCANDA.

# 2. Materials and Methods

# 2.1. Dataset and Preprocessing

This study utilized the curated dataset from the NCANDA study (Brown et al. 2015), a multi-site longitudinal neuroimaging study. The NCANDA study investigates adolescents at diverse stages of development using an accelerated longitudinal design. The study tracks a cohort of youths aged 12-21 years (49% male, assigned at birth; 64% white; over 50% at risk of heavy drinking), from baseline and annually for up to nine years.

This study included resting-state fMRI (rs-fMRI) data and key biological variables, all collected at baseline (year 0). The pivotal biological variables were age and sex assigned at birth. In addition, self-reported tobacco use was monitored in baseline and follow-up visits. Participants diagnosed with any substance use disorder according to DSM IV (1994) at baseline were excluded. To investigate the onset of tobacco use in tobacco-naive individuals, subjects who reported tobacco use at baseline were also excluded (Sharapova et al. 2020). The outcome variable, future tobacco use, was identified as any use reported during follow-up visits in years 1-3, as available in the current data release at the time of this analysis. The fMRI images were processed using the C-PAC pipeline (Craddock et al. 2013). Preprocessing steps included optimized brain extraction (Lutkenhoff et al. 2014), discarding the first five scans for stabilization, slice timing correction, nonlinear registration to MNI space, ICA-AROMA denoising (Pruim et al. 2015), linear detrending, motion correction, and CompCor (Behzadi et al. 2007). ROI-based average time series were extracted for each participant using either the Power 264 atlas (Power et al. 2011) or a customized Harvard-Oxford atlas (Zhao et al. 2021). Subjects lacking follow-up data or with fMRI data not meeting quality standards were excluded, resulting in  $N = 522$  subjects contributing to multivariate time series matrices.

![](images/84c2e4d3701cc47d0e8dc18b3d0670a400e0e9a01d9e29b932f6d6ab891d78b6.jpg)  
FIGURE 1. The proposed GNN-TF network structure for fMRI imaging and structured data in classification tasks. A classification token "cls" is added to the beginning of the sequence in all transformer models as a prompt, except for GPT2. In GPT2, following the guidelines in the OpenAI GPT2 documentation from the Python package "transformers", "cls" is placed at the end of the sequence, with sex and age features being projected to the penultimate position.

# 2.2. Method Overview

Graph neural networks (GNNs) have become instrumental in analyzing network data and forecasting outcomes. In our enhancement to the GNN framework, we integrated tabular variables, including both raw measures and hand-crafted features, into a unified GNN model. Additionally, we incorporated a transformer network structure for handling temporal brain connectivity dynamics using transfer learning. The brain connectivity networks were constructed using Pearson's correlations between brain regions, based on resting-state fMRI time series from popular brain atlases (Power et al. 2011; Zhao et al. 2021). Multiple testing correction was employed to foster sparse connections and reduce false positives. Our model utilized raw measures derived from subject-level variables such as age and sex. The hand-crafted features comprised node-level attributes related to the biological characteristics (e.g., location and functional segmentation) of brain regions. We chose these tabular variables due to data availability and our scientific objectives, though our methodological framework is adaptable to incorporate other types of tabular variables. Our GNN-TF network structure is illustrated in Fig. 1.

# 2.3. Functional Connectivity Construction

The basic Pearson's correlation matrices can be interpreted as fully connected, undirected weighted networks. These networks differ from typical GNN input data due to their fully connected nature. Historically, thresholding the correlation matrices has been a popular approach before applying GNN models, with the threshold level often treated as a critical hyperparameter (Wang et al. 2023). In our study, we employed a data-driven, multiple testing correction approach for thresholding correlation matrices, following a multi-step procedure.

Initially, Pearson's correlation matrices  $C^{(is)}$ , for  $i = 1,2,\dots ,N$ ,  $s = 1,2,\dots ,8$  were computed to quantify brain connectivity from participant  $i$  and sliding time window  $s$ . Here we utilized a sliding window approach to calculate dynamic functional connectivity to be described momentarily. Following this, the Benjamini/Hochberg False Discovery Rate Correction (FDR) was applied to the matrix of p-values. Subsequently, binary adjacency matrices were generated. In these matrices, FDR corrected p-values of 0.05 or less were denoted as 1, indicating a significant connection, while p-values above 0.05 were marked as 0, representing the absence of a significant connection. This process resulted in a binary matrix  $B^{(is)}$  for each  $i$  and  $s$ .

The final step involved computing weighted graphs  $A^{(is)} = B^{(is)} \odot C^{(is)}$ , where element-wise Hadamard multiplication  $\odot$  was performed between the binary adjacency matrix and the correlation coefficients matrix for each  $i$  and  $s$ . For the sake of simplicity, we will omit the superscripts  $i$  and  $s$  in the notation for  $A^{(is)}$  in the subsequent discussion.

# 2.4. Graph Neural Network

In our study, we chose to focus on the Graph Convolutional Network (GCN) (Kipf and Welling 2016) structure for graph data, motivated by its computational efficiency and its performance, which can be comparable to or sometimes even superior to other models (Wu et al. 2021). While the substitution of GCN with other GNN backbone architectures (Veličković et al. 2017; Brody et al. 2021; Xu et al. 2018; Gravina et al. 2022; Gasteiger et al. 2018; Monti et al. 2017) is straightforward, our description and analysis primarily centered on the GCN structure. Our model consisted of two convolutional layers, with the operation at the  $l$ th layer described as follows:

$$
H ^ {(l)} = f \left(H ^ {(l - 1)}, A\right) = \sigma \left(D ^ {- \frac {1}{2}} A D ^ {- \frac {1}{2}} H ^ {(l - 1)} W ^ {(l - 1)}\right). \tag {1}
$$

Here,  $D$  is a diagonal matrix with entries  $D_{ii} = \sum_{j}\mathbf{1}\{A_{ij}\neq 0\}$ , where  $\mathbf{1}\{\cdot \}$  is the indicator function.  $H^{(l)}$  represents the output from the  $l$ th layer, and  $W^{(l)}$  denotes the learnable parameter matrix at the  $l$ th layer.  $\sigma (\cdot)$  is the activation function. We set  $W^{(l)}$  with dimensions such that the outputs  $H^{(l)}$ , for  $l = 1,2$ , both had a dimension of 128. The Scaled Exponential Linear Unit (SELU) (Klambauer et al. 2017) was used as the activation function throughout the network.

In our GCN, the nodes corresponded to brain regions. We set the input node features  $H^{(0)}$  to the MNI space coordinates in the  $x$ ,  $y$ , and  $z$  directions, along with the one-hot encoding of suggested system classifications (Power et al. 2011). A fully connected layer with an output dimension of 128 was included as the final layer to project the convolutional output into a space relevant to our forecasting task.

# 2.5. Transformer Fusion

To capture the dynamics of the brain, we employed a sliding window strategy, a common approach in this field (Hutchison et al. 2013). Considering computational time, we opted for 8 sliding windows, each with a width of 130 TR and a step size of 20 TR (where  $\mathrm{TR} = 2.2$  seconds), aligning with recommended window sizes in (Savva et al. 2019). This window size was chosen to capture variation over a reasonable time span, essential for our goal of forecasting future smoking outcomes in 3 years. Moreover, this sample size in each sliding window provides sufficient statistical power for our network construction, as detailed in Section 2.3.

Our forecasting problem combines dynamic brain network features and tabular variables into a sequence classification framework. We framed this challenge analogously to similar problems encountered in computer vision and natural language processing, and utilized transformer models (Vaswani et al. 2017) to integrate dynamic GNN features with tabular variables. Various transformer models, including pretrained ones, were incorporated into our framework. We experimented with training the standard transformer model (Vaswani et al. 2017) from scratch, as well as employing popular pretrained transformers for other tasks, including ViT (Dosovitskiy et al. 2020), BERT (Devlin et al. 2018), and GPT-2 (Radford et al. 2018). The standard transformer model in our study had three layers, each with four-head attention. The input sequence's first position was set to the projected embedding of tabular variables (sex and age), followed by GNN features in their temporal order. This projection was achieved using a fully connected layer on batch-normalized input with SELU activation, ensuring the embedding dimension of 128

matched the GNN output. For transfer-learning tasks, a projection layer was added to align dimensions with pretrained temporal transformer models. An alternate fusion strategy, termed 'late fusion', diverges by not integrating variables directly into the transformer. Instead, they are fused outside the transformer by concatenating the transformer output with the covariate embedding before the prediction layer, as depicted in Fig. 2. However, late fusion does not respect the temporal order of the tabular and imaging data. This may not provide the optimal strategy, nor does it enhance interpretability, which our proposed approach achieves through temporal awareness. We will discuss results from this fusion approach and other state-of-the-art methods in ablation studies in Section 3.3.

Finally, we employed the negative log-likelihood loss function for binary outcomes to evaluate the transformer predictions. This involved using three fully connected layer projections followed by a log-softmax layer, set against the future smoking status.

# 3. Experiments and Results

# 3.1. Experimental Setup

In our experiment, we adopted a stratified five-fold cross-validation strategy with inner-fold ensembling to ensure robust evaluation. In the outer loop,  $20\%$  of the subjects were withheld as the testing set. Within the remaining training data, we employed an inner stratified five-fold cross-validation to train the models and determine early stopping. To mitigate the potential variance associated with GNN training, the final performance for each outer fold was computed by averaging the predictions of the five models trained in the inner loop.

To ensure a fair comparison, we adopted a unified set of hyperparameters, given that the trainable components (GNN encoders and downstream linear layers) were structurally similar across all models. For the training process, Adam was used as the optimizer with a learning rate of  $10^{-4}$  and a batch size of 32. The hidden layer dimension was set to 128 to balance model capacity and overfitting risks. To further prevent overfitting, early stopping was implemented, guided by the Area Under the Curve (AUC) observed on the validation set.

Depending on the transformers used, our model variations were termed GNN-TF, GNN-TF-V, GNN-TF-B, and GNN-TF-G, corresponding to transformers trained from scratch, ViT, BERT, and GPT-2, respectively. We compared these models with

the state-of-the-art dynamic GNN model, GC-LSTM (Chen et al. 2022). GC-LSTM aggregates graph data at each temporal snapshot using GCN, and LSTM is used to learn temporal dynamics. This structure has been shown to be superior to several other proposals. The existing GC-LSTM does not incorporate tabular variables. For a fair comparison, we applied a late fusion strategy (similar to Fig. 2) by concatenating the projected variables with the GC-LSTM output before feeding it into the final prediction layer. This modified version is termed GC-LSTM-F. We also compared our models with classical machine learning models, such as logistic regression (LR) and random forests (RF), as these are typically developed for non-sequence data. This comparison is detailed in our ablation studies in Section 3.3.

# 3.2. Performance Evaluation

The area under the receiver operating characteristic curve (AUC) and the Precision-Recall AUC (PRAUC) were utilized to evaluate the classification performance of our models. These metrics are advantageous as they do not rely on the tuning of threshold parameters, unlike accuracy. They provide an overall assessment of performance without considering the trade-off between false positives and negatives. The chance value for AUC is 0.5, while the chance level for PRAUC also varies depending on the prevalence. To ensure the robustness of our comparisons, each method was repeated five times, and the average AUC and PRAUC values across these repetitions were reported.

As summarized in Table 1, our comparative analysis of model performance highlighted the superior capabilities of the GNN-TF models across all variants. These models showed a notable enhancement in prediction performance compared to the state-of-the-art GC-LSTM models. In particular, the GNN-TF variants consistently outperformed the GC-LSTM models in both AUC and PRAUC metrics, underscoring their effectiveness in forecasting tasks. The performance remained similar across different brain atlases, indicating that our approach is robust against varying atlas choices. The enhanced results achieved by the GNN-TF models can be largely attributed to their ability to effectively integrate and learn from complex dynamic features, while simultaneously considering tabular covariates.

# 3.3. Ablation Studies

We conducted a series of ablation studies to evaluate and compare models under various conditions, using the same experimental setup and metrics previously described.

TABLE 1. Comparison of Prediction Performance  

<table><tr><td rowspan="2">Model</td><td colspan="2">Power264</td><td colspan="2">HO</td><td rowspan="2"># of Trainable 
Parameters</td></tr><tr><td>AUC</td><td>PRAUC</td><td>AUC</td><td>PRAUC</td></tr><tr><td>GCLSTM</td><td>0.520</td><td>0.171</td><td>0.447</td><td>0.159</td><td>855,554</td></tr><tr><td>GCLSTM-F</td><td>0.658</td><td>0.263</td><td>0.676</td><td>0.256</td><td>872,326</td></tr><tr><td>GNN-TF</td><td>0.697</td><td>0.267</td><td>0.710</td><td>0.268</td><td>1,917,190</td></tr><tr><td>GNN-TF-V</td><td>0.676</td><td>0.244</td><td>0.702</td><td>0.257</td><td>390,662</td></tr><tr><td>GNN-TF-B</td><td>0.665</td><td>0.274</td><td>0.695</td><td>0.281</td><td>390,662</td></tr><tr><td>GNN-TF-G</td><td>0.700</td><td>0.271</td><td>0.698</td><td>0.250</td><td>491,782</td></tr></table>

Average AUC and PRAUC metrics using the Power 264 (Power264) atlas (Power et al. 2011) and the customized Harvard-Oxford (HO) atlas (Zhao et al. 2021), and their trainable parameters. The best performed for each category is highlighted in bold.

These studies involved multiple modifications to test the robustness and flexibility of our models. Modifications included replacing the base predictive models with alternative machine learning and GNN architectures, excluding covariates, omitting functional connectivity data, abandoning the sliding window method for calculating dynamic connectivity, and excluding transformer fusion as detailed in Section 2.5. The machine learning predictive models we evaluated included Random Forests (RF) and Logistic Regression (LR). Additionally, we explored a variety of GNN backbone architectures. These comprised the Graph Attention Network (GAT) (Velicković et al. 2017), GATv2 (Brody et al. 2021), the Graph Isomorphism Network (GIN) (Xu et al. 2018), the Anti-Symmetric Graph Convolutional Layer (AntiSymmetricConv) (Gravina et al. 2022), Approximate Personalized Propagation (APPNP) (Gasteiger et al. 2018), and the Gaussian Mixture Model Convolutional Layer (GMMConv) (Monti et al. 2017). Beyond our transformer fusion (TF) approach, we also examined models using an alternative fusion strategy known as late fusion described in Fig. 2. We summarized these findings in Table 2.

The results revealed that one of our proposed models, GNN-TF, consistently outperformed almost all other models in the evaluation. Among the total of 56 metrics assessed, only five instances were noted where other models achieved higher values than our proposed GNN-TF model. It's important to highlight that none of these models surpassed the GNN-TF model in both AUC and PR AUC simultaneously. Upon a closer examination of these five metrics, we observed that the differences in performance were relatively minimal. Moreover, when comparing against a more

![](images/a84c43d71ca1697dc1b4dd5379b88de583b41b2f417efa58fd4b607d48e6ad3f.jpg)  
FIGURE 2. In ablation studies, an alternative late-fusion strategy (inside the light green box above) replace the TF-fusion module (light blue) in Fig. 1.

advanced variant of our model, GNN-TF-G, the number of metrics where other models performed slightly better reduced to only three out of the total 56.

# 4. Discussion

# 4.1. General Discussion

We introduced the use of GNN with transformer fusion as a novel method to integrate dynamic brain networks and tabular data. Traditional statistical methods abound for assessing the association between tabular covariates and various outcomes, such as substance use and mental health conditions. However, these methods often fall short in handling the dynamic network data structure that GNNs are adept at modeling. Our approach bridges this methodological gap by utilizing deep learning models capable of incorporating both data types. This integration is particularly beneficial for functional imaging studies where both types of data are relevant to the outcomes.

Our method demonstrated superior performance compared to state-of-the-art alternatives. Additionally, its simplicity and flexibility facilitate the easy integration of pretrained transformer models. It's important to note that the fields of GNN and deep learning are rapidly evolving, presenting other potential approaches that we did not explore. These opportunities are left for future research. Overall, our study underscores the importance of effectively fusing both brain dynamics and structural covariates in functional imaging research.

# 4.2. Contributions of Node Features and Connectivity

To delve deeper into understanding the influential node features and connections that predicted the outcomes, we utilized GNNExplainer (Ying et al. 2019), an

optimization-based tool. For illustrative purposes, we based our analysis on the Power 264 atlas using our GNN-TF model. Due to computational constraints associated with GNNExplainer, a random subset of 32 subjects was selected for the analysis. We then computed the mean and  $95\%$  confidence intervals (mean  $\pm 1.96$ se) of the GNNExplainer results across these subjects.

Fig. 3 presented the top 5 node features identified by GNNExplainer. The findings highlighted the MNI space locations of the brain regions as the top three significant features in predicting outcomes, with importance values ranging decently between 0.42 and 0.44 (the best possible value being 1). Additionally, regions associated with the default mode and visual systems also contributed to the prediction performance, albeit with a lesser magnitude of importance. These results indicated that structural information from specific brain regions, combined with functional brain connectivity graphs in our model, played a pivotal role in forecasting future tobacco use.

Fig. 4 displayed the top 25 brain connections identified in our study. $^{1}$  Notably, the majority of these connections were inter-hemispheric, linking almost symmetric regions across the two hemispheres of the brain. The brain regions primarily involved in these connections were predominantly located within the visual cortex and motor cortex. This pattern underscored the significance of these areas and their interconnectedness in the context of our study's focus.

# 4.3. Limitations and Future Works

Our study primarily utilized the NCANDA dataset, a longitudinal fMRI dataset of medium sample size. This unique design and sample size enabled the development of GNN fusion models with a reasonable number of parameters, allowing us to examine forecasting performance in light of the natural temporal order. However, it's noteworthy that the sample size in this study is larger than that typically found in single-site fMRI studies. In such cases, training deep learning models from scratch often risks overfitting. To counter this, we leveraged pretrained transformer models through transfer learning, effectively reducing the number of parameters. This approach may help mitigate overfitting issues, and its applicability to other datasets with varying sample sizes warrants further evaluation.

In our analyses, we focused on sex and age as covariates due to their biological significance and accurate ascertainment. While there are other potential covariates,

![](images/4c8b3fa36c41982900770dd6be217444179fbafc4324ed7f557303f7159609ce.jpg)  
FIGURE 3. Top five node features ranked by GNNExplainer. Feature importance values range from 0 (no impact on prediction) to 1 (the most impactful)

such as those derived from behavioral questionnaires, that could be associated with the outcome, their inclusion remains to be explored. Incorporating additional covariates into our framework is straightforward, but it's yet to be determined if this would improve performance or if high-dimensional covariates might introduce overfitting problems.

Our choice of using sliding windows with the current time span was driven by our interest in predicting outcomes over a three-year period. Depending on the specific outcome of interest, these choices might need to be adjusted. A plausible hypothesis is that immediate future outcomes, measured in minutes or hours, could correlate with brain dynamics over shorter time periods. The validity of this hypothesis in the context of other datasets remains an area for future investigation.

# 5. Conclusion

In this paper, we introduced a Graph Neural Network with Transformer Fusion (GNN-TF) framework designed to integrate brain dynamics with tabular covariates for predictive purposes. Based on the extensive experimental comparisons conducted using the NCANDA dataset, this approach demonstrated superior performance in comparison to other existing methods. Given its efficacy, this methodological proposal holds potential utility for other functional brain imaging studies where the integration

![](images/3246ae42cb260f29585cdb53b8a476b3df8db26b8b1f9c73824a1fcfc802cd05.jpg)  
FIGURE 4. Top 25 brain connections ranked by GNNExplainer. The brain connections are shown in yellow, and the associated brain nodes are shwon in blue.

of multiple data types is crucial for predicting clinical outcomes. The versatility and effectiveness of this approach in integrating diverse data types make it a promising tool for advancing research in the field of brain imaging and its clinical applications.

# Acknowledgments

We thank the NCANDA study and its investigators for providing the raw data analyzed in this project.

# Funding

This work was supported in part by the NIH grants R01MH126970, R01EB022911, RF1AG079324, RF1AG074204, R01NS133743, and P30AI161943. The NCANDA data collection was supported by NIH grants AA021697, AA021697-04S1, AA021695,

# Ethical considerations

Only de-identified data derived from the NCANDA study were provided to us for this secondary-analysis study, and we have no way to link the coded data to individual identities. Consequently, this study is classified as non-human-subjects research and does not require IRB approval.

We are not affiliated with the NCANDA research team. The NCANDA research team obtained Institutional Review Board (IRB) approval at each participating site, and all data collection procedures were conducted in accordance with U.S. Department of Health and Human Services regulations (45 CFR Part 46).

# Data availability statement

The data that support the findings of this study are openly available in figshare at http://doi.org/10.6084/m9.figshare.29318804, reference number

[10.6084/m9.figshare.29318804]. The code implementation of our research will be made publicly available on GitHub upon the acceptance of our work for publication.

The raw data analyzed in this study were obtained from the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA). Access to the dataset is restricted to protect human subjects, and was granted to us under a non-transferable, non-distributable data-use agreement. Researchers can obtain the raw data by completing the data-use agreements with the NCANDA study and the National Institute on Alcohol Abuse and Alcoholism (NIAAA), following the application procedure described at http://www.ncanda.org/datasharing.php.

# Author contribution statement

RZ contributed to conceptualization, formal analysis, investigation, methodology, software, validation, visualization, and writing—original draft of the manuscript. XL contributed to conceptualization, data curation, funding acquisition, methodology, resources, supervision, and writing—original draft, as well as review and editing of the manuscript.

# Conflict of interest

The authors declare no potential conflict of interests.

# References

Behzadi, Y., Restom, K., Liau, J., and Liu, T. T. (2007). A component based noise correction method (compcor) for bold and perfusion based fmri. Neuroimage, 37(1):90-101.  
Bessadok, A., Mahjoub, M. A., and Rekik, I. (2022). Graph neural networks in network neuroscience. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(5):5833-5848.  
Biswal, B., Zerrin Yetkin, F., Haughton, V. M., and Hyde, J. S. (1995). Functional connectivity in the motor cortex of resting human brain using echo-planar mri. Magnetic resonance in medicine, 34(4):537-541.  
Boer, O. D., El Marroun, H., and Franken, I. H. (2022). Brain morphology predictors of alcohol, tobacco, and cannabis use in adolescence: a systematic review. *Brain Research*, page 148020.  
Brody, S., Alon, U., and Yahav, E. (2021). How attentive are graph attention networks?  
Brown, S. A., Brumback, T., Tomlinson, K., Cummins, K., Thompson, W. K., Nagel, B. J., De Bellis, M. D., Hooper, S. R., Clark, D. B., Chung, T., et al. (2015). The national consortium on alcohol and neurodevelopment in adolescence (ncanda): a multisite study of adolescent development and substance use. Journal of studies on alcohol and drugs, 76(6):895-908.  
Bullmore, E. and Sporns, O. (2009). Complex brain networks: graph theoretical analysis of structural and functional systems. Nature reviews neuroscience, 10(3):186-198.  
Cao, P., Wen, G., Liu, X., Yang, J., and Zaiane, O. (2022). Modeling the dynamic brain network representation for autism spectrum disorder diagnosis. *Medical & Biological Engineering and Computing*, 60:1897-1913.  
Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., Wang, Y., et al. (2023). A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology.  
Chen, J. et al. (2022). Gc-lstm: Graph convolution embedded LSTM for dynamic network link prediction. Applied Intelligence, pages 1-16.  
Craddock, C., Sikka, S., Cheung, B., Khanuja, R., Ghosh, S. S., Yan, C., Li, Q., Lurie, D., Vogelstein, J., Burns, R., et al. (2013). Towards automated analysis of connectomes: The configurable pipeline for the analysis of connectomes (c-pac). Front Neuroinform, 42(10.3389).

Devika, K. and Oruganti, V. (2021). A machine learning approach for diagnosing neurological disorders using longitudinal resting-state fmri. In 2021 11th International Conference on Cloud Computing, Data Science and Engineering, pages 494-499. IEEE.  
Devlin, J. et al. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.  
Dosovitskiy, A. et al. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.  
Epstein, J. A., Griffin, K. W., and Botvin, G. J. (2000). A model of smoking among inner-city adolescents: the role of personal competence and perceived social benefits of smoking. Preventive medicine, 31(2):107-114.  
Ernst, M., Torrisi, S., Balderston, N., Grillon, C., and Hale, E. A. (2015). fmri functional connectivity applied to adolescent neurodevelopment. Annual review of clinical psychology, 11:361-377.  
Gadgil, S., Zhao, Q., Pfefferbaum, A., Sullivan, E. V., Adeli, E., and Pohl, K. M. (2020). Spatio-temporal graph convolution for resting-state fmri analysis. In Medical Image Computing and Computer Assisted Intervention-MICCAI 2020: 23rd International Conference, Lima, Peru, October 4-8, 2020, Proceedings, Part VII 23, pages 528-538. Springer.  
Gasteiger, J., Bojchevski, A., and Gunnemann, S. (2018). Predict then propagate: Graph neural networks meet personalized pagerank. arXiv preprint arXiv:1810.05997.  
Gravina, A., Bacciu, D., and Gallicchio, C. (2022). Anti-symmetric dgn: a stable architecture for deep graph networks. arXiv preprint arXiv:2210.09789.  
Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. *Neural computation*, 9(8):1735-1780.  
Hutchison, R. M., Womelsdorf, T., Allen, E. A., Bandettini, P. A., Calhoun, V. D., Corbetta, M., Della Penna, S., Duyn, J. H., Glover, G. H., Gonzalez-Castillo, J., et al. (2013). Dynamic functional connectivity: promise, issues, and interpretations. Neuroimage, 80:360-378.  
Khosla, M., Jamison, K., Ngo, G. H., Kuceyeski, A., and Sabuncu, M. R. (2019). Machine learning in resting-state fmri analysis. *Magnetic resonance imaging*, 64:101-121.  
Kipf, T. and Welling, M. (2016). Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907.  
Klambauer, G. et al. (2017). Self-normalizing neural networks. In Advances in Neural Information Processing Systems, volume 30.  
Ktena, S. I., Parisot, S., Ferrante, E., Rajchl, M., Lee, M., Glocker, B., and Rueckert, D. (2018). Metric learning with spectral graph convolutions on brain connectivity networks. NeuroImage, 169:431-442.

Li, Y., Wei, Q., Adeli, E., Pohl, K., and Zhao, Q. (2022). Joint graph convolution for analyzing brain structural and functional connectome. In Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference, Singapore, September 18-22, 2022, Proceedings, Part I, pages 231-240. Springer.  
Liu, L., Wen, G., Cao, P., Hong, T., Yang, J., Zhang, X., and Zaiane, O. (2023). Braintgl: A dynamic graph representation learning model for brain network analysis. Computers in Biology and Medicine, (106521).  
Lurie, D. J., Kessler, D., Bassett, D. S., Betzel, R. F., Breakspear, M., Kheilholz, S., Kucyi, A., Liegeois, R., Lindquist, M. A., McIntosh, A. R., et al. (2020). Questions and controversies in the study of time-varying functional connectivity in resting fmri. Network neuroscience, 4(1):30-69.  
Lutkenhoff, E. S., Rosenberg, M., Chiang, J., Zhang, K., Pickard, J. D., Owen, A. M., and Monti, M. M. (2014). Optimized brain extraction for pathological brains (optibet). *PloS one*, 9(12):e115551.  
Måsson, K., Frick, A., Boraxbekk, C.-J., Marquand, A., Williams, S., Carlbring, P., Andersson, G., and Furmark, T. (2015). Predicting long-term outcome of internet-delivered cognitive behavior therapy for social anxiety disorder using fmri and support vector machine learning. *Translational Psychiatry*, 5:e530.  
Monti, F., Boscaini, D., Masci, J., Rodola, E., Svoboda, J., and Bronstein, M. (2017). Geometric deep learning on graphs and manifolds using mixture model cnns. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5115-5124.  
Ouyang, J., Zhao, Q., Sullivan, E., Pfefferbaum, A., Tapert, S., Adeli, E., and Pohl, K. (2020). Longitudinal pooling & consistency regularization to model disease progression from mris. *IEEE Journal of Biomedical and Health Informatics*, 25:2082-2092.  
Power, J. D., Cohen, A. L., Nelson, S. M., Wig, G. S., Barnes, K. A., Church, J. A., Vogel, A. C., Laumann, T. O., Miezin, F. M., Schlaggar, B. L., et al. (2011). Functional network organization of the human brain. *Neuron*, 72(4):665-678.  
Pruim, R. H., Mennes, M., van Rooij, D., Llera, A., Buitelaar, J. K., and Beckmann, C. F. (2015). Ica-aroma: A robust ica-based strategy for removing motion artifacts from fmri data. Neuroimage, 112:267-277.  
Radford, A. et al. (2018). Improving language understanding by generative pre-training.  
Savva, A. et al. (2019). Assessment of dynamic functional connectivity in resting-state fmri using the sliding window technique. *Brain and Behavior*, 9:e01255.  
Sharapova, S., Reyes-Guzman, C., Singh, T., Phillips, E., Marynak, K. L., and Agaku, I. (2020). Age of tobacco use initiation and association with current use and nicotine dependence among us middle and high school students, 2014-2016. *Tobacco control*,

29(1):49-54.  
Sun, H., Wang, A., and He, S. (2022). Temporal and spatial analysis of alzheimer's disease based on an improved convolutional neural network and a resting-state fmri brain functional network. International Journal of Environmental Research and Public Health, 19:4508.  
Thirunavukarasu, A. J., Ting, D. S. J., Elangovan, K., Gutierrez, L., Tan, T. F., and Ting, D. S. W. (2023). Large language models in medicine. Nature medicine, 29(8):1930-1940.  
Van Den Heuvel, M. P. and Pol, H. E. H. (2010). Exploring the brain network: a review on resting-state fmri functional connectivity. European neuropsychopharmacology, 20(8):519-534.  
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.  
Velicković, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y. (2017). Graph attention networks. arXiv preprint arXiv:1710.10903.  
Wang, L., Li, K., and Hu, X. P. (2021). Graph convolutional network for fmri analysis based on connectivity neighborhood. Network Neuroscience, 5(1):83-95.  
Wang, Z., Xu, Y., Peng, D., Gao, J., and Lu, F. (2023). Brain functional activity-based classification of autism spectrum disorder using an attention-based graph neural network combined with gene expression. *Cerebral Cortex*, 33(10):6407-6419.  
Warren, C. W., Lea, V., Lee, J., Jones, N. R., Asma, S., and McKenna, M. (2009). Change in tobacco use among 13-15 year olds between 1999 and 2008: findings from the global youth tobacco survey. *Global health promotion*, 16(2_suppl):38-90.  
Wu, J., Sun, J., Sun, H., and Sun, G. (2021). Performance analysis of graph neural network frameworks. In 2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), pages 118-127. IEEE.  
Xu, K., Hu, W., Leskovec, J., and Jegelka, S. (2018). How powerful are graph neural networks? arXiv preprint arXiv:1810.00826.  
Yaesoubi, M., Allen, E. A., Miller, R. L., and Calhoun, V. D. (2015). Dynamic coherence analysis of resting fmri data to jointly capture state-based phase, frequency, and time-domain information. Neuroimage, 120:133-142.  
Ying, Z. et al. (2019). Gnnexplainer: Generating explanations for graph neural networks. In Advances in Neural Information Processing Systems, volume 32.  
Zhang, J., Kucyi, A., Raya, J., Nielsen, A. N., Nomi, J. S., Damoiseaux, J. S., Greene, D. J., Horovitz, S. G., Uddin, L. Q., and Whitfield-Gabrieli, S. (2021). What have we really learned from functional connectivity in clinical populations? NeuroImage, 242:118466.  
Zhao, Y., Caffo, B., Luo, X., Initiative, A. D. N., et al. (2021). Principal regression for high

TABLE 2. Additional Comparisons of Performance  

<table><tr><td>Backbone</td><td>Tabular</td><td>Connectivity</td><td>Dynamic</td><td>TF</td><td>AUC, Accuracy</td></tr><tr><td>GCN, Transformer</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>0.697, 0.267 (our baseline)</td></tr><tr><td>RF</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>0.551, 0.188</td></tr><tr><td>RF</td><td>✓</td><td>✘</td><td>✘</td><td>✘</td><td>0.600, 0.206</td></tr><tr><td>RF</td><td>✓</td><td>✓</td><td>✘</td><td>✘</td><td>0.552, 0.185</td></tr><tr><td>LR</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>0.520, 0.183</td></tr><tr><td>LR</td><td>✓</td><td>✘</td><td>✘</td><td>✘</td><td>0.672, 0.268</td></tr><tr><td>LR</td><td>✓</td><td>✓</td><td>✘</td><td>✘</td><td>0.582, 0.207</td></tr><tr><td>GCN</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>0.530, 0.176</td></tr><tr><td>GCN</td><td>✓</td><td>✓</td><td>✘</td><td>✘</td><td>0.674, 0.254</td></tr><tr><td>GAT</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>0.563, 0.195</td></tr><tr><td>GAT</td><td>✓</td><td>✓</td><td>✘</td><td>✘</td><td>0.673, 0.260</td></tr><tr><td>GAT2</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>0.535, 0.179</td></tr><tr><td>GAT2</td><td>✓</td><td>✓</td><td>✘</td><td>✘</td><td>0.675, 0.260</td></tr><tr><td>GIN</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>0.507, 0.169</td></tr><tr><td>GIN</td><td>✓</td><td>✓</td><td>✘</td><td>✘</td><td>0.620, 0.243</td></tr><tr><td>AntiSymmetric</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>0.469, 0.152</td></tr><tr><td>AntiSymmetric</td><td>✓</td><td>✓</td><td>✘</td><td>✘</td><td>0.701, 0.256</td></tr><tr><td>APPNP</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>0.501, 0.178</td></tr><tr><td>APPNP</td><td>✓</td><td>✓</td><td>✘</td><td>✘</td><td>0.682, 0.263</td></tr><tr><td>GMM</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>0.568, 0.208</td></tr><tr><td>GMM</td><td>✓</td><td>✓</td><td>✘</td><td>✘</td><td>0.670, 0.276</td></tr><tr><td>GCN, Transformer</td><td>✘</td><td>✓</td><td>✓</td><td>✘</td><td>0.502, 0.163</td></tr><tr><td>GCN, Transformer</td><td>✓</td><td>✓</td><td>✓</td><td>✘</td><td>0.676, 0.259</td></tr><tr><td>GCN, ViT</td><td>✘</td><td>✓</td><td>✓</td><td>✘</td><td>0.522, 0.178</td></tr><tr><td>GCN, ViT</td><td>✓</td><td>✓</td><td>✓</td><td>✘</td><td>0.668, 0.276</td></tr><tr><td>GCN, BERT</td><td>✘</td><td>✓</td><td>✓</td><td>✘</td><td>0.522, 0.180</td></tr><tr><td>GCN, BERT</td><td>✓</td><td>✓</td><td>✓</td><td>✘</td><td>0.674, 0.264</td></tr><tr><td>GCN, GPT2</td><td>✘</td><td>✓</td><td>✓</td><td>✘</td><td>0.484, 0.162</td></tr><tr><td>GCN, GPT2</td><td>✓</td><td>✓</td><td>✓</td><td>✘</td><td>0.672, 0.269</td></tr></table>

The performance changes in AUC and PRAUC, respectively, were assessed by comparing with GNN-TF, the reference model on the first line. The late fusion strategy was used when all options except when TF were checked. Higher values in either AUC or PRAUC than the reference are highlighted in bold. TF: our transformer fusion approach. Only our baseline model, GNN-TF, is listed here for comparison, without the improvements from pretraining. Our other models may provide improved performance; see Table 1.