中国电信人工智能研究院：多模型协作的规模定律
📖标题：The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models
🌐来源：arXiv, 2510.01719

🛎️文章简介
🔸研究问题：多模型协作系统的性能如何随着总参数量的变化而变化？是否存在一个理论上的性能上限？
🔸主要贡献：提出多模型协作的规模定律，该定律预测了基于总参数预算的大规模语言模型集合的性能上限。通过理想化的集成Oracle假设，量化了多模型协作的内在上限。

📝重点思路
🔸引入理想化的集成Oracle假设，其中每个样本的总交叉熵损失由模型池中任何模型的最小损失决定。
🔸通过枚举固定模型池中的所有可能子集，构建经验性能前沿，并提取Pareto最优点。
🔸研究单模型、同质多模型和异质多模型组合的性能，比较它们的幂律关系和渐近损失下限。
🔸通过非线性最小二乘回归拟合幂律公式，定量分析不同组合的扩展效率。

🔎分析总结
🔸多模型系统表现出与单模型类似的幂律关系，但具有更陡峭的下降趋势和显著更低的渐近损失下限。
🔸异质多模型组合在相同参数预算下表现优于同质多模型组合，表明模型多样性是合作增益的主要驱动力。
🔸同质多模型组合在小参数预算下表现出快速的早期改进，但在较大参数预算下迅速饱和。
🔸异质多模型组合不仅提高了扩展效率，还显著降低了渐近损失下限，表明通过多种模型架构和训练方法可以实现互补的建模能力。

💡个人观点
论文通过理论和实证分析，明确了多模型协作的性能上限和扩展规律，为理解和设计多模型系统提供了坚实的理论基础。这不仅丰富了现有的单一模型扩展理论，还为未来的多模型集成技术指明了方向。