北京大学人工智能研究院：大规模语言模型的有效深度影响因素
📖标题：What Affects the Effective Depth of Large Language Models?
🌐来源：NeurIPS 2025, Mechanistic Interpretability Workshop

🛎️文章简介
🔸研究问题：大规模语言模型（LLMs）的有效深度如何受模型规模、训练类型和任务难度的影响？
🔸主要贡献：论文系统地研究了有效深度的变化，发现模型规模增加时有效层数增加但有效深度比保持稳定；长-CoT模型未显著增加有效深度；任务难度对有效深度无显著影响。

📝重点思路
🔸使用Qwen-2.5模型家族（1.5B-32B参数）进行分析，包括残差余弦相似度、logit镜头、层效应、残差擦除和集成梯度等方法。
🔸研究发现，模型规模增加时，有效层数增加，但有效深度比保持稳定。
🔸比较基础模型和长-CoT模型，发现长-CoT模型的有效深度没有显著增加，其性能提升主要来自更长的上下文处理能力。
🔸评估不同难度的任务，发现模型在更难的任务上并没有动态使用更多层。

🔎分析总结
🔸模型规模增加时，虽然绝对有效层数增加，但有效深度比保持稳定，表明更大模型并未改变计算策略。
🔸长-CoT模型的性能提升主要归因于优化后的长序列处理能力，而非单个token的深层计算。
🔸任务难度对模型的有效深度无显著影响，模型不会根据问题难度动态调整使用的层数。
🔸当前的LLMs未能充分利用其可用深度，这为未来研究提供了机会，如提高层利用率、模型剪枝和早期退出机制。

💡个人观点
论文通过系统的实验和分析，揭示了大规模语言模型在不同条件下的有效深度利用情况，为优化模型架构和训练方法提供了重要参考。