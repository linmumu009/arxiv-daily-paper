九州工业大学：基于储层计算的矩阵乘法自由语言模型
📖标题：Reservoir Computing inspired Matrix Multiplication-free Language Model
🌐来源：arXiv

🛎️文章简介
🔸研究问题：大型语言模型（LLM）在自然语言处理中取得了最先进的性能，但其高计算成本仍然是一个主要瓶颈。本文通过引入储层计算（RC）概念，进一步减少矩阵乘法自由语言模型（MatMul-free LM）的训练成本。
🔸主要贡献：提出了一种RC启发的MatMul-free LM（RC MatMul-free LM），通过部分固定和共享选定层的权重，并插入储层层来获得丰富的动态表示，同时减少了训练开销。此外，通过结合多种操作减少内存访问。实验结果表明，该架构将参数数量减少了最多19%，训练时间减少了9.9%，推理时间减少了8.0%，同时保持了与基线模型相当的性能。

📝重点思路
🔸引入RC概念，部分固定和共享选定层的权重，并插入储层层，以减少参数数量和计算开销。
🔸通过逻辑运算实现矩阵乘法，使用三元值{+1, 0, -1}量化参数，显著减少内存使用。
🔸结合多种操作减少内存访问，包括将激活函数融入递归处理，以加速计算。
🔸通过固定和共享参数，减少反向传播操作和内存访问次数，进一步减少训练时间和推理时间。

🔎分析总结
🔸RC MatMul-free LM通过固定和共享参数，显著减少了模型的参数数量和计算开销，同时保持了与基线模型相当的性能。
🔸参数固定和共享对模型性能的影响较小，表明RNN和三元LLM在这些条件下仍能保持较高的准确性。
🔸通过优化内核操作，进一步减少了内存访问次数和处理时间，提高了模型的效率。
🔸RC MatMul-free LM的改进为解决LLM的高计算成本问题迈出了重要一步。

💡个人观点
论文通过引入储层计算概念，成功地减少了MatMul-free LM的参数数量和计算开销，同时保持了模型的性能。这一方法为大规模语言模型的高效部署提供了新的思路，具有重要的实际应用价值。